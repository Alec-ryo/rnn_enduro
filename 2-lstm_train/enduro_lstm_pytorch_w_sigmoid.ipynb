{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ahead-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d093f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "economic-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"../1-generate/data/\"\n",
    "n_epoch = 3000\n",
    "match = 35\n",
    "hidden_neurons = 200\n",
    "\n",
    "model_path = \"\"\n",
    "stop_train = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "gothic-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ACTIONS = {\n",
    "    \"noop\": 0,\n",
    "    \"accelerate\": 1,\n",
    "    \"right\": 2,\n",
    "    \"left\": 3,\n",
    "    \"break\": 4,\n",
    "    \"right_break\": 5,\n",
    "    \"left_break\": 6,\n",
    "    \"right_accelerate\": 7,\n",
    "    \"left_accelerate\": 8,\n",
    "}\n",
    "'''\n",
    "\n",
    "ACTIONS = {\n",
    "    \"right\": 2,\n",
    "    \"left\": 3,\n",
    "}\n",
    "\n",
    "ACTIONS_LIST = list(ACTIONS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "competitive-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(m):\n",
    "    path = data_path + \"match_\" + str(m) + \"/npz/\"\n",
    "\n",
    "    actions = np.load(path + 'actions.npz')\n",
    "    lifes = np.load(path + 'lifes.npz')\n",
    "    frames = np.load(path + 'frames.npz')\n",
    "    rewards = np.load(path + 'rewards.npz')\n",
    "\n",
    "    arr_actions = actions.f.arr_0\n",
    "    arr_lifes = lifes.f.arr_0\n",
    "    arr_frames = frames.f.arr_0\n",
    "    arr_rewards = rewards.f.arr_0\n",
    "\n",
    "    print(\"Successfully loaded NPZ.\")\n",
    "\n",
    "    return arr_actions.shape[0], arr_frames, arr_actions, arr_rewards, arr_lifes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "endangered-sacramento",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded NPZ.\n"
     ]
    }
   ],
   "source": [
    "num_of_frames, frames, actions, rewards, lifes = load_npz(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stretch-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_action_data(action):\n",
    "\n",
    "    new_action = np.zeros((1, len(ACTIONS)), dtype=int) \n",
    "\n",
    "    new_action[0, ACTIONS_LIST.index(action)] = 1\n",
    "\n",
    "    return new_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "composed-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_one_hot = [prepare_action_data(i) for i in actions]\n",
    "Y_train = np.array(action_one_hot)\n",
    "Y_train = Y_train.reshape(1, len(Y_train), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "documentary-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = frames.reshape(1, len(frames) ,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "muslim-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "Y_train = torch.tensor(Y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "becoming-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.h0 = torch.zeros(self.n_layers, 1, self.hidden_dim).to(device)\n",
    "        self.c0 = torch.zeros(self.n_layers, 1, self.hidden_dim).to(device)\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)  \n",
    "        \n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        # hidden = self.init_hidden(batch_size)\n",
    "        self.h0 = torch.zeros(self.n_layers, 1, self.hidden_dim).to(device)\n",
    "        self.c0 = torch.zeros(self.n_layers, 1, self.hidden_dim).to(device)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.lstm(x, (self.h0, self.c0))\n",
    "        \n",
    "        out = self.sigmoid1(out)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cognitive-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size=20400, output_size=2, hidden_dim=200, n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "private-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.cuda()\n",
    "X_train = X_train.cuda() \n",
    "Y_train = Y_train.cuda()\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 5000\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "rental-japan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120, 20400])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sealed-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-avenue",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Run\n",
    "\n",
    "# model.train()\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "while(True):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    X_train.to(device)\n",
    "    output, hidden = model(X_train)\n",
    "    # print(output)\n",
    "    loss = criterion(output, Y_train.view(-1,2).float())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordinglyw\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        loss_arr.append(loss.item())\n",
    "        acc = float((torch.sum((torch.argmax(output, axis=1) == torch.argmax(Y_train.squeeze(), axis=1)).int())/120))\n",
    "        print('Epoch: {}/{}.............'.format(epoch, 5000), end=' ')\n",
    "        print(\"Loss: {:.15f} Acc: {:.15f}\".format(loss.item(), acc))\n",
    "        \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592147ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loss_arr = np.array(loss_arr)\n",
    "np.savez(\"Myfile\", loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ab67be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for loss\n",
    "plt.plot(loss_arr)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "liquid-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-149-e7d8c41a385c>:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "model.eval() # eval mode\n",
    "out, hidden = model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "australian-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0->tensor([0.9290, 0.0710], grad_fn=<SelectBackward>)\n",
      "1->tensor([0.5556, 0.4444], grad_fn=<SelectBackward>)\n",
      "2->tensor([0.4703, 0.5297], grad_fn=<SelectBackward>)\n",
      "3->tensor([0.4587, 0.5413], grad_fn=<SelectBackward>)\n",
      "4->tensor([0.4571, 0.5429], grad_fn=<SelectBackward>)\n",
      "5->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "6->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "7->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "8->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "9->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "10->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "11->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "12->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "13->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "14->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "15->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "16->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "17->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "18->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "19->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "20->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "21->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "22->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "23->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "24->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "25->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "26->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "27->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "28->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "29->tensor([0.4959, 0.5041], grad_fn=<SelectBackward>)\n",
      "30->tensor([0.4959, 0.5041], grad_fn=<SelectBackward>)\n",
      "31->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "32->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "33->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "34->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "35->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "36->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "37->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "38->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "39->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "40->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "41->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "42->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "43->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "44->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "45->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "46->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "47->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "48->tensor([0.2942, 0.7058], grad_fn=<SelectBackward>)\n",
      "49->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "50->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "51->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "52->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "53->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "54->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "55->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "56->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "57->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "58->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "59->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "60->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "61->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "62->tensor([0.1910, 0.8090], grad_fn=<SelectBackward>)\n",
      "63->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "64->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "65->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "66->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "67->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "68->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "69->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "70->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "71->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "72->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "73->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "74->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "75->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "76->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "77->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "78->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "79->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "80->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "81->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "82->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "83->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "84->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "85->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "86->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "87->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "88->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "89->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "90->tensor([0.8596, 0.1404], grad_fn=<SelectBackward>)\n",
      "91->tensor([0.8326, 0.1674], grad_fn=<SelectBackward>)\n",
      "92->tensor([0.4581, 0.5419], grad_fn=<SelectBackward>)\n",
      "93->tensor([0.4570, 0.5430], grad_fn=<SelectBackward>)\n",
      "94->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "95->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "96->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "97->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "98->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "99->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "100->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "101->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "102->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "103->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "104->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "105->tensor([0.7588, 0.2412], grad_fn=<SelectBackward>)\n",
      "106->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "107->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "108->tensor([0.4569, 0.5431], grad_fn=<SelectBackward>)\n",
      "109->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "110->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "111->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "112->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "113->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "114->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "115->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "116->tensor([0.8360, 0.1640], grad_fn=<SelectBackward>)\n",
      "117->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "118->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n",
      "119->tensor([0.9478, 0.0522], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(120):\n",
    "    print(str(i) + \"->\" + str(out[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "worst-denver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for loss\n",
    "plt.plot(loss_arr)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_dir, best_model_dir):\n",
    "    f_path = checkpoint_dir + '/' + 'checkpoint.pt'\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir + '/' + 'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': n_epochs + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}\n",
    "model_path = r\"/home/ryo/Desktop/tcc/workspace/2-train_lstm/\"\n",
    "save_ckp(checkpoint, True, model_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-detection",
   "metadata": {},
   "source": [
    "## Trained with sigmoid output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "civil-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000/25000............. Loss: 0.222982302308083 Acc: 0.591666638851166\n",
      "Epoch: 5010/25000............. Loss: 0.222971484065056 Acc: 0.591666638851166\n",
      "Epoch: 5020/25000............. Loss: 0.222966954112053 Acc: 0.591666638851166\n",
      "Epoch: 5030/25000............. Loss: 0.222961366176605 Acc: 0.591666638851166\n",
      "Epoch: 5040/25000............. Loss: 0.222956269979477 Acc: 0.591666638851166\n",
      "Epoch: 5050/25000............. Loss: 0.222951456904411 Acc: 0.591666638851166\n",
      "Epoch: 5060/25000............. Loss: 0.222946539521217 Acc: 0.591666638851166\n",
      "Epoch: 5070/25000............. Loss: 0.222941935062408 Acc: 0.591666638851166\n",
      "Epoch: 5080/25000............. Loss: 0.222959235310555 Acc: 0.591666638851166\n",
      "Epoch: 5090/25000............. Loss: 0.223307505249977 Acc: 0.591666638851166\n",
      "Epoch: 5100/25000............. Loss: 0.222927808761597 Acc: 0.591666638851166\n",
      "Epoch: 5110/25000............. Loss: 0.222968384623528 Acc: 0.600000023841858\n",
      "Epoch: 5120/25000............. Loss: 0.222917661070824 Acc: 0.591666638851166\n",
      "Epoch: 5130/25000............. Loss: 0.222919002175331 Acc: 0.591666638851166\n",
      "Epoch: 5140/25000............. Loss: 0.222908362746239 Acc: 0.591666638851166\n",
      "Epoch: 5150/25000............. Loss: 0.222903877496719 Acc: 0.591666638851166\n",
      "Epoch: 5160/25000............. Loss: 0.222899302840233 Acc: 0.591666638851166\n",
      "Epoch: 5170/25000............. Loss: 0.222919523715973 Acc: 0.591666638851166\n",
      "Epoch: 5180/25000............. Loss: 0.223224416375160 Acc: 0.591666638851166\n",
      "Epoch: 5190/25000............. Loss: 0.222885563969612 Acc: 0.591666638851166\n",
      "Epoch: 5200/25000............. Loss: 0.222924068570137 Acc: 0.600000023841858\n",
      "Epoch: 5210/25000............. Loss: 0.222876444458961 Acc: 0.591666638851166\n",
      "Epoch: 5220/25000............. Loss: 0.222876578569412 Acc: 0.591666638851166\n",
      "Epoch: 5230/25000............. Loss: 0.222866266965866 Acc: 0.591666638851166\n",
      "Epoch: 5240/25000............. Loss: 0.222862303256989 Acc: 0.591666638851166\n",
      "Epoch: 5250/25000............. Loss: 0.222857311367989 Acc: 0.591666638851166\n",
      "Epoch: 5260/25000............. Loss: 0.222852483391762 Acc: 0.591666638851166\n",
      "Epoch: 5270/25000............. Loss: 0.222847968339920 Acc: 0.591666638851166\n",
      "Epoch: 5280/25000............. Loss: 0.222853019833565 Acc: 0.591666638851166\n",
      "Epoch: 5290/25000............. Loss: 0.223340511322021 Acc: 0.591666638851166\n",
      "Epoch: 5300/25000............. Loss: 0.222915783524513 Acc: 0.600000023841858\n",
      "Epoch: 5310/25000............. Loss: 0.222864434123039 Acc: 0.600000023841858\n",
      "Epoch: 5320/25000............. Loss: 0.222829818725586 Acc: 0.591666638851166\n",
      "Epoch: 5330/25000............. Loss: 0.222826793789864 Acc: 0.591666638851166\n",
      "Epoch: 5340/25000............. Loss: 0.222816899418831 Acc: 0.600000023841858\n",
      "Epoch: 5350/25000............. Loss: 0.222812175750732 Acc: 0.600000023841858\n",
      "Epoch: 5360/25000............. Loss: 0.222808614373207 Acc: 0.591666638851166\n",
      "Epoch: 5370/25000............. Loss: 0.222867682576180 Acc: 0.600000023841858\n",
      "Epoch: 5380/25000............. Loss: 0.222844809293747 Acc: 0.600000023841858\n",
      "Epoch: 5390/25000............. Loss: 0.222823798656464 Acc: 0.600000023841858\n",
      "Epoch: 5400/25000............. Loss: 0.222817808389664 Acc: 0.591666638851166\n",
      "Epoch: 5410/25000............. Loss: 0.222785189747810 Acc: 0.591666638851166\n",
      "Epoch: 5420/25000............. Loss: 0.222784280776978 Acc: 0.600000023841858\n",
      "Epoch: 5430/25000............. Loss: 0.222777813673019 Acc: 0.591666638851166\n",
      "Epoch: 5440/25000............. Loss: 0.222772225737572 Acc: 0.591666638851166\n",
      "Epoch: 5450/25000............. Loss: 0.222767695784569 Acc: 0.591666638851166\n",
      "Epoch: 5460/25000............. Loss: 0.222763314843178 Acc: 0.591666638851166\n",
      "Epoch: 5470/25000............. Loss: 0.222758963704109 Acc: 0.591666638851166\n",
      "Epoch: 5480/25000............. Loss: 0.222756356000900 Acc: 0.600000023841858\n",
      "Epoch: 5490/25000............. Loss: 0.222891941666603 Acc: 0.600000023841858\n",
      "Epoch: 5500/25000............. Loss: 0.222779035568237 Acc: 0.591666638851166\n",
      "Epoch: 5510/25000............. Loss: 0.222819671034813 Acc: 0.600000023841858\n",
      "Epoch: 5520/25000............. Loss: 0.222737386822701 Acc: 0.600000023841858\n",
      "Epoch: 5530/25000............. Loss: 0.222743362188339 Acc: 0.591666638851166\n",
      "Epoch: 5540/25000............. Loss: 0.222730919718742 Acc: 0.600000023841858\n",
      "Epoch: 5550/25000............. Loss: 0.222837701439857 Acc: 0.600000023841858\n",
      "Epoch: 5560/25000............. Loss: 0.222722262144089 Acc: 0.591666638851166\n",
      "Epoch: 5570/25000............. Loss: 0.222779452800751 Acc: 0.600000023841858\n",
      "Epoch: 5580/25000............. Loss: 0.222723826766014 Acc: 0.591666638851166\n",
      "Epoch: 5590/25000............. Loss: 0.222709640860558 Acc: 0.591666638851166\n",
      "Epoch: 5600/25000............. Loss: 0.222707211971283 Acc: 0.600000023841858\n",
      "Epoch: 5610/25000............. Loss: 0.222700312733650 Acc: 0.591666638851166\n",
      "Epoch: 5620/25000............. Loss: 0.222695350646973 Acc: 0.600000023841858\n",
      "Epoch: 5630/25000............. Loss: 0.222691103816032 Acc: 0.600000023841858\n",
      "Epoch: 5640/25000............. Loss: 0.222686901688576 Acc: 0.600000023841858\n",
      "Epoch: 5650/25000............. Loss: 0.222682729363441 Acc: 0.600000023841858\n",
      "Epoch: 5660/25000............. Loss: 0.222678512334824 Acc: 0.600000023841858\n",
      "Epoch: 5670/25000............. Loss: 0.222674295306206 Acc: 0.600000023841858\n",
      "Epoch: 5680/25000............. Loss: 0.222670301795006 Acc: 0.591666638851166\n",
      "Epoch: 5690/25000............. Loss: 0.222702115774155 Acc: 0.591666638851166\n",
      "Epoch: 5700/25000............. Loss: 0.222806364297867 Acc: 0.591666638851166\n",
      "Epoch: 5710/25000............. Loss: 0.222796350717545 Acc: 0.591666638851166\n",
      "Epoch: 5720/25000............. Loss: 0.222664728760719 Acc: 0.591666638851166\n",
      "Epoch: 5730/25000............. Loss: 0.222767069935799 Acc: 0.591666638851166\n",
      "Epoch: 5740/25000............. Loss: 0.222660318017006 Acc: 0.600000023841858\n",
      "Epoch: 5750/25000............. Loss: 0.222730830311775 Acc: 0.591666638851166\n",
      "Epoch: 5760/25000............. Loss: 0.222637936472893 Acc: 0.600000023841858\n",
      "Epoch: 5770/25000............. Loss: 0.222645387053490 Acc: 0.600000023841858\n",
      "Epoch: 5780/25000............. Loss: 0.222630351781845 Acc: 0.600000023841858\n",
      "Epoch: 5790/25000............. Loss: 0.222627073526382 Acc: 0.600000023841858\n",
      "Epoch: 5800/25000............. Loss: 0.222622781991959 Acc: 0.600000023841858\n",
      "Epoch: 5810/25000............. Loss: 0.222618401050568 Acc: 0.600000023841858\n",
      "Epoch: 5820/25000............. Loss: 0.222614422440529 Acc: 0.600000023841858\n",
      "Epoch: 5830/25000............. Loss: 0.222610473632812 Acc: 0.600000023841858\n",
      "Epoch: 5840/25000............. Loss: 0.222606524825096 Acc: 0.600000023841858\n",
      "Epoch: 5850/25000............. Loss: 0.222602620720863 Acc: 0.600000023841858\n",
      "Epoch: 5860/25000............. Loss: 0.222598686814308 Acc: 0.600000023841858\n",
      "Epoch: 5870/25000............. Loss: 0.222594693303108 Acc: 0.600000023841858\n",
      "Epoch: 5880/25000............. Loss: 0.222590714693069 Acc: 0.600000023841858\n",
      "Epoch: 5890/25000............. Loss: 0.222586736083031 Acc: 0.600000023841858\n",
      "Epoch: 5900/25000............. Loss: 0.222582772374153 Acc: 0.600000023841858\n",
      "Epoch: 5910/25000............. Loss: 0.222578942775726 Acc: 0.600000023841858\n",
      "Epoch: 5920/25000............. Loss: 0.222584158182144 Acc: 0.591666638851166\n",
      "Epoch: 5930/25000............. Loss: 0.223140716552734 Acc: 0.591666638851166\n",
      "Epoch: 5940/25000............. Loss: 0.222748234868050 Acc: 0.600000023841858\n",
      "Epoch: 5950/25000............. Loss: 0.222643181681633 Acc: 0.591666638851166\n",
      "Epoch: 5960/25000............. Loss: 0.222579628229141 Acc: 0.591666638851166\n",
      "Epoch: 5970/25000............. Loss: 0.222564622759819 Acc: 0.600000023841858\n",
      "Epoch: 5980/25000............. Loss: 0.222556501626968 Acc: 0.600000023841858\n",
      "Epoch: 5990/25000............. Loss: 0.222548708319664 Acc: 0.600000023841858\n",
      "Epoch: 6000/25000............. Loss: 0.222544938325882 Acc: 0.600000023841858\n",
      "Epoch: 6010/25000............. Loss: 0.222541123628616 Acc: 0.600000023841858\n",
      "Epoch: 6020/25000............. Loss: 0.222537234425545 Acc: 0.600000023841858\n",
      "Epoch: 6030/25000............. Loss: 0.222533404827118 Acc: 0.600000023841858\n",
      "Epoch: 6040/25000............. Loss: 0.222529605031013 Acc: 0.600000023841858\n",
      "Epoch: 6050/25000............. Loss: 0.222525805234909 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6060/25000............. Loss: 0.222522020339966 Acc: 0.600000023841858\n",
      "Epoch: 6070/25000............. Loss: 0.222518235445023 Acc: 0.600000023841858\n",
      "Epoch: 6080/25000............. Loss: 0.222514435648918 Acc: 0.600000023841858\n",
      "Epoch: 6090/25000............. Loss: 0.222510620951653 Acc: 0.600000023841858\n",
      "Epoch: 6100/25000............. Loss: 0.222506821155548 Acc: 0.600000023841858\n",
      "Epoch: 6110/25000............. Loss: 0.222506210207939 Acc: 0.600000023841858\n",
      "Epoch: 6120/25000............. Loss: 0.222766846418381 Acc: 0.600000023841858\n",
      "Epoch: 6130/25000............. Loss: 0.223067685961723 Acc: 0.591666638851166\n",
      "Epoch: 6140/25000............. Loss: 0.222714021801949 Acc: 0.600000023841858\n",
      "Epoch: 6150/25000............. Loss: 0.222520321607590 Acc: 0.600000023841858\n",
      "Epoch: 6160/25000............. Loss: 0.222500011324883 Acc: 0.591666638851166\n",
      "Epoch: 6170/25000............. Loss: 0.222490772604942 Acc: 0.591666638851166\n",
      "Epoch: 6180/25000............. Loss: 0.222479000687599 Acc: 0.600000023841858\n",
      "Epoch: 6190/25000............. Loss: 0.222475245594978 Acc: 0.600000023841858\n",
      "Epoch: 6200/25000............. Loss: 0.222470968961716 Acc: 0.600000023841858\n",
      "Epoch: 6210/25000............. Loss: 0.222467228770256 Acc: 0.600000023841858\n",
      "Epoch: 6220/25000............. Loss: 0.222463652491570 Acc: 0.600000023841858\n",
      "Epoch: 6230/25000............. Loss: 0.222460061311722 Acc: 0.600000023841858\n",
      "Epoch: 6240/25000............. Loss: 0.222456455230713 Acc: 0.600000023841858\n",
      "Epoch: 6250/25000............. Loss: 0.222452878952026 Acc: 0.600000023841858\n",
      "Epoch: 6260/25000............. Loss: 0.222449287772179 Acc: 0.600000023841858\n",
      "Epoch: 6270/25000............. Loss: 0.222445666790009 Acc: 0.600000023841858\n",
      "Epoch: 6280/25000............. Loss: 0.222442090511322 Acc: 0.600000023841858\n",
      "Epoch: 6290/25000............. Loss: 0.222438469529152 Acc: 0.600000023841858\n",
      "Epoch: 6300/25000............. Loss: 0.222434833645821 Acc: 0.600000023841858\n",
      "Epoch: 6310/25000............. Loss: 0.222432211041451 Acc: 0.600000023841858\n",
      "Epoch: 6320/25000............. Loss: 0.222528025507927 Acc: 0.591666638851166\n",
      "Epoch: 6330/25000............. Loss: 0.222427666187286 Acc: 0.600000023841858\n",
      "Epoch: 6340/25000............. Loss: 0.222518950700760 Acc: 0.591666638851166\n",
      "Epoch: 6350/25000............. Loss: 0.222507566213608 Acc: 0.600000023841858\n",
      "Epoch: 6360/25000............. Loss: 0.222433432936668 Acc: 0.600000023841858\n",
      "Epoch: 6370/25000............. Loss: 0.222504064440727 Acc: 0.600000023841858\n",
      "Epoch: 6380/25000............. Loss: 0.222442716360092 Acc: 0.600000023841858\n",
      "Epoch: 6390/25000............. Loss: 0.222404092550278 Acc: 0.600000023841858\n",
      "Epoch: 6400/25000............. Loss: 0.222403347492218 Acc: 0.600000023841858\n",
      "Epoch: 6410/25000............. Loss: 0.222398415207863 Acc: 0.600000023841858\n",
      "Epoch: 6420/25000............. Loss: 0.222393572330475 Acc: 0.600000023841858\n",
      "Epoch: 6430/25000............. Loss: 0.222390338778496 Acc: 0.600000023841858\n",
      "Epoch: 6440/25000............. Loss: 0.222386762499809 Acc: 0.600000023841858\n",
      "Epoch: 6450/25000............. Loss: 0.222383320331573 Acc: 0.600000023841858\n",
      "Epoch: 6460/25000............. Loss: 0.222379952669144 Acc: 0.600000023841858\n",
      "Epoch: 6470/25000............. Loss: 0.222376585006714 Acc: 0.600000023841858\n",
      "Epoch: 6480/25000............. Loss: 0.222373157739639 Acc: 0.600000023841858\n",
      "Epoch: 6490/25000............. Loss: 0.222369730472565 Acc: 0.600000023841858\n",
      "Epoch: 6500/25000............. Loss: 0.222366333007812 Acc: 0.600000023841858\n",
      "Epoch: 6510/25000............. Loss: 0.222363412380219 Acc: 0.600000023841858\n",
      "Epoch: 6520/25000............. Loss: 0.222391068935394 Acc: 0.600000023841858\n",
      "Epoch: 6530/25000............. Loss: 0.222604647278786 Acc: 0.600000023841858\n",
      "Epoch: 6540/25000............. Loss: 0.222353935241699 Acc: 0.600000023841858\n",
      "Epoch: 6550/25000............. Loss: 0.222382038831711 Acc: 0.591666638851166\n",
      "Epoch: 6560/25000............. Loss: 0.222351312637329 Acc: 0.600000023841858\n",
      "Epoch: 6570/25000............. Loss: 0.222344145178795 Acc: 0.600000023841858\n",
      "Epoch: 6580/25000............. Loss: 0.222382709383965 Acc: 0.600000023841858\n",
      "Epoch: 6590/25000............. Loss: 0.222489088773727 Acc: 0.600000023841858\n",
      "Epoch: 6600/25000............. Loss: 0.222354754805565 Acc: 0.600000023841858\n",
      "Epoch: 6610/25000............. Loss: 0.222361430525780 Acc: 0.591666638851166\n",
      "Epoch: 6620/25000............. Loss: 0.222328409552574 Acc: 0.600000023841858\n",
      "Epoch: 6630/25000............. Loss: 0.222327932715416 Acc: 0.600000023841858\n",
      "Epoch: 6640/25000............. Loss: 0.222319826483727 Acc: 0.600000023841858\n",
      "Epoch: 6650/25000............. Loss: 0.222316905856133 Acc: 0.600000023841858\n",
      "Epoch: 6660/25000............. Loss: 0.222313538193703 Acc: 0.600000023841858\n",
      "Epoch: 6670/25000............. Loss: 0.222310110926628 Acc: 0.600000023841858\n",
      "Epoch: 6680/25000............. Loss: 0.222306773066521 Acc: 0.600000023841858\n",
      "Epoch: 6690/25000............. Loss: 0.222303777933121 Acc: 0.600000023841858\n",
      "Epoch: 6700/25000............. Loss: 0.222318902611732 Acc: 0.600000023841858\n",
      "Epoch: 6710/25000............. Loss: 0.222686365246773 Acc: 0.600000023841858\n",
      "Epoch: 6720/25000............. Loss: 0.222320050001144 Acc: 0.591666638851166\n",
      "Epoch: 6730/25000............. Loss: 0.222320601344109 Acc: 0.591666638851166\n",
      "Epoch: 6740/25000............. Loss: 0.222294732928276 Acc: 0.600000023841858\n",
      "Epoch: 6750/25000............. Loss: 0.222285747528076 Acc: 0.600000023841858\n",
      "Epoch: 6760/25000............. Loss: 0.222283169627190 Acc: 0.600000023841858\n",
      "Epoch: 6770/25000............. Loss: 0.222285136580467 Acc: 0.600000023841858\n",
      "Epoch: 6780/25000............. Loss: 0.222660496830940 Acc: 0.600000023841858\n",
      "Epoch: 6790/25000............. Loss: 0.222431942820549 Acc: 0.591666638851166\n",
      "Epoch: 6800/25000............. Loss: 0.222268238663673 Acc: 0.600000023841858\n",
      "Epoch: 6810/25000............. Loss: 0.222285434603691 Acc: 0.600000023841858\n",
      "Epoch: 6820/25000............. Loss: 0.222262576222420 Acc: 0.600000023841858\n",
      "Epoch: 6830/25000............. Loss: 0.222260624170303 Acc: 0.600000023841858\n",
      "Epoch: 6840/25000............. Loss: 0.222256705164909 Acc: 0.600000023841858\n",
      "Epoch: 6850/25000............. Loss: 0.222252771258354 Acc: 0.600000023841858\n",
      "Epoch: 6860/25000............. Loss: 0.222249537706375 Acc: 0.600000023841858\n",
      "Epoch: 6870/25000............. Loss: 0.222246363759041 Acc: 0.600000023841858\n",
      "Epoch: 6880/25000............. Loss: 0.222243458032608 Acc: 0.600000023841858\n",
      "Epoch: 6890/25000............. Loss: 0.222255766391754 Acc: 0.600000023841858\n",
      "Epoch: 6900/25000............. Loss: 0.222688928246498 Acc: 0.591666638851166\n",
      "Epoch: 6910/25000............. Loss: 0.222261458635330 Acc: 0.600000023841858\n",
      "Epoch: 6920/25000............. Loss: 0.222270533442497 Acc: 0.600000023841858\n",
      "Epoch: 6930/25000............. Loss: 0.222230598330498 Acc: 0.600000023841858\n",
      "Epoch: 6940/25000............. Loss: 0.222229689359665 Acc: 0.600000023841858\n",
      "Epoch: 6950/25000............. Loss: 0.222223088145256 Acc: 0.600000023841858\n",
      "Epoch: 6960/25000............. Loss: 0.222237139940262 Acc: 0.600000023841858\n",
      "Epoch: 6970/25000............. Loss: 0.222600072622299 Acc: 0.591666638851166\n",
      "Epoch: 6980/25000............. Loss: 0.222259432077408 Acc: 0.600000023841858\n",
      "Epoch: 6990/25000............. Loss: 0.222223415970802 Acc: 0.600000023841858\n",
      "Epoch: 7000/25000............. Loss: 0.222220614552498 Acc: 0.600000023841858\n",
      "Epoch: 7010/25000............. Loss: 0.222203627228737 Acc: 0.600000023841858\n",
      "Epoch: 7020/25000............. Loss: 0.222201630473137 Acc: 0.600000023841858\n",
      "Epoch: 7030/25000............. Loss: 0.222198322415352 Acc: 0.600000023841858\n",
      "Epoch: 7040/25000............. Loss: 0.222194775938988 Acc: 0.600000023841858\n",
      "Epoch: 7050/25000............. Loss: 0.222191587090492 Acc: 0.600000023841858\n",
      "Epoch: 7060/25000............. Loss: 0.222188517451286 Acc: 0.600000023841858\n",
      "Epoch: 7070/25000............. Loss: 0.222185447812080 Acc: 0.600000023841858\n",
      "Epoch: 7080/25000............. Loss: 0.222182437777519 Acc: 0.600000023841858\n",
      "Epoch: 7090/25000............. Loss: 0.222179487347603 Acc: 0.600000023841858\n",
      "Epoch: 7100/25000............. Loss: 0.222194224596024 Acc: 0.600000023841858\n",
      "Epoch: 7110/25000............. Loss: 0.222627505660057 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7120/25000............. Loss: 0.222235515713692 Acc: 0.600000023841858\n",
      "Epoch: 7130/25000............. Loss: 0.222168862819672 Acc: 0.600000023841858\n",
      "Epoch: 7140/25000............. Loss: 0.222202494740486 Acc: 0.591666638851166\n",
      "Epoch: 7150/25000............. Loss: 0.222489103674889 Acc: 0.591666638851166\n",
      "Epoch: 7160/25000............. Loss: 0.222169443964958 Acc: 0.600000023841858\n",
      "Epoch: 7170/25000............. Loss: 0.222185447812080 Acc: 0.600000023841858\n",
      "Epoch: 7180/25000............. Loss: 0.222161069512367 Acc: 0.600000023841858\n",
      "Epoch: 7190/25000............. Loss: 0.222150996327400 Acc: 0.600000023841858\n",
      "Epoch: 7200/25000............. Loss: 0.222149357199669 Acc: 0.600000023841858\n",
      "Epoch: 7210/25000............. Loss: 0.222145229578018 Acc: 0.600000023841858\n",
      "Epoch: 7220/25000............. Loss: 0.222141876816750 Acc: 0.600000023841858\n",
      "Epoch: 7230/25000............. Loss: 0.222138971090317 Acc: 0.600000023841858\n",
      "Epoch: 7240/25000............. Loss: 0.222136080265045 Acc: 0.600000023841858\n",
      "Epoch: 7250/25000............. Loss: 0.222133234143257 Acc: 0.600000023841858\n",
      "Epoch: 7260/25000............. Loss: 0.222130358219147 Acc: 0.600000023841858\n",
      "Epoch: 7270/25000............. Loss: 0.222127452492714 Acc: 0.600000023841858\n",
      "Epoch: 7280/25000............. Loss: 0.222124576568604 Acc: 0.600000023841858\n",
      "Epoch: 7290/25000............. Loss: 0.222121670842171 Acc: 0.600000023841858\n",
      "Epoch: 7300/25000............. Loss: 0.222118765115738 Acc: 0.600000023841858\n",
      "Epoch: 7310/25000............. Loss: 0.222116172313690 Acc: 0.600000023841858\n",
      "Epoch: 7320/25000............. Loss: 0.222130209207535 Acc: 0.600000023841858\n",
      "Epoch: 7330/25000............. Loss: 0.222506135702133 Acc: 0.591666638851166\n",
      "Epoch: 7340/25000............. Loss: 0.222172692418098 Acc: 0.600000023841858\n",
      "Epoch: 7350/25000............. Loss: 0.222546488046646 Acc: 0.600000023841858\n",
      "Epoch: 7360/25000............. Loss: 0.222201660275459 Acc: 0.591666638851166\n",
      "Epoch: 7370/25000............. Loss: 0.222112149000168 Acc: 0.600000023841858\n",
      "Epoch: 7380/25000............. Loss: 0.222112119197845 Acc: 0.600000023841858\n",
      "Epoch: 7390/25000............. Loss: 0.222094431519508 Acc: 0.600000023841858\n",
      "Epoch: 7400/25000............. Loss: 0.222093164920807 Acc: 0.600000023841858\n",
      "Epoch: 7410/25000............. Loss: 0.222088500857353 Acc: 0.600000023841858\n",
      "Epoch: 7420/25000............. Loss: 0.222085192799568 Acc: 0.600000023841858\n",
      "Epoch: 7430/25000............. Loss: 0.222082436084747 Acc: 0.600000023841858\n",
      "Epoch: 7440/25000............. Loss: 0.222079649567604 Acc: 0.600000023841858\n",
      "Epoch: 7450/25000............. Loss: 0.222076863050461 Acc: 0.600000023841858\n",
      "Epoch: 7460/25000............. Loss: 0.222074061632156 Acc: 0.600000023841858\n",
      "Epoch: 7470/25000............. Loss: 0.222071275115013 Acc: 0.600000023841858\n",
      "Epoch: 7480/25000............. Loss: 0.222068518400192 Acc: 0.600000023841858\n",
      "Epoch: 7490/25000............. Loss: 0.222066313028336 Acc: 0.600000023841858\n",
      "Epoch: 7500/25000............. Loss: 0.222095146775246 Acc: 0.600000023841858\n",
      "Epoch: 7510/25000............. Loss: 0.222306936979294 Acc: 0.591666638851166\n",
      "Epoch: 7520/25000............. Loss: 0.222062677145004 Acc: 0.600000023841858\n",
      "Epoch: 7530/25000............. Loss: 0.222087517380714 Acc: 0.600000023841858\n",
      "Epoch: 7540/25000............. Loss: 0.222515866160393 Acc: 0.591666638851166\n",
      "Epoch: 7550/25000............. Loss: 0.222169399261475 Acc: 0.600000023841858\n",
      "Epoch: 7560/25000............. Loss: 0.222059324383736 Acc: 0.600000023841858\n",
      "Epoch: 7570/25000............. Loss: 0.222059905529022 Acc: 0.600000023841858\n",
      "Epoch: 7580/25000............. Loss: 0.222042754292488 Acc: 0.600000023841858\n",
      "Epoch: 7590/25000............. Loss: 0.222041338682175 Acc: 0.600000023841858\n",
      "Epoch: 7600/25000............. Loss: 0.222036153078079 Acc: 0.600000023841858\n",
      "Epoch: 7610/25000............. Loss: 0.222033441066742 Acc: 0.600000023841858\n",
      "Epoch: 7620/25000............. Loss: 0.222030833363533 Acc: 0.600000023841858\n",
      "Epoch: 7630/25000............. Loss: 0.222028061747551 Acc: 0.600000023841858\n",
      "Epoch: 7640/25000............. Loss: 0.222025364637375 Acc: 0.600000023841858\n",
      "Epoch: 7650/25000............. Loss: 0.222022697329521 Acc: 0.600000023841858\n",
      "Epoch: 7660/25000............. Loss: 0.222020596265793 Acc: 0.600000023841858\n",
      "Epoch: 7670/25000............. Loss: 0.222042962908745 Acc: 0.600000023841858\n",
      "Epoch: 7680/25000............. Loss: 0.222302660346031 Acc: 0.600000023841858\n",
      "Epoch: 7690/25000............. Loss: 0.222047865390778 Acc: 0.600000023841858\n",
      "Epoch: 7700/25000............. Loss: 0.222014516592026 Acc: 0.600000023841858\n",
      "Epoch: 7710/25000............. Loss: 0.222019299864769 Acc: 0.600000023841858\n",
      "Epoch: 7720/25000............. Loss: 0.222007110714912 Acc: 0.600000023841858\n",
      "Epoch: 7730/25000............. Loss: 0.222001746296883 Acc: 0.600000023841858\n",
      "Epoch: 7740/25000............. Loss: 0.222028657793999 Acc: 0.600000023841858\n",
      "Epoch: 7750/25000............. Loss: 0.222214460372925 Acc: 0.600000023841858\n",
      "Epoch: 7760/25000............. Loss: 0.222113266587257 Acc: 0.600000023841858\n",
      "Epoch: 7770/25000............. Loss: 0.221994340419769 Acc: 0.600000023841858\n",
      "Epoch: 7780/25000............. Loss: 0.221993833780289 Acc: 0.600000023841858\n",
      "Epoch: 7790/25000............. Loss: 0.221992820501328 Acc: 0.600000023841858\n",
      "Epoch: 7800/25000............. Loss: 0.221984252333641 Acc: 0.600000023841858\n",
      "Epoch: 7810/25000............. Loss: 0.221981152892113 Acc: 0.600000023841858\n",
      "Epoch: 7820/25000............. Loss: 0.221978664398193 Acc: 0.600000023841858\n",
      "Epoch: 7830/25000............. Loss: 0.221979632973671 Acc: 0.600000023841858\n",
      "Epoch: 7840/25000............. Loss: 0.222177281975746 Acc: 0.600000023841858\n",
      "Epoch: 7850/25000............. Loss: 0.222047567367554 Acc: 0.591666638851166\n",
      "Epoch: 7860/25000............. Loss: 0.222025841474533 Acc: 0.600000023841858\n",
      "Epoch: 7870/25000............. Loss: 0.221966415643692 Acc: 0.600000023841858\n",
      "Epoch: 7880/25000............. Loss: 0.221967786550522 Acc: 0.600000023841858\n",
      "Epoch: 7890/25000............. Loss: 0.221963554620743 Acc: 0.600000023841858\n",
      "Epoch: 7900/25000............. Loss: 0.221958681941032 Acc: 0.600000023841858\n",
      "Epoch: 7910/25000............. Loss: 0.221955671906471 Acc: 0.600000023841858\n",
      "Epoch: 7920/25000............. Loss: 0.221953108906746 Acc: 0.600000023841858\n",
      "Epoch: 7930/25000............. Loss: 0.221950560808182 Acc: 0.600000023841858\n",
      "Epoch: 7940/25000............. Loss: 0.221948012709618 Acc: 0.600000023841858\n",
      "Epoch: 7950/25000............. Loss: 0.221945479512215 Acc: 0.600000023841858\n",
      "Epoch: 7960/25000............. Loss: 0.221942931413651 Acc: 0.600000023841858\n",
      "Epoch: 7970/25000............. Loss: 0.221940562129021 Acc: 0.600000023841858\n",
      "Epoch: 7980/25000............. Loss: 0.221956565976143 Acc: 0.600000023841858\n",
      "Epoch: 7990/25000............. Loss: 0.222352653741837 Acc: 0.600000023841858\n",
      "Epoch: 8000/25000............. Loss: 0.221933782100677 Acc: 0.600000023841858\n",
      "Epoch: 8010/25000............. Loss: 0.222033634781837 Acc: 0.591666638851166\n",
      "Epoch: 8020/25000............. Loss: 0.222013756632805 Acc: 0.591666638851166\n",
      "Epoch: 8030/25000............. Loss: 0.221958667039871 Acc: 0.600000023841858\n",
      "Epoch: 8040/25000............. Loss: 0.221952274441719 Acc: 0.600000023841858\n",
      "Epoch: 8050/25000............. Loss: 0.221922114491463 Acc: 0.600000023841858\n",
      "Epoch: 8060/25000............. Loss: 0.221922829747200 Acc: 0.600000023841858\n",
      "Epoch: 8070/25000............. Loss: 0.221916511654854 Acc: 0.600000023841858\n",
      "Epoch: 8080/25000............. Loss: 0.221913471817970 Acc: 0.600000023841858\n",
      "Epoch: 8090/25000............. Loss: 0.221911132335663 Acc: 0.600000023841858\n",
      "Epoch: 8100/25000............. Loss: 0.221908643841743 Acc: 0.600000023841858\n",
      "Epoch: 8110/25000............. Loss: 0.221906200051308 Acc: 0.600000023841858\n",
      "Epoch: 8120/25000............. Loss: 0.221903756260872 Acc: 0.600000023841858\n",
      "Epoch: 8130/25000............. Loss: 0.221901342272758 Acc: 0.600000023841858\n",
      "Epoch: 8140/25000............. Loss: 0.221898868680000 Acc: 0.600000023841858\n",
      "Epoch: 8150/25000............. Loss: 0.221896395087242 Acc: 0.600000023841858\n",
      "Epoch: 8160/25000............. Loss: 0.221893951296806 Acc: 0.600000023841858\n",
      "Epoch: 8170/25000............. Loss: 0.221891507506371 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8180/25000............. Loss: 0.221890777349472 Acc: 0.600000023841858\n",
      "Epoch: 8190/25000............. Loss: 0.222008615732193 Acc: 0.600000023841858\n",
      "Epoch: 8200/25000............. Loss: 0.222262576222420 Acc: 0.600000023841858\n",
      "Epoch: 8210/25000............. Loss: 0.222068160772324 Acc: 0.600000023841858\n",
      "Epoch: 8220/25000............. Loss: 0.221882626414299 Acc: 0.600000023841858\n",
      "Epoch: 8230/25000............. Loss: 0.221898943185806 Acc: 0.600000023841858\n",
      "Epoch: 8240/25000............. Loss: 0.221880972385406 Acc: 0.600000023841858\n",
      "Epoch: 8250/25000............. Loss: 0.221872508525848 Acc: 0.600000023841858\n",
      "Epoch: 8260/25000............. Loss: 0.221870645880699 Acc: 0.600000023841858\n",
      "Epoch: 8270/25000............. Loss: 0.221868142485619 Acc: 0.600000023841858\n",
      "Epoch: 8280/25000............. Loss: 0.221865579485893 Acc: 0.600000023841858\n",
      "Epoch: 8290/25000............. Loss: 0.221863105893135 Acc: 0.600000023841858\n",
      "Epoch: 8300/25000............. Loss: 0.221860736608505 Acc: 0.600000023841858\n",
      "Epoch: 8310/25000............. Loss: 0.221858367323875 Acc: 0.600000023841858\n",
      "Epoch: 8320/25000............. Loss: 0.221856012940407 Acc: 0.600000023841858\n",
      "Epoch: 8330/25000............. Loss: 0.221853643655777 Acc: 0.600000023841858\n",
      "Epoch: 8340/25000............. Loss: 0.221851289272308 Acc: 0.600000023841858\n",
      "Epoch: 8350/25000............. Loss: 0.221849128603935 Acc: 0.600000023841858\n",
      "Epoch: 8360/25000............. Loss: 0.221855729818344 Acc: 0.600000023841858\n",
      "Epoch: 8370/25000............. Loss: 0.222191721200943 Acc: 0.591666638851166\n",
      "Epoch: 8380/25000............. Loss: 0.222369328141212 Acc: 0.600000023841858\n",
      "Epoch: 8390/25000............. Loss: 0.221952110528946 Acc: 0.600000023841858\n",
      "Epoch: 8400/25000............. Loss: 0.221841618418694 Acc: 0.600000023841858\n",
      "Epoch: 8410/25000............. Loss: 0.221856683492661 Acc: 0.600000023841858\n",
      "Epoch: 8420/25000............. Loss: 0.221836969256401 Acc: 0.600000023841858\n",
      "Epoch: 8430/25000............. Loss: 0.221830710768700 Acc: 0.600000023841858\n",
      "Epoch: 8440/25000............. Loss: 0.221828594803810 Acc: 0.600000023841858\n",
      "Epoch: 8450/25000............. Loss: 0.221826225519180 Acc: 0.600000023841858\n",
      "Epoch: 8460/25000............. Loss: 0.221823766827583 Acc: 0.600000023841858\n",
      "Epoch: 8470/25000............. Loss: 0.221821382641792 Acc: 0.600000023841858\n",
      "Epoch: 8480/25000............. Loss: 0.221819058060646 Acc: 0.600000023841858\n",
      "Epoch: 8490/25000............. Loss: 0.221816763281822 Acc: 0.600000023841858\n",
      "Epoch: 8500/25000............. Loss: 0.221814468502998 Acc: 0.600000023841858\n",
      "Epoch: 8510/25000............. Loss: 0.221812158823013 Acc: 0.600000023841858\n",
      "Epoch: 8520/25000............. Loss: 0.221810445189476 Acc: 0.600000023841858\n",
      "Epoch: 8530/25000............. Loss: 0.221838712692261 Acc: 0.600000023841858\n",
      "Epoch: 8540/25000............. Loss: 0.222062379121780 Acc: 0.591666638851166\n",
      "Epoch: 8550/25000............. Loss: 0.221970826387405 Acc: 0.600000023841858\n",
      "Epoch: 8560/25000............. Loss: 0.221855506300926 Acc: 0.600000023841858\n",
      "Epoch: 8570/25000............. Loss: 0.221874967217445 Acc: 0.600000023841858\n",
      "Epoch: 8580/25000............. Loss: 0.221801176667213 Acc: 0.600000023841858\n",
      "Epoch: 8590/25000............. Loss: 0.221797630190849 Acc: 0.600000023841858\n",
      "Epoch: 8600/25000............. Loss: 0.221795648336411 Acc: 0.600000023841858\n",
      "Epoch: 8610/25000............. Loss: 0.221790745854378 Acc: 0.600000023841858\n",
      "Epoch: 8620/25000............. Loss: 0.221787735819817 Acc: 0.600000023841858\n",
      "Epoch: 8630/25000............. Loss: 0.221785381436348 Acc: 0.600000023841858\n",
      "Epoch: 8640/25000............. Loss: 0.221783161163330 Acc: 0.600000023841858\n",
      "Epoch: 8650/25000............. Loss: 0.221780940890312 Acc: 0.600000023841858\n",
      "Epoch: 8660/25000............. Loss: 0.221778675913811 Acc: 0.600000023841858\n",
      "Epoch: 8670/25000............. Loss: 0.221776440739632 Acc: 0.600000023841858\n",
      "Epoch: 8680/25000............. Loss: 0.221774205565453 Acc: 0.600000023841858\n",
      "Epoch: 8690/25000............. Loss: 0.221772029995918 Acc: 0.600000023841858\n",
      "Epoch: 8700/25000............. Loss: 0.221774742007256 Acc: 0.600000023841858\n",
      "Epoch: 8710/25000............. Loss: 0.222087636590004 Acc: 0.600000023841858\n",
      "Epoch: 8720/25000............. Loss: 0.222029864788055 Acc: 0.591666638851166\n",
      "Epoch: 8730/25000............. Loss: 0.221775487065315 Acc: 0.600000023841858\n",
      "Epoch: 8740/25000............. Loss: 0.221820443868637 Acc: 0.600000023841858\n",
      "Epoch: 8750/25000............. Loss: 0.221784278750420 Acc: 0.600000023841858\n",
      "Epoch: 8760/25000............. Loss: 0.221757724881172 Acc: 0.600000023841858\n",
      "Epoch: 8770/25000............. Loss: 0.221757784485817 Acc: 0.600000023841858\n",
      "Epoch: 8780/25000............. Loss: 0.221754059195518 Acc: 0.600000023841858\n",
      "Epoch: 8790/25000............. Loss: 0.221750795841217 Acc: 0.600000023841858\n",
      "Epoch: 8800/25000............. Loss: 0.221748381853104 Acc: 0.600000023841858\n",
      "Epoch: 8810/25000............. Loss: 0.221746191382408 Acc: 0.600000023841858\n",
      "Epoch: 8820/25000............. Loss: 0.221744000911713 Acc: 0.600000023841858\n",
      "Epoch: 8830/25000............. Loss: 0.221741810441017 Acc: 0.600000023841858\n",
      "Epoch: 8840/25000............. Loss: 0.221739634871483 Acc: 0.600000023841858\n",
      "Epoch: 8850/25000............. Loss: 0.221737459301949 Acc: 0.600000023841858\n",
      "Epoch: 8860/25000............. Loss: 0.221735298633575 Acc: 0.600000023841858\n",
      "Epoch: 8870/25000............. Loss: 0.221733152866364 Acc: 0.600000023841858\n",
      "Epoch: 8880/25000............. Loss: 0.221731886267662 Acc: 0.600000023841858\n",
      "Epoch: 8890/25000............. Loss: 0.221782773733139 Acc: 0.600000023841858\n",
      "Epoch: 8900/25000............. Loss: 0.221912831068039 Acc: 0.600000023841858\n",
      "Epoch: 8910/25000............. Loss: 0.221735656261444 Acc: 0.600000023841858\n",
      "Epoch: 8920/25000............. Loss: 0.221852019429207 Acc: 0.591666638851166\n",
      "Epoch: 8930/25000............. Loss: 0.221722319722176 Acc: 0.600000023841858\n",
      "Epoch: 8940/25000............. Loss: 0.221731662750244 Acc: 0.600000023841858\n",
      "Epoch: 8950/25000............. Loss: 0.221718057990074 Acc: 0.600000023841858\n",
      "Epoch: 8960/25000............. Loss: 0.221716389060020 Acc: 0.600000023841858\n",
      "Epoch: 8970/25000............. Loss: 0.221712455153465 Acc: 0.600000023841858\n",
      "Epoch: 8980/25000............. Loss: 0.221710234880447 Acc: 0.600000023841858\n",
      "Epoch: 8990/25000............. Loss: 0.221708193421364 Acc: 0.600000023841858\n",
      "Epoch: 9000/25000............. Loss: 0.221706062555313 Acc: 0.600000023841858\n",
      "Epoch: 9010/25000............. Loss: 0.221703976392746 Acc: 0.600000023841858\n",
      "Epoch: 9020/25000............. Loss: 0.221701875329018 Acc: 0.600000023841858\n",
      "Epoch: 9030/25000............. Loss: 0.221699744462967 Acc: 0.600000023841858\n",
      "Epoch: 9040/25000............. Loss: 0.221697658300400 Acc: 0.600000023841858\n",
      "Epoch: 9050/25000............. Loss: 0.221695631742477 Acc: 0.600000023841858\n",
      "Epoch: 9060/25000............. Loss: 0.221696436405182 Acc: 0.600000023841858\n",
      "Epoch: 9070/25000............. Loss: 0.221895828843117 Acc: 0.591666638851166\n",
      "Epoch: 9080/25000............. Loss: 0.221781224012375 Acc: 0.600000023841858\n",
      "Epoch: 9090/25000............. Loss: 0.221742942929268 Acc: 0.600000023841858\n",
      "Epoch: 9100/25000............. Loss: 0.221687421202660 Acc: 0.600000023841858\n",
      "Epoch: 9110/25000............. Loss: 0.221716389060020 Acc: 0.600000023841858\n",
      "Epoch: 9120/25000............. Loss: 0.221999689936638 Acc: 0.600000023841858\n",
      "Epoch: 9130/25000............. Loss: 0.221753835678101 Acc: 0.600000023841858\n",
      "Epoch: 9140/25000............. Loss: 0.221679016947746 Acc: 0.600000023841858\n",
      "Epoch: 9150/25000............. Loss: 0.221690237522125 Acc: 0.600000023841858\n",
      "Epoch: 9160/25000............. Loss: 0.221678733825684 Acc: 0.600000023841858\n",
      "Epoch: 9170/25000............. Loss: 0.221671283245087 Acc: 0.600000023841858\n",
      "Epoch: 9180/25000............. Loss: 0.221670150756836 Acc: 0.600000023841858\n",
      "Epoch: 9190/25000............. Loss: 0.221667259931564 Acc: 0.600000023841858\n",
      "Epoch: 9200/25000............. Loss: 0.221665397286415 Acc: 0.600000023841858\n",
      "Epoch: 9210/25000............. Loss: 0.221663281321526 Acc: 0.600000023841858\n",
      "Epoch: 9220/25000............. Loss: 0.221661299467087 Acc: 0.600000023841858\n",
      "Epoch: 9230/25000............. Loss: 0.221659258008003 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9240/25000............. Loss: 0.221657305955887 Acc: 0.600000023841858\n",
      "Epoch: 9250/25000............. Loss: 0.221659258008003 Acc: 0.600000023841858\n",
      "Epoch: 9260/25000............. Loss: 0.221991509199142 Acc: 0.591666638851166\n",
      "Epoch: 9270/25000............. Loss: 0.221845626831055 Acc: 0.600000023841858\n",
      "Epoch: 9280/25000............. Loss: 0.221649646759033 Acc: 0.600000023841858\n",
      "Epoch: 9290/25000............. Loss: 0.221671774983406 Acc: 0.600000023841858\n",
      "Epoch: 9300/25000............. Loss: 0.221645519137383 Acc: 0.600000023841858\n",
      "Epoch: 9310/25000............. Loss: 0.221646606922150 Acc: 0.600000023841858\n",
      "Epoch: 9320/25000............. Loss: 0.221641510725021 Acc: 0.600000023841858\n",
      "Epoch: 9330/25000............. Loss: 0.221639975905418 Acc: 0.600000023841858\n",
      "Epoch: 9340/25000............. Loss: 0.221655905246735 Acc: 0.600000023841858\n",
      "Epoch: 9350/25000............. Loss: 0.222022995352745 Acc: 0.600000023841858\n",
      "Epoch: 9360/25000............. Loss: 0.221662938594818 Acc: 0.600000023841858\n",
      "Epoch: 9370/25000............. Loss: 0.221659600734711 Acc: 0.600000023841858\n",
      "Epoch: 9380/25000............. Loss: 0.221638098359108 Acc: 0.600000023841858\n",
      "Epoch: 9390/25000............. Loss: 0.221629053354263 Acc: 0.600000023841858\n",
      "Epoch: 9400/25000............. Loss: 0.221628025174141 Acc: 0.600000023841858\n",
      "Epoch: 9410/25000............. Loss: 0.221624225378036 Acc: 0.600000023841858\n",
      "Epoch: 9420/25000............. Loss: 0.221621975302696 Acc: 0.600000023841858\n",
      "Epoch: 9430/25000............. Loss: 0.221620038151741 Acc: 0.600000023841858\n",
      "Epoch: 9440/25000............. Loss: 0.221618101000786 Acc: 0.600000023841858\n",
      "Epoch: 9450/25000............. Loss: 0.221616268157959 Acc: 0.600000023841858\n",
      "Epoch: 9460/25000............. Loss: 0.221628189086914 Acc: 0.600000023841858\n",
      "Epoch: 9470/25000............. Loss: 0.222099363803864 Acc: 0.600000023841858\n",
      "Epoch: 9480/25000............. Loss: 0.221619173884392 Acc: 0.600000023841858\n",
      "Epoch: 9490/25000............. Loss: 0.221663162112236 Acc: 0.600000023841858\n",
      "Epoch: 9500/25000............. Loss: 0.221608415246010 Acc: 0.600000023841858\n",
      "Epoch: 9510/25000............. Loss: 0.221609935164452 Acc: 0.600000023841858\n",
      "Epoch: 9520/25000............. Loss: 0.221604183316231 Acc: 0.600000023841858\n",
      "Epoch: 9530/25000............. Loss: 0.221661821007729 Acc: 0.600000023841858\n",
      "Epoch: 9540/25000............. Loss: 0.221652492880821 Acc: 0.600000023841858\n",
      "Epoch: 9550/25000............. Loss: 0.221639603376389 Acc: 0.600000023841858\n",
      "Epoch: 9560/25000............. Loss: 0.221617907285690 Acc: 0.600000023841858\n",
      "Epoch: 9570/25000............. Loss: 0.221595719456673 Acc: 0.600000023841858\n",
      "Epoch: 9580/25000............. Loss: 0.221595898270607 Acc: 0.600000023841858\n",
      "Epoch: 9590/25000............. Loss: 0.221589803695679 Acc: 0.600000023841858\n",
      "Epoch: 9600/25000............. Loss: 0.221587851643562 Acc: 0.600000023841858\n",
      "Epoch: 9610/25000............. Loss: 0.221586003899574 Acc: 0.600000023841858\n",
      "Epoch: 9620/25000............. Loss: 0.221584051847458 Acc: 0.600000023841858\n",
      "Epoch: 9630/25000............. Loss: 0.221582114696503 Acc: 0.600000023841858\n",
      "Epoch: 9640/25000............. Loss: 0.221580222249031 Acc: 0.600000023841858\n",
      "Epoch: 9650/25000............. Loss: 0.221578314900398 Acc: 0.600000023841858\n",
      "Epoch: 9660/25000............. Loss: 0.221576854586601 Acc: 0.600000023841858\n",
      "Epoch: 9670/25000............. Loss: 0.221601650118828 Acc: 0.600000023841858\n",
      "Epoch: 9680/25000............. Loss: 0.221870243549347 Acc: 0.591666638851166\n",
      "Epoch: 9690/25000............. Loss: 0.221576124429703 Acc: 0.600000023841858\n",
      "Epoch: 9700/25000............. Loss: 0.221600189805031 Acc: 0.600000023841858\n",
      "Epoch: 9710/25000............. Loss: 0.221573546528816 Acc: 0.600000023841858\n",
      "Epoch: 9720/25000............. Loss: 0.221593156456947 Acc: 0.600000023841858\n",
      "Epoch: 9730/25000............. Loss: 0.221859142184258 Acc: 0.600000023841858\n",
      "Epoch: 9740/25000............. Loss: 0.221570387482643 Acc: 0.600000023841858\n",
      "Epoch: 9750/25000............. Loss: 0.221598178148270 Acc: 0.600000023841858\n",
      "Epoch: 9760/25000............. Loss: 0.221562549471855 Acc: 0.600000023841858\n",
      "Epoch: 9770/25000............. Loss: 0.221560165286064 Acc: 0.600000023841858\n",
      "Epoch: 9780/25000............. Loss: 0.221554711461067 Acc: 0.600000023841858\n",
      "Epoch: 9790/25000............. Loss: 0.221553280949593 Acc: 0.600000023841858\n",
      "Epoch: 9800/25000............. Loss: 0.221550747752190 Acc: 0.600000023841858\n",
      "Epoch: 9810/25000............. Loss: 0.221548855304718 Acc: 0.600000023841858\n",
      "Epoch: 9820/25000............. Loss: 0.221547037363052 Acc: 0.600000023841858\n",
      "Epoch: 9830/25000............. Loss: 0.221545204520226 Acc: 0.600000023841858\n",
      "Epoch: 9840/25000............. Loss: 0.221543461084366 Acc: 0.600000023841858\n",
      "Epoch: 9850/25000............. Loss: 0.221547096967697 Acc: 0.600000023841858\n",
      "Epoch: 9860/25000............. Loss: 0.221901699900627 Acc: 0.591666638851166\n",
      "Epoch: 9870/25000............. Loss: 0.221711218357086 Acc: 0.600000023841858\n",
      "Epoch: 9880/25000............. Loss: 0.221536979079247 Acc: 0.600000023841858\n",
      "Epoch: 9890/25000............. Loss: 0.221555754542351 Acc: 0.600000023841858\n",
      "Epoch: 9900/25000............. Loss: 0.221533074975014 Acc: 0.600000023841858\n",
      "Epoch: 9910/25000............. Loss: 0.221532821655273 Acc: 0.600000023841858\n",
      "Epoch: 9920/25000............. Loss: 0.221534863114357 Acc: 0.600000023841858\n",
      "Epoch: 9930/25000............. Loss: 0.221862152218819 Acc: 0.600000023841858\n",
      "Epoch: 9940/25000............. Loss: 0.221703395247459 Acc: 0.600000023841858\n",
      "Epoch: 9950/25000............. Loss: 0.221526771783829 Acc: 0.600000023841858\n",
      "Epoch: 9960/25000............. Loss: 0.221542418003082 Acc: 0.600000023841858\n",
      "Epoch: 9970/25000............. Loss: 0.221521288156509 Acc: 0.600000023841858\n",
      "Epoch: 9980/25000............. Loss: 0.221520021557808 Acc: 0.600000023841858\n",
      "Epoch: 9990/25000............. Loss: 0.221517562866211 Acc: 0.600000023841858\n",
      "Epoch: 10000/25000............. Loss: 0.221514850854874 Acc: 0.600000023841858\n",
      "Epoch: 10010/25000............. Loss: 0.221513003110886 Acc: 0.600000023841858\n",
      "Epoch: 10020/25000............. Loss: 0.221511259675026 Acc: 0.600000023841858\n",
      "Epoch: 10030/25000............. Loss: 0.221509456634521 Acc: 0.600000023841858\n",
      "Epoch: 10040/25000............. Loss: 0.221508413553238 Acc: 0.600000023841858\n",
      "Epoch: 10050/25000............. Loss: 0.221569150686264 Acc: 0.600000023841858\n",
      "Epoch: 10060/25000............. Loss: 0.221537157893181 Acc: 0.600000023841858\n",
      "Epoch: 10070/25000............. Loss: 0.221572250127792 Acc: 0.600000023841858\n",
      "Epoch: 10080/25000............. Loss: 0.221509695053101 Acc: 0.600000023841858\n",
      "Epoch: 10090/25000............. Loss: 0.221508756279945 Acc: 0.600000023841858\n",
      "Epoch: 10100/25000............. Loss: 0.221498861908913 Acc: 0.600000023841858\n",
      "Epoch: 10110/25000............. Loss: 0.221496969461441 Acc: 0.600000023841858\n",
      "Epoch: 10120/25000............. Loss: 0.221579849720001 Acc: 0.600000023841858\n",
      "Epoch: 10130/25000............. Loss: 0.221493273973465 Acc: 0.600000023841858\n",
      "Epoch: 10140/25000............. Loss: 0.221574708819389 Acc: 0.600000023841858\n",
      "Epoch: 10150/25000............. Loss: 0.221490770578384 Acc: 0.600000023841858\n",
      "Epoch: 10160/25000............. Loss: 0.221497952938080 Acc: 0.600000023841858\n",
      "Epoch: 10170/25000............. Loss: 0.221485957503319 Acc: 0.600000023841858\n",
      "Epoch: 10180/25000............. Loss: 0.221484199166298 Acc: 0.600000023841858\n",
      "Epoch: 10190/25000............. Loss: 0.221482306718826 Acc: 0.600000023841858\n",
      "Epoch: 10200/25000............. Loss: 0.221480101346970 Acc: 0.600000023841858\n",
      "Epoch: 10210/25000............. Loss: 0.221478298306465 Acc: 0.600000023841858\n",
      "Epoch: 10220/25000............. Loss: 0.221476584672928 Acc: 0.600000023841858\n",
      "Epoch: 10230/25000............. Loss: 0.221475034952164 Acc: 0.600000023841858\n",
      "Epoch: 10240/25000............. Loss: 0.221481561660767 Acc: 0.600000023841858\n",
      "Epoch: 10250/25000............. Loss: 0.221832737326622 Acc: 0.600000023841858\n",
      "Epoch: 10260/25000............. Loss: 0.221615627408028 Acc: 0.600000023841858\n",
      "Epoch: 10270/25000............. Loss: 0.221470504999161 Acc: 0.600000023841858\n",
      "Epoch: 10280/25000............. Loss: 0.221480295062065 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10290/25000............. Loss: 0.221468925476074 Acc: 0.600000023841858\n",
      "Epoch: 10300/25000............. Loss: 0.221462890505791 Acc: 0.600000023841858\n",
      "Epoch: 10310/25000............. Loss: 0.221462666988373 Acc: 0.600000023841858\n",
      "Epoch: 10320/25000............. Loss: 0.221567213535309 Acc: 0.600000023841858\n",
      "Epoch: 10330/25000............. Loss: 0.221469983458519 Acc: 0.600000023841858\n",
      "Epoch: 10340/25000............. Loss: 0.221548095345497 Acc: 0.600000023841858\n",
      "Epoch: 10350/25000............. Loss: 0.221459627151489 Acc: 0.600000023841858\n",
      "Epoch: 10360/25000............. Loss: 0.221461370587349 Acc: 0.600000023841858\n",
      "Epoch: 10370/25000............. Loss: 0.221452653408051 Acc: 0.600000023841858\n",
      "Epoch: 10380/25000............. Loss: 0.221450805664062 Acc: 0.600000023841858\n",
      "Epoch: 10390/25000............. Loss: 0.221447885036469 Acc: 0.600000023841858\n",
      "Epoch: 10400/25000............. Loss: 0.221446350216866 Acc: 0.600000023841858\n",
      "Epoch: 10410/25000............. Loss: 0.221444591879845 Acc: 0.600000023841858\n",
      "Epoch: 10420/25000............. Loss: 0.221443131566048 Acc: 0.600000023841858\n",
      "Epoch: 10430/25000............. Loss: 0.221472159028053 Acc: 0.600000023841858\n",
      "Epoch: 10440/25000............. Loss: 0.221667602658272 Acc: 0.600000023841858\n",
      "Epoch: 10450/25000............. Loss: 0.221494734287262 Acc: 0.600000023841858\n",
      "Epoch: 10460/25000............. Loss: 0.221445634961128 Acc: 0.600000023841858\n",
      "Epoch: 10470/25000............. Loss: 0.221451699733734 Acc: 0.600000023841858\n",
      "Epoch: 10480/25000............. Loss: 0.221433371305466 Acc: 0.600000023841858\n",
      "Epoch: 10490/25000............. Loss: 0.221433252096176 Acc: 0.600000023841858\n",
      "Epoch: 10500/25000............. Loss: 0.221429795026779 Acc: 0.600000023841858\n",
      "Epoch: 10510/25000............. Loss: 0.221428409218788 Acc: 0.600000023841858\n",
      "Epoch: 10520/25000............. Loss: 0.221430316567421 Acc: 0.600000023841858\n",
      "Epoch: 10530/25000............. Loss: 0.221727088093758 Acc: 0.591666638851166\n",
      "Epoch: 10540/25000............. Loss: 0.221599578857422 Acc: 0.600000023841858\n",
      "Epoch: 10550/25000............. Loss: 0.221429973840714 Acc: 0.600000023841858\n",
      "Epoch: 10560/25000............. Loss: 0.221441745758057 Acc: 0.600000023841858\n",
      "Epoch: 10570/25000............. Loss: 0.221419468522072 Acc: 0.600000023841858\n",
      "Epoch: 10580/25000............. Loss: 0.221418991684914 Acc: 0.600000023841858\n",
      "Epoch: 10590/25000............. Loss: 0.221415981650352 Acc: 0.600000023841858\n",
      "Epoch: 10600/25000............. Loss: 0.221413567662239 Acc: 0.600000023841858\n",
      "Epoch: 10610/25000............. Loss: 0.221411958336830 Acc: 0.600000023841858\n",
      "Epoch: 10620/25000............. Loss: 0.221410349011421 Acc: 0.600000023841858\n",
      "Epoch: 10630/25000............. Loss: 0.221408694982529 Acc: 0.600000023841858\n",
      "Epoch: 10640/25000............. Loss: 0.221407353878021 Acc: 0.600000023841858\n",
      "Epoch: 10650/25000............. Loss: 0.221433073282242 Acc: 0.600000023841858\n",
      "Epoch: 10660/25000............. Loss: 0.221686631441116 Acc: 0.600000023841858\n",
      "Epoch: 10670/25000............. Loss: 0.221412748098373 Acc: 0.600000023841858\n",
      "Epoch: 10680/25000............. Loss: 0.221437349915504 Acc: 0.600000023841858\n",
      "Epoch: 10690/25000............. Loss: 0.221404299139977 Acc: 0.600000023841858\n",
      "Epoch: 10700/25000............. Loss: 0.221401184797287 Acc: 0.600000023841858\n",
      "Epoch: 10710/25000............. Loss: 0.221396356821060 Acc: 0.600000023841858\n",
      "Epoch: 10720/25000............. Loss: 0.221398159861565 Acc: 0.600000023841858\n",
      "Epoch: 10730/25000............. Loss: 0.221697643399239 Acc: 0.591666638851166\n",
      "Epoch: 10740/25000............. Loss: 0.221590593457222 Acc: 0.600000023841858\n",
      "Epoch: 10750/25000............. Loss: 0.221390992403030 Acc: 0.600000023841858\n",
      "Epoch: 10760/25000............. Loss: 0.221414268016815 Acc: 0.600000023841858\n",
      "Epoch: 10770/25000............. Loss: 0.221387103199959 Acc: 0.600000023841858\n",
      "Epoch: 10780/25000............. Loss: 0.221388101577759 Acc: 0.600000023841858\n",
      "Epoch: 10790/25000............. Loss: 0.221383333206177 Acc: 0.600000023841858\n",
      "Epoch: 10800/25000............. Loss: 0.221382170915604 Acc: 0.600000023841858\n",
      "Epoch: 10810/25000............. Loss: 0.221380293369293 Acc: 0.600000023841858\n",
      "Epoch: 10820/25000............. Loss: 0.221378609538078 Acc: 0.600000023841858\n",
      "Epoch: 10830/25000............. Loss: 0.221377044916153 Acc: 0.600000023841858\n",
      "Epoch: 10840/25000............. Loss: 0.221375465393066 Acc: 0.600000023841858\n",
      "Epoch: 10850/25000............. Loss: 0.221374183893204 Acc: 0.600000023841858\n",
      "Epoch: 10860/25000............. Loss: 0.221407786011696 Acc: 0.600000023841858\n",
      "Epoch: 10870/25000............. Loss: 0.221553176641464 Acc: 0.600000023841858\n",
      "Epoch: 10880/25000............. Loss: 0.221435606479645 Acc: 0.600000023841858\n",
      "Epoch: 10890/25000............. Loss: 0.221374854445457 Acc: 0.600000023841858\n",
      "Epoch: 10900/25000............. Loss: 0.221382513642311 Acc: 0.600000023841858\n",
      "Epoch: 10910/25000............. Loss: 0.221364870667458 Acc: 0.600000023841858\n",
      "Epoch: 10920/25000............. Loss: 0.221364974975586 Acc: 0.600000023841858\n",
      "Epoch: 10930/25000............. Loss: 0.221362307667732 Acc: 0.600000023841858\n",
      "Epoch: 10940/25000............. Loss: 0.221457555890083 Acc: 0.600000023841858\n",
      "Epoch: 10950/25000............. Loss: 0.221367359161377 Acc: 0.600000023841858\n",
      "Epoch: 10960/25000............. Loss: 0.221453055739403 Acc: 0.600000023841858\n",
      "Epoch: 10970/25000............. Loss: 0.221369281411171 Acc: 0.600000023841858\n",
      "Epoch: 10980/25000............. Loss: 0.221357554197311 Acc: 0.600000023841858\n",
      "Epoch: 10990/25000............. Loss: 0.221356689929962 Acc: 0.600000023841858\n",
      "Epoch: 11000/25000............. Loss: 0.221351042389870 Acc: 0.600000023841858\n",
      "Epoch: 11010/25000............. Loss: 0.221349939703941 Acc: 0.600000023841858\n",
      "Epoch: 11020/25000............. Loss: 0.221348017454147 Acc: 0.600000023841858\n",
      "Epoch: 11030/25000............. Loss: 0.221346363425255 Acc: 0.600000023841858\n",
      "Epoch: 11040/25000............. Loss: 0.221344858407974 Acc: 0.600000023841858\n",
      "Epoch: 11050/25000............. Loss: 0.221343353390694 Acc: 0.600000023841858\n",
      "Epoch: 11060/25000............. Loss: 0.221341863274574 Acc: 0.600000023841858\n",
      "Epoch: 11070/25000............. Loss: 0.221343949437141 Acc: 0.600000023841858\n",
      "Epoch: 11080/25000............. Loss: 0.221598193049431 Acc: 0.600000023841858\n",
      "Epoch: 11090/25000............. Loss: 0.221489399671555 Acc: 0.600000023841858\n",
      "Epoch: 11100/25000............. Loss: 0.221360415220261 Acc: 0.600000023841858\n",
      "Epoch: 11110/25000............. Loss: 0.221347585320473 Acc: 0.600000023841858\n",
      "Epoch: 11120/25000............. Loss: 0.221337795257568 Acc: 0.600000023841858\n",
      "Epoch: 11130/25000............. Loss: 0.221331581473351 Acc: 0.600000023841858\n",
      "Epoch: 11140/25000............. Loss: 0.221330836415291 Acc: 0.600000023841858\n",
      "Epoch: 11150/25000............. Loss: 0.221329301595688 Acc: 0.600000023841858\n",
      "Epoch: 11160/25000............. Loss: 0.221420064568520 Acc: 0.600000023841858\n",
      "Epoch: 11170/25000............. Loss: 0.221328407526016 Acc: 0.600000023841858\n",
      "Epoch: 11180/25000............. Loss: 0.221425607800484 Acc: 0.600000023841858\n",
      "Epoch: 11190/25000............. Loss: 0.221329748630524 Acc: 0.600000023841858\n",
      "Epoch: 11200/25000............. Loss: 0.221327602863312 Acc: 0.600000023841858\n",
      "Epoch: 11210/25000............. Loss: 0.221322342753410 Acc: 0.600000023841858\n",
      "Epoch: 11220/25000............. Loss: 0.221318557858467 Acc: 0.600000023841858\n",
      "Epoch: 11230/25000............. Loss: 0.221316635608673 Acc: 0.600000023841858\n",
      "Epoch: 11240/25000............. Loss: 0.221315160393715 Acc: 0.600000023841858\n",
      "Epoch: 11250/25000............. Loss: 0.221313625574112 Acc: 0.600000023841858\n",
      "Epoch: 11260/25000............. Loss: 0.221319064497948 Acc: 0.600000023841858\n",
      "Epoch: 11270/25000............. Loss: 0.221660569310188 Acc: 0.591666638851166\n",
      "Epoch: 11280/25000............. Loss: 0.221461266279221 Acc: 0.600000023841858\n",
      "Epoch: 11290/25000............. Loss: 0.221313983201981 Acc: 0.600000023841858\n",
      "Epoch: 11300/25000............. Loss: 0.221319153904915 Acc: 0.600000023841858\n",
      "Epoch: 11310/25000............. Loss: 0.221309617161751 Acc: 0.600000023841858\n",
      "Epoch: 11320/25000............. Loss: 0.221303209662437 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11330/25000............. Loss: 0.221302226185799 Acc: 0.600000023841858\n",
      "Epoch: 11340/25000............. Loss: 0.221300631761551 Acc: 0.600000023841858\n",
      "Epoch: 11350/25000............. Loss: 0.221298933029175 Acc: 0.600000023841858\n",
      "Epoch: 11360/25000............. Loss: 0.221297800540924 Acc: 0.600000023841858\n",
      "Epoch: 11370/25000............. Loss: 0.221328720450401 Acc: 0.600000023841858\n",
      "Epoch: 11380/25000............. Loss: 0.221521124243736 Acc: 0.600000023841858\n",
      "Epoch: 11390/25000............. Loss: 0.221298620104790 Acc: 0.600000023841858\n",
      "Epoch: 11400/25000............. Loss: 0.221330612897873 Acc: 0.600000023841858\n",
      "Epoch: 11410/25000............. Loss: 0.221290335059166 Acc: 0.600000023841858\n",
      "Epoch: 11420/25000............. Loss: 0.221293941140175 Acc: 0.600000023841858\n",
      "Epoch: 11430/25000............. Loss: 0.221288263797760 Acc: 0.600000023841858\n",
      "Epoch: 11440/25000............. Loss: 0.221319451928139 Acc: 0.600000023841858\n",
      "Epoch: 11450/25000............. Loss: 0.221511691808701 Acc: 0.600000023841858\n",
      "Epoch: 11460/25000............. Loss: 0.221293702721596 Acc: 0.600000023841858\n",
      "Epoch: 11470/25000............. Loss: 0.221295341849327 Acc: 0.600000023841858\n",
      "Epoch: 11480/25000............. Loss: 0.221292093396187 Acc: 0.600000023841858\n",
      "Epoch: 11490/25000............. Loss: 0.221280142664909 Acc: 0.600000023841858\n",
      "Epoch: 11500/25000............. Loss: 0.221277236938477 Acc: 0.600000023841858\n",
      "Epoch: 11510/25000............. Loss: 0.221275866031647 Acc: 0.600000023841858\n",
      "Epoch: 11520/25000............. Loss: 0.221274420619011 Acc: 0.600000023841858\n",
      "Epoch: 11530/25000............. Loss: 0.221272915601730 Acc: 0.600000023841858\n",
      "Epoch: 11540/25000............. Loss: 0.221271514892578 Acc: 0.600000023841858\n",
      "Epoch: 11550/25000............. Loss: 0.221270367503166 Acc: 0.600000023841858\n",
      "Epoch: 11560/25000............. Loss: 0.221285954117775 Acc: 0.600000023841858\n",
      "Epoch: 11570/25000............. Loss: 0.221651419997215 Acc: 0.600000023841858\n",
      "Epoch: 11580/25000............. Loss: 0.221309855580330 Acc: 0.600000023841858\n",
      "Epoch: 11590/25000............. Loss: 0.221282988786697 Acc: 0.600000023841858\n",
      "Epoch: 11600/25000............. Loss: 0.221277683973312 Acc: 0.600000023841858\n",
      "Epoch: 11610/25000............. Loss: 0.221413552761078 Acc: 0.600000023841858\n",
      "Epoch: 11620/25000............. Loss: 0.221293434500694 Acc: 0.600000023841858\n",
      "Epoch: 11630/25000............. Loss: 0.221329987049103 Acc: 0.600000023841858\n",
      "Epoch: 11640/25000............. Loss: 0.221259772777557 Acc: 0.600000023841858\n",
      "Epoch: 11650/25000............. Loss: 0.221261635422707 Acc: 0.600000023841858\n",
      "Epoch: 11660/25000............. Loss: 0.221257627010345 Acc: 0.600000023841858\n",
      "Epoch: 11670/25000............. Loss: 0.221253439784050 Acc: 0.600000023841858\n",
      "Epoch: 11680/25000............. Loss: 0.221251890063286 Acc: 0.600000023841858\n",
      "Epoch: 11690/25000............. Loss: 0.221250504255295 Acc: 0.600000023841858\n",
      "Epoch: 11700/25000............. Loss: 0.221249073743820 Acc: 0.600000023841858\n",
      "Epoch: 11710/25000............. Loss: 0.221247643232346 Acc: 0.600000023841858\n",
      "Epoch: 11720/25000............. Loss: 0.221246242523193 Acc: 0.600000023841858\n",
      "Epoch: 11730/25000............. Loss: 0.221244856715202 Acc: 0.600000023841858\n",
      "Epoch: 11740/25000............. Loss: 0.221245139837265 Acc: 0.600000023841858\n",
      "Epoch: 11750/25000............. Loss: 0.221386238932610 Acc: 0.600000023841858\n",
      "Epoch: 11760/25000............. Loss: 0.221283748745918 Acc: 0.600000023841858\n",
      "Epoch: 11770/25000............. Loss: 0.221314415335655 Acc: 0.600000023841858\n",
      "Epoch: 11780/25000............. Loss: 0.221242338418961 Acc: 0.600000023841858\n",
      "Epoch: 11790/25000............. Loss: 0.221420451998711 Acc: 0.600000023841858\n",
      "Epoch: 11800/25000............. Loss: 0.221301466226578 Acc: 0.600000023841858\n",
      "Epoch: 11810/25000............. Loss: 0.221301123499870 Acc: 0.600000023841858\n",
      "Epoch: 11820/25000............. Loss: 0.221234366297722 Acc: 0.600000023841858\n",
      "Epoch: 11830/25000............. Loss: 0.221240699291229 Acc: 0.600000023841858\n",
      "Epoch: 11840/25000............. Loss: 0.221230030059814 Acc: 0.600000023841858\n",
      "Epoch: 11850/25000............. Loss: 0.221229031682014 Acc: 0.600000023841858\n",
      "Epoch: 11860/25000............. Loss: 0.221227571368217 Acc: 0.600000023841858\n",
      "Epoch: 11870/25000............. Loss: 0.221225902438164 Acc: 0.600000023841858\n",
      "Epoch: 11880/25000............. Loss: 0.221224442124367 Acc: 0.600000023841858\n",
      "Epoch: 11890/25000............. Loss: 0.221223056316376 Acc: 0.600000023841858\n",
      "Epoch: 11900/25000............. Loss: 0.221221700310707 Acc: 0.600000023841858\n",
      "Epoch: 11910/25000............. Loss: 0.221220299601555 Acc: 0.600000023841858\n",
      "Epoch: 11920/25000............. Loss: 0.221218928694725 Acc: 0.600000023841858\n",
      "Epoch: 11930/25000............. Loss: 0.221218332648277 Acc: 0.600000023841858\n",
      "Epoch: 11940/25000............. Loss: 0.221262291073799 Acc: 0.600000023841858\n",
      "Epoch: 11950/25000............. Loss: 0.221339538693428 Acc: 0.600000023841858\n",
      "Epoch: 11960/25000............. Loss: 0.221222370862961 Acc: 0.600000023841858\n",
      "Epoch: 11970/25000............. Loss: 0.221253171563148 Acc: 0.600000023841858\n",
      "Epoch: 11980/25000............. Loss: 0.221557721495628 Acc: 0.600000023841858\n",
      "Epoch: 11990/25000............. Loss: 0.221371978521347 Acc: 0.600000023841858\n",
      "Epoch: 12000/25000............. Loss: 0.221214011311531 Acc: 0.600000023841858\n",
      "Epoch: 12010/25000............. Loss: 0.221222653985023 Acc: 0.600000023841858\n",
      "Epoch: 12020/25000............. Loss: 0.221209242939949 Acc: 0.600000023841858\n",
      "Epoch: 12030/25000............. Loss: 0.221204414963722 Acc: 0.600000023841858\n",
      "Epoch: 12040/25000............. Loss: 0.221203714609146 Acc: 0.600000023841858\n",
      "Epoch: 12050/25000............. Loss: 0.221201956272125 Acc: 0.600000023841858\n",
      "Epoch: 12060/25000............. Loss: 0.221200391650200 Acc: 0.600000023841858\n",
      "Epoch: 12070/25000............. Loss: 0.221198976039886 Acc: 0.600000023841858\n",
      "Epoch: 12080/25000............. Loss: 0.221197620034218 Acc: 0.600000023841858\n",
      "Epoch: 12090/25000............. Loss: 0.221196278929710 Acc: 0.600000023841858\n",
      "Epoch: 12100/25000............. Loss: 0.221194997429848 Acc: 0.600000023841858\n",
      "Epoch: 12110/25000............. Loss: 0.221195474267006 Acc: 0.600000023841858\n",
      "Epoch: 12120/25000............. Loss: 0.221301212906837 Acc: 0.600000023841858\n",
      "Epoch: 12130/25000............. Loss: 0.221191421151161 Acc: 0.600000023841858\n",
      "Epoch: 12140/25000............. Loss: 0.221243306994438 Acc: 0.600000023841858\n",
      "Epoch: 12150/25000............. Loss: 0.221204802393913 Acc: 0.600000023841858\n",
      "Epoch: 12160/25000............. Loss: 0.221190109848976 Acc: 0.600000023841858\n",
      "Epoch: 12170/25000............. Loss: 0.221520647406578 Acc: 0.600000023841858\n",
      "Epoch: 12180/25000............. Loss: 0.221418619155884 Acc: 0.600000023841858\n",
      "Epoch: 12190/25000............. Loss: 0.221188291907310 Acc: 0.600000023841858\n",
      "Epoch: 12200/25000............. Loss: 0.221197888255119 Acc: 0.600000023841858\n",
      "Epoch: 12210/25000............. Loss: 0.221188694238663 Acc: 0.600000023841858\n",
      "Epoch: 12220/25000............. Loss: 0.221179440617561 Acc: 0.600000023841858\n",
      "Epoch: 12230/25000............. Loss: 0.221179261803627 Acc: 0.600000023841858\n",
      "Epoch: 12240/25000............. Loss: 0.221176713705063 Acc: 0.600000023841858\n",
      "Epoch: 12250/25000............. Loss: 0.221175506711006 Acc: 0.600000023841858\n",
      "Epoch: 12260/25000............. Loss: 0.221174195408821 Acc: 0.600000023841858\n",
      "Epoch: 12270/25000............. Loss: 0.221172809600830 Acc: 0.600000023841858\n",
      "Epoch: 12280/25000............. Loss: 0.221171766519547 Acc: 0.600000023841858\n",
      "Epoch: 12290/25000............. Loss: 0.221185714006424 Acc: 0.600000023841858\n",
      "Epoch: 12300/25000............. Loss: 0.221600770950317 Acc: 0.600000023841858\n",
      "Epoch: 12310/25000............. Loss: 0.221208289265633 Acc: 0.600000023841858\n",
      "Epoch: 12320/25000............. Loss: 0.221194937825203 Acc: 0.600000023841858\n",
      "Epoch: 12330/25000............. Loss: 0.221172958612442 Acc: 0.600000023841858\n",
      "Epoch: 12340/25000............. Loss: 0.221165955066681 Acc: 0.600000023841858\n",
      "Epoch: 12350/25000............. Loss: 0.221164703369141 Acc: 0.600000023841858\n",
      "Epoch: 12360/25000............. Loss: 0.221161410212517 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12370/25000............. Loss: 0.221160277724266 Acc: 0.600000023841858\n",
      "Epoch: 12380/25000............. Loss: 0.221172511577606 Acc: 0.600000023841858\n",
      "Epoch: 12390/25000............. Loss: 0.221562817692757 Acc: 0.600000023841858\n",
      "Epoch: 12400/25000............. Loss: 0.221232399344444 Acc: 0.600000023841858\n",
      "Epoch: 12410/25000............. Loss: 0.221165210008621 Acc: 0.600000023841858\n",
      "Epoch: 12420/25000............. Loss: 0.221169099211693 Acc: 0.600000023841858\n",
      "Epoch: 12430/25000............. Loss: 0.221152439713478 Acc: 0.600000023841858\n",
      "Epoch: 12440/25000............. Loss: 0.221152722835541 Acc: 0.600000023841858\n",
      "Epoch: 12450/25000............. Loss: 0.221150711178780 Acc: 0.600000023841858\n",
      "Epoch: 12460/25000............. Loss: 0.221148774027824 Acc: 0.600000023841858\n",
      "Epoch: 12470/25000............. Loss: 0.221147373318672 Acc: 0.600000023841858\n",
      "Epoch: 12480/25000............. Loss: 0.221146106719971 Acc: 0.600000023841858\n",
      "Epoch: 12490/25000............. Loss: 0.221145570278168 Acc: 0.600000023841858\n",
      "Epoch: 12500/25000............. Loss: 0.221303045749664 Acc: 0.600000023841858\n",
      "Epoch: 12510/25000............. Loss: 0.221326455473900 Acc: 0.600000023841858\n",
      "Epoch: 12520/25000............. Loss: 0.221141725778580 Acc: 0.600000023841858\n",
      "Epoch: 12530/25000............. Loss: 0.221156701445580 Acc: 0.600000023841858\n",
      "Epoch: 12540/25000............. Loss: 0.221152469515800 Acc: 0.600000023841858\n",
      "Epoch: 12550/25000............. Loss: 0.221141427755356 Acc: 0.600000023841858\n",
      "Epoch: 12560/25000............. Loss: 0.221136674284935 Acc: 0.600000023841858\n",
      "Epoch: 12570/25000............. Loss: 0.221179738640785 Acc: 0.600000023841858\n",
      "Epoch: 12580/25000............. Loss: 0.221248343586922 Acc: 0.600000023841858\n",
      "Epoch: 12590/25000............. Loss: 0.221182912588120 Acc: 0.600000023841858\n",
      "Epoch: 12600/25000............. Loss: 0.221147865056992 Acc: 0.600000023841858\n",
      "Epoch: 12610/25000............. Loss: 0.221140086650848 Acc: 0.600000023841858\n",
      "Epoch: 12620/25000............. Loss: 0.221130549907684 Acc: 0.600000023841858\n",
      "Epoch: 12630/25000............. Loss: 0.221128568053246 Acc: 0.600000023841858\n",
      "Epoch: 12640/25000............. Loss: 0.221127077937126 Acc: 0.600000023841858\n",
      "Epoch: 12650/25000............. Loss: 0.221125274896622 Acc: 0.600000023841858\n",
      "Epoch: 12660/25000............. Loss: 0.221124038100243 Acc: 0.600000023841858\n",
      "Epoch: 12670/25000............. Loss: 0.221122831106186 Acc: 0.600000023841858\n",
      "Epoch: 12680/25000............. Loss: 0.221121594309807 Acc: 0.600000023841858\n",
      "Epoch: 12690/25000............. Loss: 0.221120372414589 Acc: 0.600000023841858\n",
      "Epoch: 12700/25000............. Loss: 0.221119135618210 Acc: 0.600000023841858\n",
      "Epoch: 12710/25000............. Loss: 0.221117898821831 Acc: 0.600000023841858\n",
      "Epoch: 12720/25000............. Loss: 0.221116617321968 Acc: 0.600000023841858\n",
      "Epoch: 12730/25000............. Loss: 0.221115455031395 Acc: 0.600000023841858\n",
      "Epoch: 12740/25000............. Loss: 0.221118003129959 Acc: 0.600000023841858\n",
      "Epoch: 12750/25000............. Loss: 0.221385493874550 Acc: 0.600000023841858\n",
      "Epoch: 12760/25000............. Loss: 0.221263498067856 Acc: 0.600000023841858\n",
      "Epoch: 12770/25000............. Loss: 0.221140801906586 Acc: 0.600000023841858\n",
      "Epoch: 12780/25000............. Loss: 0.221561223268509 Acc: 0.600000023841858\n",
      "Epoch: 12790/25000............. Loss: 0.221248418092728 Acc: 0.600000023841858\n",
      "Epoch: 12800/25000............. Loss: 0.221120700240135 Acc: 0.600000023841858\n",
      "Epoch: 12810/25000............. Loss: 0.221120893955231 Acc: 0.600000023841858\n",
      "Epoch: 12820/25000............. Loss: 0.221107333898544 Acc: 0.600000023841858\n",
      "Epoch: 12830/25000............. Loss: 0.221105664968491 Acc: 0.600000023841858\n",
      "Epoch: 12840/25000............. Loss: 0.221102192997932 Acc: 0.600000023841858\n",
      "Epoch: 12850/25000............. Loss: 0.221101284027100 Acc: 0.600000023841858\n",
      "Epoch: 12860/25000............. Loss: 0.221099942922592 Acc: 0.600000023841858\n",
      "Epoch: 12870/25000............. Loss: 0.221098646521568 Acc: 0.600000023841858\n",
      "Epoch: 12880/25000............. Loss: 0.221097424626350 Acc: 0.600000023841858\n",
      "Epoch: 12890/25000............. Loss: 0.221096232533455 Acc: 0.600000023841858\n",
      "Epoch: 12900/25000............. Loss: 0.221095010638237 Acc: 0.600000023841858\n",
      "Epoch: 12910/25000............. Loss: 0.221093803644180 Acc: 0.600000023841858\n",
      "Epoch: 12920/25000............. Loss: 0.221092596650124 Acc: 0.600000023841858\n",
      "Epoch: 12930/25000............. Loss: 0.221092015504837 Acc: 0.600000023841858\n",
      "Epoch: 12940/25000............. Loss: 0.221154049038887 Acc: 0.600000023841858\n",
      "Epoch: 12950/25000............. Loss: 0.221111357212067 Acc: 0.600000023841858\n",
      "Epoch: 12960/25000............. Loss: 0.221179947257042 Acc: 0.600000023841858\n",
      "Epoch: 12970/25000............. Loss: 0.221092626452446 Acc: 0.600000023841858\n",
      "Epoch: 12980/25000............. Loss: 0.221461862325668 Acc: 0.600000023841858\n",
      "Epoch: 12990/25000............. Loss: 0.221262127161026 Acc: 0.600000023841858\n",
      "Epoch: 13000/25000............. Loss: 0.221085101366043 Acc: 0.600000023841858\n",
      "Epoch: 13010/25000............. Loss: 0.221104487776756 Acc: 0.600000023841858\n",
      "Epoch: 13020/25000............. Loss: 0.221080958843231 Acc: 0.600000023841858\n",
      "Epoch: 13030/25000............. Loss: 0.221082374453545 Acc: 0.600000023841858\n",
      "Epoch: 13040/25000............. Loss: 0.221079096198082 Acc: 0.600000023841858\n",
      "Epoch: 13050/25000............. Loss: 0.221077382564545 Acc: 0.600000023841858\n",
      "Epoch: 13060/25000............. Loss: 0.221076294779778 Acc: 0.600000023841858\n",
      "Epoch: 13070/25000............. Loss: 0.221075102686882 Acc: 0.600000023841858\n",
      "Epoch: 13080/25000............. Loss: 0.221073880791664 Acc: 0.600000023841858\n",
      "Epoch: 13090/25000............. Loss: 0.221072703599930 Acc: 0.600000023841858\n",
      "Epoch: 13100/25000............. Loss: 0.221071541309357 Acc: 0.600000023841858\n",
      "Epoch: 13110/25000............. Loss: 0.221070334315300 Acc: 0.600000023841858\n",
      "Epoch: 13120/25000............. Loss: 0.221069142222404 Acc: 0.600000023841858\n",
      "Epoch: 13130/25000............. Loss: 0.221067994832993 Acc: 0.600000023841858\n",
      "Epoch: 13140/25000............. Loss: 0.221068218350410 Acc: 0.600000023841858\n",
      "Epoch: 13150/25000............. Loss: 0.221202149987221 Acc: 0.600000023841858\n",
      "Epoch: 13160/25000............. Loss: 0.221107974648476 Acc: 0.600000023841858\n",
      "Epoch: 13170/25000............. Loss: 0.221184790134430 Acc: 0.600000023841858\n",
      "Epoch: 13180/25000............. Loss: 0.221193984150887 Acc: 0.600000023841858\n",
      "Epoch: 13190/25000............. Loss: 0.221088021993637 Acc: 0.600000023841858\n",
      "Epoch: 13200/25000............. Loss: 0.221092939376831 Acc: 0.600000023841858\n",
      "Epoch: 13210/25000............. Loss: 0.221060588955879 Acc: 0.600000023841858\n",
      "Epoch: 13220/25000............. Loss: 0.221062406897545 Acc: 0.600000023841858\n",
      "Epoch: 13230/25000............. Loss: 0.221057191491127 Acc: 0.600000023841858\n",
      "Epoch: 13240/25000............. Loss: 0.221055507659912 Acc: 0.600000023841858\n",
      "Epoch: 13250/25000............. Loss: 0.221054419875145 Acc: 0.600000023841858\n",
      "Epoch: 13260/25000............. Loss: 0.221053212881088 Acc: 0.600000023841858\n",
      "Epoch: 13270/25000............. Loss: 0.221052065491676 Acc: 0.600000023841858\n",
      "Epoch: 13280/25000............. Loss: 0.221050873398781 Acc: 0.600000023841858\n",
      "Epoch: 13290/25000............. Loss: 0.221049711108208 Acc: 0.600000023841858\n",
      "Epoch: 13300/25000............. Loss: 0.221048578619957 Acc: 0.600000023841858\n",
      "Epoch: 13310/25000............. Loss: 0.221047401428223 Acc: 0.600000023841858\n",
      "Epoch: 13320/25000............. Loss: 0.221046239137650 Acc: 0.600000023841858\n",
      "Epoch: 13330/25000............. Loss: 0.221045181155205 Acc: 0.600000023841858\n",
      "Epoch: 13340/25000............. Loss: 0.221046984195709 Acc: 0.600000023841858\n",
      "Epoch: 13350/25000............. Loss: 0.221235886216164 Acc: 0.600000023841858\n",
      "Epoch: 13360/25000............. Loss: 0.221169814467430 Acc: 0.600000023841858\n",
      "Epoch: 13370/25000............. Loss: 0.221165314316750 Acc: 0.600000023841858\n",
      "Epoch: 13380/25000............. Loss: 0.221092507243156 Acc: 0.600000023841858\n",
      "Epoch: 13390/25000............. Loss: 0.221062690019608 Acc: 0.600000023841858\n",
      "Epoch: 13400/25000............. Loss: 0.221045956015587 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13410/25000............. Loss: 0.221039071679115 Acc: 0.600000023841858\n",
      "Epoch: 13420/25000............. Loss: 0.221035271883011 Acc: 0.600000023841858\n",
      "Epoch: 13430/25000............. Loss: 0.221034571528435 Acc: 0.600000023841858\n",
      "Epoch: 13440/25000............. Loss: 0.221033006906509 Acc: 0.600000023841858\n",
      "Epoch: 13450/25000............. Loss: 0.221031725406647 Acc: 0.600000023841858\n",
      "Epoch: 13460/25000............. Loss: 0.221030578017235 Acc: 0.600000023841858\n",
      "Epoch: 13470/25000............. Loss: 0.221029445528984 Acc: 0.600000023841858\n",
      "Epoch: 13480/25000............. Loss: 0.221028342843056 Acc: 0.600000023841858\n",
      "Epoch: 13490/25000............. Loss: 0.221027210354805 Acc: 0.600000023841858\n",
      "Epoch: 13500/25000............. Loss: 0.221026122570038 Acc: 0.600000023841858\n",
      "Epoch: 13510/25000............. Loss: 0.221025541424751 Acc: 0.600000023841858\n",
      "Epoch: 13520/25000............. Loss: 0.221048071980476 Acc: 0.600000023841858\n",
      "Epoch: 13530/25000............. Loss: 0.221321046352386 Acc: 0.600000023841858\n",
      "Epoch: 13540/25000............. Loss: 0.221067205071449 Acc: 0.600000023841858\n",
      "Epoch: 13550/25000............. Loss: 0.221023380756378 Acc: 0.600000023841858\n",
      "Epoch: 13560/25000............. Loss: 0.221132978796959 Acc: 0.600000023841858\n",
      "Epoch: 13570/25000............. Loss: 0.221029981970787 Acc: 0.600000023841858\n",
      "Epoch: 13580/25000............. Loss: 0.221110835671425 Acc: 0.600000023841858\n",
      "Epoch: 13590/25000............. Loss: 0.221019268035889 Acc: 0.600000023841858\n",
      "Epoch: 13600/25000............. Loss: 0.221024781465530 Acc: 0.600000023841858\n",
      "Epoch: 13610/25000............. Loss: 0.221014872193336 Acc: 0.600000023841858\n",
      "Epoch: 13620/25000............. Loss: 0.221014365553856 Acc: 0.600000023841858\n",
      "Epoch: 13630/25000............. Loss: 0.221011772751808 Acc: 0.600000023841858\n",
      "Epoch: 13640/25000............. Loss: 0.221010819077492 Acc: 0.600000023841858\n",
      "Epoch: 13650/25000............. Loss: 0.221009641885757 Acc: 0.600000023841858\n",
      "Epoch: 13660/25000............. Loss: 0.221008524298668 Acc: 0.600000023841858\n",
      "Epoch: 13670/25000............. Loss: 0.221007406711578 Acc: 0.600000023841858\n",
      "Epoch: 13680/25000............. Loss: 0.221006929874420 Acc: 0.600000023841858\n",
      "Epoch: 13690/25000............. Loss: 0.221061617136002 Acc: 0.600000023841858\n",
      "Epoch: 13700/25000............. Loss: 0.221056684851646 Acc: 0.600000023841858\n",
      "Epoch: 13710/25000............. Loss: 0.221068322658539 Acc: 0.600000023841858\n",
      "Epoch: 13720/25000............. Loss: 0.221012651920319 Acc: 0.600000023841858\n",
      "Epoch: 13730/25000............. Loss: 0.221010953187943 Acc: 0.600000023841858\n",
      "Epoch: 13740/25000............. Loss: 0.221001297235489 Acc: 0.600000023841858\n",
      "Epoch: 13750/25000............. Loss: 0.220999523997307 Acc: 0.600000023841858\n",
      "Epoch: 13760/25000............. Loss: 0.221005111932755 Acc: 0.600000023841858\n",
      "Epoch: 13770/25000............. Loss: 0.221405610442162 Acc: 0.600000023841858\n",
      "Epoch: 13780/25000............. Loss: 0.221151217818260 Acc: 0.600000023841858\n",
      "Epoch: 13790/25000............. Loss: 0.220998808741570 Acc: 0.600000023841858\n",
      "Epoch: 13800/25000............. Loss: 0.221012204885483 Acc: 0.600000023841858\n",
      "Epoch: 13810/25000............. Loss: 0.220992848277092 Acc: 0.600000023841858\n",
      "Epoch: 13820/25000............. Loss: 0.220994040369987 Acc: 0.600000023841858\n",
      "Epoch: 13830/25000............. Loss: 0.220990359783173 Acc: 0.600000023841858\n",
      "Epoch: 13840/25000............. Loss: 0.220989227294922 Acc: 0.600000023841858\n",
      "Epoch: 13850/25000............. Loss: 0.220988139510155 Acc: 0.600000023841858\n",
      "Epoch: 13860/25000............. Loss: 0.220987021923065 Acc: 0.600000023841858\n",
      "Epoch: 13870/25000............. Loss: 0.220985919237137 Acc: 0.600000023841858\n",
      "Epoch: 13880/25000............. Loss: 0.220984831452370 Acc: 0.600000023841858\n",
      "Epoch: 13890/25000............. Loss: 0.220983743667603 Acc: 0.600000023841858\n",
      "Epoch: 13900/25000............. Loss: 0.220983713865280 Acc: 0.600000023841858\n",
      "Epoch: 13910/25000............. Loss: 0.221271604299545 Acc: 0.600000023841858\n",
      "Epoch: 13920/25000............. Loss: 0.221314162015915 Acc: 0.600000023841858\n",
      "Epoch: 13930/25000............. Loss: 0.221054926514626 Acc: 0.600000023841858\n",
      "Epoch: 13940/25000............. Loss: 0.220992177724838 Acc: 0.600000023841858\n",
      "Epoch: 13950/25000............. Loss: 0.220978900790215 Acc: 0.600000023841858\n",
      "Epoch: 13960/25000............. Loss: 0.221007332205772 Acc: 0.600000023841858\n",
      "Epoch: 13970/25000............. Loss: 0.221230298280716 Acc: 0.600000023841858\n",
      "Epoch: 13980/25000............. Loss: 0.220976382493973 Acc: 0.600000023841858\n",
      "Epoch: 13990/25000............. Loss: 0.221013203263283 Acc: 0.600000023841858\n",
      "Epoch: 14000/25000............. Loss: 0.220972374081612 Acc: 0.600000023841858\n",
      "Epoch: 14010/25000............. Loss: 0.220976158976555 Acc: 0.600000023841858\n",
      "Epoch: 14020/25000............. Loss: 0.220971077680588 Acc: 0.600000023841858\n",
      "Epoch: 14030/25000............. Loss: 0.220969289541245 Acc: 0.600000023841858\n",
      "Epoch: 14040/25000............. Loss: 0.220968395471573 Acc: 0.600000023841858\n",
      "Epoch: 14050/25000............. Loss: 0.220967292785645 Acc: 0.600000023841858\n",
      "Epoch: 14060/25000............. Loss: 0.220966205000877 Acc: 0.600000023841858\n",
      "Epoch: 14070/25000............. Loss: 0.220965161919594 Acc: 0.600000023841858\n",
      "Epoch: 14080/25000............. Loss: 0.220964118838310 Acc: 0.600000023841858\n",
      "Epoch: 14090/25000............. Loss: 0.220963045954704 Acc: 0.600000023841858\n",
      "Epoch: 14100/25000............. Loss: 0.220962017774582 Acc: 0.600000023841858\n",
      "Epoch: 14110/25000............. Loss: 0.220960944890976 Acc: 0.600000023841858\n",
      "Epoch: 14120/25000............. Loss: 0.220959916710854 Acc: 0.600000023841858\n",
      "Epoch: 14130/25000............. Loss: 0.220958858728409 Acc: 0.600000023841858\n",
      "Epoch: 14140/25000............. Loss: 0.220957845449448 Acc: 0.600000023841858\n",
      "Epoch: 14150/25000............. Loss: 0.220959573984146 Acc: 0.600000023841858\n",
      "Epoch: 14160/25000............. Loss: 0.221232265233994 Acc: 0.600000023841858\n",
      "Epoch: 14170/25000............. Loss: 0.221162483096123 Acc: 0.600000023841858\n",
      "Epoch: 14180/25000............. Loss: 0.221395701169968 Acc: 0.600000023841858\n",
      "Epoch: 14190/25000............. Loss: 0.221095368266106 Acc: 0.600000023841858\n",
      "Epoch: 14200/25000............. Loss: 0.220965608954430 Acc: 0.600000023841858\n",
      "Epoch: 14210/25000............. Loss: 0.220968291163445 Acc: 0.600000023841858\n",
      "Epoch: 14220/25000............. Loss: 0.220951050519943 Acc: 0.600000023841858\n",
      "Epoch: 14230/25000............. Loss: 0.220951586961746 Acc: 0.600000023841858\n",
      "Epoch: 14240/25000............. Loss: 0.220947787165642 Acc: 0.600000023841858\n",
      "Epoch: 14250/25000............. Loss: 0.220946729183197 Acc: 0.600000023841858\n",
      "Epoch: 14260/25000............. Loss: 0.220945760607719 Acc: 0.600000023841858\n",
      "Epoch: 14270/25000............. Loss: 0.220944657921791 Acc: 0.600000023841858\n",
      "Epoch: 14280/25000............. Loss: 0.220943614840508 Acc: 0.600000023841858\n",
      "Epoch: 14290/25000............. Loss: 0.220942571759224 Acc: 0.600000023841858\n",
      "Epoch: 14300/25000............. Loss: 0.220941588282585 Acc: 0.600000023841858\n",
      "Epoch: 14310/25000............. Loss: 0.220940530300140 Acc: 0.600000023841858\n",
      "Epoch: 14320/25000............. Loss: 0.220939502120018 Acc: 0.600000023841858\n",
      "Epoch: 14330/25000............. Loss: 0.220938488841057 Acc: 0.600000023841858\n",
      "Epoch: 14340/25000............. Loss: 0.220937445759773 Acc: 0.600000023841858\n",
      "Epoch: 14350/25000............. Loss: 0.220936536788940 Acc: 0.600000023841858\n",
      "Epoch: 14360/25000............. Loss: 0.220942184329033 Acc: 0.600000023841858\n",
      "Epoch: 14370/25000............. Loss: 0.221333876252174 Acc: 0.600000023841858\n",
      "Epoch: 14380/25000............. Loss: 0.221588984131813 Acc: 0.600000023841858\n",
      "Epoch: 14390/25000............. Loss: 0.220996201038361 Acc: 0.600000023841858\n",
      "Epoch: 14400/25000............. Loss: 0.220996588468552 Acc: 0.600000023841858\n",
      "Epoch: 14410/25000............. Loss: 0.220931604504585 Acc: 0.600000023841858\n",
      "Epoch: 14420/25000............. Loss: 0.220939263701439 Acc: 0.600000023841858\n",
      "Epoch: 14430/25000............. Loss: 0.220929056406021 Acc: 0.600000023841858\n",
      "Epoch: 14440/25000............. Loss: 0.220928356051445 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14450/25000............. Loss: 0.220926761627197 Acc: 0.600000023841858\n",
      "Epoch: 14460/25000............. Loss: 0.220925539731979 Acc: 0.600000023841858\n",
      "Epoch: 14470/25000............. Loss: 0.220924586057663 Acc: 0.600000023841858\n",
      "Epoch: 14480/25000............. Loss: 0.220923602581024 Acc: 0.600000023841858\n",
      "Epoch: 14490/25000............. Loss: 0.220922559499741 Acc: 0.600000023841858\n",
      "Epoch: 14500/25000............. Loss: 0.220921576023102 Acc: 0.600000023841858\n",
      "Epoch: 14510/25000............. Loss: 0.220920547842979 Acc: 0.600000023841858\n",
      "Epoch: 14520/25000............. Loss: 0.220919564366341 Acc: 0.600000023841858\n",
      "Epoch: 14530/25000............. Loss: 0.220918551087379 Acc: 0.600000023841858\n",
      "Epoch: 14540/25000............. Loss: 0.220917686820030 Acc: 0.600000023841858\n",
      "Epoch: 14550/25000............. Loss: 0.220921814441681 Acc: 0.600000023841858\n",
      "Epoch: 14560/25000............. Loss: 0.221194222569466 Acc: 0.600000023841858\n",
      "Epoch: 14570/25000............. Loss: 0.221054852008820 Acc: 0.600000023841858\n",
      "Epoch: 14580/25000............. Loss: 0.220962300896645 Acc: 0.600000023841858\n",
      "Epoch: 14590/25000............. Loss: 0.221279814839363 Acc: 0.600000023841858\n",
      "Epoch: 14600/25000............. Loss: 0.220920503139496 Acc: 0.600000023841858\n",
      "Epoch: 14610/25000............. Loss: 0.220956712961197 Acc: 0.600000023841858\n",
      "Epoch: 14620/25000............. Loss: 0.220909729599953 Acc: 0.600000023841858\n",
      "Epoch: 14630/25000............. Loss: 0.220914706587791 Acc: 0.600000023841858\n",
      "Epoch: 14640/25000............. Loss: 0.220908179879189 Acc: 0.600000023841858\n",
      "Epoch: 14650/25000............. Loss: 0.220907151699066 Acc: 0.600000023841858\n",
      "Epoch: 14660/25000............. Loss: 0.220906123518944 Acc: 0.600000023841858\n",
      "Epoch: 14670/25000............. Loss: 0.220904961228371 Acc: 0.600000023841858\n",
      "Epoch: 14680/25000............. Loss: 0.220903903245926 Acc: 0.600000023841858\n",
      "Epoch: 14690/25000............. Loss: 0.220902904868126 Acc: 0.600000023841858\n",
      "Epoch: 14700/25000............. Loss: 0.220901921391487 Acc: 0.600000023841858\n",
      "Epoch: 14710/25000............. Loss: 0.220900952816010 Acc: 0.600000023841858\n",
      "Epoch: 14720/25000............. Loss: 0.220899969339371 Acc: 0.600000023841858\n",
      "Epoch: 14730/25000............. Loss: 0.220899373292923 Acc: 0.600000023841858\n",
      "Epoch: 14740/25000............. Loss: 0.220945432782173 Acc: 0.600000023841858\n",
      "Epoch: 14750/25000............. Loss: 0.220972314476967 Acc: 0.600000023841858\n",
      "Epoch: 14760/25000............. Loss: 0.220991984009743 Acc: 0.600000023841858\n",
      "Epoch: 14770/25000............. Loss: 0.220895349979401 Acc: 0.600000023841858\n",
      "Epoch: 14780/25000............. Loss: 0.220908343791962 Acc: 0.600000023841858\n",
      "Epoch: 14790/25000............. Loss: 0.220908477902412 Acc: 0.600000023841858\n",
      "Epoch: 14800/25000............. Loss: 0.221458435058594 Acc: 0.600000023841858\n",
      "Epoch: 14810/25000............. Loss: 0.220937922596931 Acc: 0.600000023841858\n",
      "Epoch: 14820/25000............. Loss: 0.220893085002899 Acc: 0.600000023841858\n",
      "Epoch: 14830/25000............. Loss: 0.220902159810066 Acc: 0.600000023841858\n",
      "Epoch: 14840/25000............. Loss: 0.220896750688553 Acc: 0.600000023841858\n",
      "Epoch: 14850/25000............. Loss: 0.220888823270798 Acc: 0.600000023841858\n",
      "Epoch: 14860/25000............. Loss: 0.220886752009392 Acc: 0.600000023841858\n",
      "Epoch: 14870/25000............. Loss: 0.220886096358299 Acc: 0.600000023841858\n",
      "Epoch: 14880/25000............. Loss: 0.220884770154953 Acc: 0.600000023841858\n",
      "Epoch: 14890/25000............. Loss: 0.220883890986443 Acc: 0.600000023841858\n",
      "Epoch: 14900/25000............. Loss: 0.220882907509804 Acc: 0.600000023841858\n",
      "Epoch: 14910/25000............. Loss: 0.220881968736649 Acc: 0.600000023841858\n",
      "Epoch: 14920/25000............. Loss: 0.220881000161171 Acc: 0.600000023841858\n",
      "Epoch: 14930/25000............. Loss: 0.220880046486855 Acc: 0.600000023841858\n",
      "Epoch: 14940/25000............. Loss: 0.220879122614861 Acc: 0.600000023841858\n",
      "Epoch: 14950/25000............. Loss: 0.220879942178726 Acc: 0.600000023841858\n",
      "Epoch: 14960/25000............. Loss: 0.221040695905685 Acc: 0.600000023841858\n",
      "Epoch: 14970/25000............. Loss: 0.220953494310379 Acc: 0.600000023841858\n",
      "Epoch: 14980/25000............. Loss: 0.220935627818108 Acc: 0.600000023841858\n",
      "Epoch: 14990/25000............. Loss: 0.220884904265404 Acc: 0.600000023841858\n",
      "Epoch: 15000/25000............. Loss: 0.220879584550858 Acc: 0.600000023841858\n",
      "Epoch: 15010/25000............. Loss: 0.220873683691025 Acc: 0.600000023841858\n",
      "Epoch: 15020/25000............. Loss: 0.220872849225998 Acc: 0.600000023841858\n",
      "Epoch: 15030/25000............. Loss: 0.220871686935425 Acc: 0.600000023841858\n",
      "Epoch: 15040/25000............. Loss: 0.220955342054367 Acc: 0.600000023841858\n",
      "Epoch: 15050/25000............. Loss: 0.220870479941368 Acc: 0.600000023841858\n",
      "Epoch: 15060/25000............. Loss: 0.220948740839958 Acc: 0.600000023841858\n",
      "Epoch: 15070/25000............. Loss: 0.220870807766914 Acc: 0.600000023841858\n",
      "Epoch: 15080/25000............. Loss: 0.220875725150108 Acc: 0.600000023841858\n",
      "Epoch: 15090/25000............. Loss: 0.220866546034813 Acc: 0.600000023841858\n",
      "Epoch: 15100/25000............. Loss: 0.220864519476891 Acc: 0.600000023841858\n",
      "Epoch: 15110/25000............. Loss: 0.220863759517670 Acc: 0.600000023841858\n",
      "Epoch: 15120/25000............. Loss: 0.220862418413162 Acc: 0.600000023841858\n",
      "Epoch: 15130/25000............. Loss: 0.220861375331879 Acc: 0.600000023841858\n",
      "Epoch: 15140/25000............. Loss: 0.220860466361046 Acc: 0.600000023841858\n",
      "Epoch: 15150/25000............. Loss: 0.220862001180649 Acc: 0.600000023841858\n",
      "Epoch: 15160/25000............. Loss: 0.221049562096596 Acc: 0.600000023841858\n",
      "Epoch: 15170/25000............. Loss: 0.220943421125412 Acc: 0.600000023841858\n",
      "Epoch: 15180/25000............. Loss: 0.220914512872696 Acc: 0.600000023841858\n",
      "Epoch: 15190/25000............. Loss: 0.220859155058861 Acc: 0.600000023841858\n",
      "Epoch: 15200/25000............. Loss: 0.220863759517670 Acc: 0.600000023841858\n",
      "Epoch: 15210/25000............. Loss: 0.220854014158249 Acc: 0.600000023841858\n",
      "Epoch: 15220/25000............. Loss: 0.220854535698891 Acc: 0.600000023841858\n",
      "Epoch: 15230/25000............. Loss: 0.220901682972908 Acc: 0.600000023841858\n",
      "Epoch: 15240/25000............. Loss: 0.220960080623627 Acc: 0.600000023841858\n",
      "Epoch: 15250/25000............. Loss: 0.220862150192261 Acc: 0.600000023841858\n",
      "Epoch: 15260/25000............. Loss: 0.220882371068001 Acc: 0.600000023841858\n",
      "Epoch: 15270/25000............. Loss: 0.220848903059959 Acc: 0.600000023841858\n",
      "Epoch: 15280/25000............. Loss: 0.220850333571434 Acc: 0.600000023841858\n",
      "Epoch: 15290/25000............. Loss: 0.220848172903061 Acc: 0.600000023841858\n",
      "Epoch: 15300/25000............. Loss: 0.220845952630043 Acc: 0.600000023841858\n",
      "Epoch: 15310/25000............. Loss: 0.220844835042953 Acc: 0.600000023841858\n",
      "Epoch: 15320/25000............. Loss: 0.220843881368637 Acc: 0.600000023841858\n",
      "Epoch: 15330/25000............. Loss: 0.220843032002449 Acc: 0.600000023841858\n",
      "Epoch: 15340/25000............. Loss: 0.220843687653542 Acc: 0.600000023841858\n",
      "Epoch: 15350/25000............. Loss: 0.220982789993286 Acc: 0.600000023841858\n",
      "Epoch: 15360/25000............. Loss: 0.220884352922440 Acc: 0.600000023841858\n",
      "Epoch: 15370/25000............. Loss: 0.220914110541344 Acc: 0.600000023841858\n",
      "Epoch: 15380/25000............. Loss: 0.220842093229294 Acc: 0.600000023841858\n",
      "Epoch: 15390/25000............. Loss: 0.220846980810165 Acc: 0.600000023841858\n",
      "Epoch: 15400/25000............. Loss: 0.220838606357574 Acc: 0.600000023841858\n",
      "Epoch: 15410/25000............. Loss: 0.220947816967964 Acc: 0.600000023841858\n",
      "Epoch: 15420/25000............. Loss: 0.220836117863655 Acc: 0.600000023841858\n",
      "Epoch: 15430/25000............. Loss: 0.220893129706383 Acc: 0.600000023841858\n",
      "Epoch: 15440/25000............. Loss: 0.220846757292747 Acc: 0.600000023841858\n",
      "Epoch: 15450/25000............. Loss: 0.220833212137222 Acc: 0.600000023841858\n",
      "Epoch: 15460/25000............. Loss: 0.220834553241730 Acc: 0.600000023841858\n",
      "Epoch: 15470/25000............. Loss: 0.220831453800201 Acc: 0.600000023841858\n",
      "Epoch: 15480/25000............. Loss: 0.220829665660858 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15490/25000............. Loss: 0.220828667283058 Acc: 0.600000023841858\n",
      "Epoch: 15500/25000............. Loss: 0.220827713608742 Acc: 0.600000023841858\n",
      "Epoch: 15510/25000............. Loss: 0.220826834440231 Acc: 0.600000023841858\n",
      "Epoch: 15520/25000............. Loss: 0.220825910568237 Acc: 0.600000023841858\n",
      "Epoch: 15530/25000............. Loss: 0.220825046300888 Acc: 0.600000023841858\n",
      "Epoch: 15540/25000............. Loss: 0.220827534794807 Acc: 0.600000023841858\n",
      "Epoch: 15550/25000............. Loss: 0.221093758940697 Acc: 0.600000023841858\n",
      "Epoch: 15560/25000............. Loss: 0.220992505550385 Acc: 0.600000023841858\n",
      "Epoch: 15570/25000............. Loss: 0.220836207270622 Acc: 0.600000023841858\n",
      "Epoch: 15580/25000............. Loss: 0.220863088965416 Acc: 0.600000023841858\n",
      "Epoch: 15590/25000............. Loss: 0.221146613359451 Acc: 0.600000023841858\n",
      "Epoch: 15600/25000............. Loss: 0.220845520496368 Acc: 0.600000023841858\n",
      "Epoch: 15610/25000............. Loss: 0.220837593078613 Acc: 0.600000023841858\n",
      "Epoch: 15620/25000............. Loss: 0.220828980207443 Acc: 0.600000023841858\n",
      "Epoch: 15630/25000............. Loss: 0.220816224813461 Acc: 0.600000023841858\n",
      "Epoch: 15640/25000............. Loss: 0.220816388726234 Acc: 0.600000023841858\n",
      "Epoch: 15650/25000............. Loss: 0.220815181732178 Acc: 0.600000023841858\n",
      "Epoch: 15660/25000............. Loss: 0.220813825726509 Acc: 0.600000023841858\n",
      "Epoch: 15670/25000............. Loss: 0.220812797546387 Acc: 0.600000023841858\n",
      "Epoch: 15680/25000............. Loss: 0.220811858773232 Acc: 0.600000023841858\n",
      "Epoch: 15690/25000............. Loss: 0.220810934901237 Acc: 0.600000023841858\n",
      "Epoch: 15700/25000............. Loss: 0.220810025930405 Acc: 0.600000023841858\n",
      "Epoch: 15710/25000............. Loss: 0.220809176564217 Acc: 0.600000023841858\n",
      "Epoch: 15720/25000............. Loss: 0.220808252692223 Acc: 0.600000023841858\n",
      "Epoch: 15730/25000............. Loss: 0.220807358622551 Acc: 0.600000023841858\n",
      "Epoch: 15740/25000............. Loss: 0.220806613564491 Acc: 0.600000023841858\n",
      "Epoch: 15750/25000............. Loss: 0.220819130539894 Acc: 0.600000023841858\n",
      "Epoch: 15760/25000............. Loss: 0.221493333578110 Acc: 0.591666638851166\n",
      "Epoch: 15770/25000............. Loss: 0.220807746052742 Acc: 0.600000023841858\n",
      "Epoch: 15780/25000............. Loss: 0.220894470810890 Acc: 0.600000023841858\n",
      "Epoch: 15790/25000............. Loss: 0.220837652683258 Acc: 0.600000023841858\n",
      "Epoch: 15800/25000............. Loss: 0.220806375145912 Acc: 0.600000023841858\n",
      "Epoch: 15810/25000............. Loss: 0.220804780721664 Acc: 0.600000023841858\n",
      "Epoch: 15820/25000............. Loss: 0.220801338553429 Acc: 0.600000023841858\n",
      "Epoch: 15830/25000............. Loss: 0.220798790454865 Acc: 0.600000023841858\n",
      "Epoch: 15840/25000............. Loss: 0.220798075199127 Acc: 0.600000023841858\n",
      "Epoch: 15850/25000............. Loss: 0.220797121524811 Acc: 0.600000023841858\n",
      "Epoch: 15860/25000............. Loss: 0.220796257257462 Acc: 0.600000023841858\n",
      "Epoch: 15870/25000............. Loss: 0.220795392990112 Acc: 0.600000023841858\n",
      "Epoch: 15880/25000............. Loss: 0.220794573426247 Acc: 0.600000023841858\n",
      "Epoch: 15890/25000............. Loss: 0.220793709158897 Acc: 0.600000023841858\n",
      "Epoch: 15900/25000............. Loss: 0.220792815089226 Acc: 0.600000023841858\n",
      "Epoch: 15910/25000............. Loss: 0.220791950821877 Acc: 0.600000023841858\n",
      "Epoch: 15920/25000............. Loss: 0.220791101455688 Acc: 0.600000023841858\n",
      "Epoch: 15930/25000............. Loss: 0.220790758728981 Acc: 0.600000023841858\n",
      "Epoch: 15940/25000............. Loss: 0.220832765102386 Acc: 0.600000023841858\n",
      "Epoch: 15950/25000............. Loss: 0.220920547842979 Acc: 0.600000023841858\n",
      "Epoch: 15960/25000............. Loss: 0.220817282795906 Acc: 0.600000023841858\n",
      "Epoch: 15970/25000............. Loss: 0.220814436674118 Acc: 0.600000023841858\n",
      "Epoch: 15980/25000............. Loss: 0.220801025629044 Acc: 0.600000023841858\n",
      "Epoch: 15990/25000............. Loss: 0.221154689788818 Acc: 0.600000023841858\n",
      "Epoch: 16000/25000............. Loss: 0.220902860164642 Acc: 0.600000023841858\n",
      "Epoch: 16010/25000............. Loss: 0.220784470438957 Acc: 0.600000023841858\n",
      "Epoch: 16020/25000............. Loss: 0.220795229077339 Acc: 0.600000023841858\n",
      "Epoch: 16030/25000............. Loss: 0.220786362886429 Acc: 0.600000023841858\n",
      "Epoch: 16040/25000............. Loss: 0.220781043171883 Acc: 0.600000023841858\n",
      "Epoch: 16050/25000............. Loss: 0.220780164003372 Acc: 0.600000023841858\n",
      "Epoch: 16060/25000............. Loss: 0.220779344439507 Acc: 0.600000023841858\n",
      "Epoch: 16070/25000............. Loss: 0.220778435468674 Acc: 0.600000023841858\n",
      "Epoch: 16080/25000............. Loss: 0.220777541399002 Acc: 0.600000023841858\n",
      "Epoch: 16090/25000............. Loss: 0.220776692032814 Acc: 0.600000023841858\n",
      "Epoch: 16100/25000............. Loss: 0.220775842666626 Acc: 0.600000023841858\n",
      "Epoch: 16110/25000............. Loss: 0.220774993300438 Acc: 0.600000023841858\n",
      "Epoch: 16120/25000............. Loss: 0.220774263143539 Acc: 0.600000023841858\n",
      "Epoch: 16130/25000............. Loss: 0.220781013369560 Acc: 0.600000023841858\n",
      "Epoch: 16140/25000............. Loss: 0.221164792776108 Acc: 0.600000023841858\n",
      "Epoch: 16150/25000............. Loss: 0.220924466848373 Acc: 0.600000023841858\n",
      "Epoch: 16160/25000............. Loss: 0.221054419875145 Acc: 0.600000023841858\n",
      "Epoch: 16170/25000............. Loss: 0.220922380685806 Acc: 0.600000023841858\n",
      "Epoch: 16180/25000............. Loss: 0.220799595117569 Acc: 0.600000023841858\n",
      "Epoch: 16190/25000............. Loss: 0.220772519707680 Acc: 0.600000023841858\n",
      "Epoch: 16200/25000............. Loss: 0.220775648951530 Acc: 0.600000023841858\n",
      "Epoch: 16210/25000............. Loss: 0.220768064260483 Acc: 0.600000023841858\n",
      "Epoch: 16220/25000............. Loss: 0.220765843987465 Acc: 0.600000023841858\n",
      "Epoch: 16230/25000............. Loss: 0.220765069127083 Acc: 0.600000023841858\n",
      "Epoch: 16240/25000............. Loss: 0.220764219760895 Acc: 0.600000023841858\n",
      "Epoch: 16250/25000............. Loss: 0.220763340592384 Acc: 0.600000023841858\n",
      "Epoch: 16260/25000............. Loss: 0.220762506127357 Acc: 0.600000023841858\n",
      "Epoch: 16270/25000............. Loss: 0.220761686563492 Acc: 0.600000023841858\n",
      "Epoch: 16280/25000............. Loss: 0.220760852098465 Acc: 0.600000023841858\n",
      "Epoch: 16290/25000............. Loss: 0.220760017633438 Acc: 0.600000023841858\n",
      "Epoch: 16300/25000............. Loss: 0.220759168267250 Acc: 0.600000023841858\n",
      "Epoch: 16310/25000............. Loss: 0.220759049057961 Acc: 0.600000023841858\n",
      "Epoch: 16320/25000............. Loss: 0.220804378390312 Acc: 0.600000023841858\n",
      "Epoch: 16330/25000............. Loss: 0.220891728997231 Acc: 0.600000023841858\n",
      "Epoch: 16340/25000............. Loss: 0.221148088574409 Acc: 0.600000023841858\n",
      "Epoch: 16350/25000............. Loss: 0.220804750919342 Acc: 0.600000023841858\n",
      "Epoch: 16360/25000............. Loss: 0.220789566636086 Acc: 0.600000023841858\n",
      "Epoch: 16370/25000............. Loss: 0.220761805772781 Acc: 0.600000023841858\n",
      "Epoch: 16380/25000............. Loss: 0.220756709575653 Acc: 0.600000023841858\n",
      "Epoch: 16390/25000............. Loss: 0.220753699541092 Acc: 0.600000023841858\n",
      "Epoch: 16400/25000............. Loss: 0.220751091837883 Acc: 0.600000023841858\n",
      "Epoch: 16410/25000............. Loss: 0.220750272274017 Acc: 0.600000023841858\n",
      "Epoch: 16420/25000............. Loss: 0.220749437808990 Acc: 0.600000023841858\n",
      "Epoch: 16430/25000............. Loss: 0.220748603343964 Acc: 0.600000023841858\n",
      "Epoch: 16440/25000............. Loss: 0.220747768878937 Acc: 0.600000023841858\n",
      "Epoch: 16450/25000............. Loss: 0.220746964216232 Acc: 0.600000023841858\n",
      "Epoch: 16460/25000............. Loss: 0.220746129751205 Acc: 0.600000023841858\n",
      "Epoch: 16470/25000............. Loss: 0.220745325088501 Acc: 0.600000023841858\n",
      "Epoch: 16480/25000............. Loss: 0.220744505524635 Acc: 0.600000023841858\n",
      "Epoch: 16490/25000............. Loss: 0.220743656158447 Acc: 0.600000023841858\n",
      "Epoch: 16500/25000............. Loss: 0.220744162797928 Acc: 0.600000023841858\n",
      "Epoch: 16510/25000............. Loss: 0.220873326063156 Acc: 0.600000023841858\n",
      "Epoch: 16520/25000............. Loss: 0.220782652497292 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16530/25000............. Loss: 0.221049621701241 Acc: 0.600000023841858\n",
      "Epoch: 16540/25000............. Loss: 0.220917612314224 Acc: 0.600000023841858\n",
      "Epoch: 16550/25000............. Loss: 0.220761284232140 Acc: 0.600000023841858\n",
      "Epoch: 16560/25000............. Loss: 0.220766320824623 Acc: 0.600000023841858\n",
      "Epoch: 16570/25000............. Loss: 0.220738559961319 Acc: 0.600000023841858\n",
      "Epoch: 16580/25000............. Loss: 0.220740213990211 Acc: 0.600000023841858\n",
      "Epoch: 16590/25000............. Loss: 0.220735818147659 Acc: 0.600000023841858\n",
      "Epoch: 16600/25000............. Loss: 0.220735341310501 Acc: 0.600000023841858\n",
      "Epoch: 16610/25000............. Loss: 0.220734298229218 Acc: 0.600000023841858\n",
      "Epoch: 16620/25000............. Loss: 0.220733359456062 Acc: 0.600000023841858\n",
      "Epoch: 16630/25000............. Loss: 0.220732510089874 Acc: 0.600000023841858\n",
      "Epoch: 16640/25000............. Loss: 0.220731735229492 Acc: 0.600000023841858\n",
      "Epoch: 16650/25000............. Loss: 0.220730945467949 Acc: 0.600000023841858\n",
      "Epoch: 16660/25000............. Loss: 0.220730111002922 Acc: 0.600000023841858\n",
      "Epoch: 16670/25000............. Loss: 0.220729351043701 Acc: 0.600000023841858\n",
      "Epoch: 16680/25000............. Loss: 0.220728531479836 Acc: 0.600000023841858\n",
      "Epoch: 16690/25000............. Loss: 0.220727711915970 Acc: 0.600000023841858\n",
      "Epoch: 16700/25000............. Loss: 0.220726907253265 Acc: 0.600000023841858\n",
      "Epoch: 16710/25000............. Loss: 0.220726564526558 Acc: 0.600000023841858\n",
      "Epoch: 16720/25000............. Loss: 0.220795780420303 Acc: 0.600000023841858\n",
      "Epoch: 16730/25000............. Loss: 0.220734104514122 Acc: 0.600000023841858\n",
      "Epoch: 16740/25000............. Loss: 0.221254304051399 Acc: 0.600000023841858\n",
      "Epoch: 16750/25000............. Loss: 0.220874354243279 Acc: 0.600000023841858\n",
      "Epoch: 16760/25000............. Loss: 0.220727518200874 Acc: 0.600000023841858\n",
      "Epoch: 16770/25000............. Loss: 0.220744878053665 Acc: 0.600000023841858\n",
      "Epoch: 16780/25000............. Loss: 0.220721051096916 Acc: 0.600000023841858\n",
      "Epoch: 16790/25000............. Loss: 0.220722734928131 Acc: 0.600000023841858\n",
      "Epoch: 16800/25000............. Loss: 0.220719724893570 Acc: 0.600000023841858\n",
      "Epoch: 16810/25000............. Loss: 0.220718443393707 Acc: 0.600000023841858\n",
      "Epoch: 16820/25000............. Loss: 0.220717594027519 Acc: 0.600000023841858\n",
      "Epoch: 16830/25000............. Loss: 0.220716819167137 Acc: 0.600000023841858\n",
      "Epoch: 16840/25000............. Loss: 0.220715999603271 Acc: 0.600000023841858\n",
      "Epoch: 16850/25000............. Loss: 0.220715269446373 Acc: 0.600000023841858\n",
      "Epoch: 16860/25000............. Loss: 0.220714479684830 Acc: 0.600000023841858\n",
      "Epoch: 16870/25000............. Loss: 0.220713660120964 Acc: 0.600000023841858\n",
      "Epoch: 16880/25000............. Loss: 0.220712885260582 Acc: 0.600000023841858\n",
      "Epoch: 16890/25000............. Loss: 0.220712125301361 Acc: 0.600000023841858\n",
      "Epoch: 16900/25000............. Loss: 0.220711320638657 Acc: 0.600000023841858\n",
      "Epoch: 16910/25000............. Loss: 0.220710530877113 Acc: 0.600000023841858\n",
      "Epoch: 16920/25000............. Loss: 0.220710098743439 Acc: 0.600000023841858\n",
      "Epoch: 16930/25000............. Loss: 0.220752745866776 Acc: 0.600000023841858\n",
      "Epoch: 16940/25000............. Loss: 0.220814466476440 Acc: 0.600000023841858\n",
      "Epoch: 16950/25000............. Loss: 0.220801934599876 Acc: 0.600000023841858\n",
      "Epoch: 16960/25000............. Loss: 0.221212714910507 Acc: 0.600000023841858\n",
      "Epoch: 16970/25000............. Loss: 0.220721498131752 Acc: 0.600000023841858\n",
      "Epoch: 16980/25000............. Loss: 0.220748841762543 Acc: 0.600000023841858\n",
      "Epoch: 16990/25000............. Loss: 0.220719739794731 Acc: 0.600000023841858\n",
      "Epoch: 17000/25000............. Loss: 0.220703944563866 Acc: 0.600000023841858\n",
      "Epoch: 17010/25000............. Loss: 0.220705673098564 Acc: 0.600000023841858\n",
      "Epoch: 17020/25000............. Loss: 0.220702171325684 Acc: 0.600000023841858\n",
      "Epoch: 17030/25000............. Loss: 0.220701709389687 Acc: 0.600000023841858\n",
      "Epoch: 17040/25000............. Loss: 0.220700651407242 Acc: 0.600000023841858\n",
      "Epoch: 17050/25000............. Loss: 0.220699861645699 Acc: 0.600000023841858\n",
      "Epoch: 17060/25000............. Loss: 0.220699086785316 Acc: 0.600000023841858\n",
      "Epoch: 17070/25000............. Loss: 0.220698356628418 Acc: 0.600000023841858\n",
      "Epoch: 17080/25000............. Loss: 0.220697566866875 Acc: 0.600000023841858\n",
      "Epoch: 17090/25000............. Loss: 0.220696792006493 Acc: 0.600000023841858\n",
      "Epoch: 17100/25000............. Loss: 0.220696017146111 Acc: 0.600000023841858\n",
      "Epoch: 17110/25000............. Loss: 0.220695272088051 Acc: 0.600000023841858\n",
      "Epoch: 17120/25000............. Loss: 0.220694527029991 Acc: 0.600000023841858\n",
      "Epoch: 17130/25000............. Loss: 0.220694571733475 Acc: 0.600000023841858\n",
      "Epoch: 17140/25000............. Loss: 0.220749124884605 Acc: 0.600000023841858\n",
      "Epoch: 17150/25000............. Loss: 0.220760494470596 Acc: 0.600000023841858\n",
      "Epoch: 17160/25000............. Loss: 0.220715448260307 Acc: 0.600000023841858\n",
      "Epoch: 17170/25000............. Loss: 0.220720320940018 Acc: 0.600000023841858\n",
      "Epoch: 17180/25000............. Loss: 0.220691516995430 Acc: 0.600000023841858\n",
      "Epoch: 17190/25000............. Loss: 0.221024438738823 Acc: 0.600000023841858\n",
      "Epoch: 17200/25000............. Loss: 0.220982894301414 Acc: 0.600000023841858\n",
      "Epoch: 17210/25000............. Loss: 0.220740541815758 Acc: 0.600000023841858\n",
      "Epoch: 17220/25000............. Loss: 0.220688477158546 Acc: 0.600000023841858\n",
      "Epoch: 17230/25000............. Loss: 0.220688283443451 Acc: 0.600000023841858\n",
      "Epoch: 17240/25000............. Loss: 0.220689281821251 Acc: 0.600000023841858\n",
      "Epoch: 17250/25000............. Loss: 0.220685869455338 Acc: 0.600000023841858\n",
      "Epoch: 17260/25000............. Loss: 0.220683991909027 Acc: 0.600000023841858\n",
      "Epoch: 17270/25000............. Loss: 0.220683395862579 Acc: 0.600000023841858\n",
      "Epoch: 17280/25000............. Loss: 0.220682457089424 Acc: 0.600000023841858\n",
      "Epoch: 17290/25000............. Loss: 0.220681771636009 Acc: 0.600000023841858\n",
      "Epoch: 17300/25000............. Loss: 0.220681086182594 Acc: 0.600000023841858\n",
      "Epoch: 17310/25000............. Loss: 0.220683127641678 Acc: 0.600000023841858\n",
      "Epoch: 17320/25000............. Loss: 0.220827788114548 Acc: 0.600000023841858\n",
      "Epoch: 17330/25000............. Loss: 0.220701456069946 Acc: 0.600000023841858\n",
      "Epoch: 17340/25000............. Loss: 0.220736652612686 Acc: 0.600000023841858\n",
      "Epoch: 17350/25000............. Loss: 0.220687597990036 Acc: 0.600000023841858\n",
      "Epoch: 17360/25000............. Loss: 0.220677003264427 Acc: 0.600000023841858\n",
      "Epoch: 17370/25000............. Loss: 0.220678031444550 Acc: 0.600000023841858\n",
      "Epoch: 17380/25000............. Loss: 0.220676168799400 Acc: 0.600000023841858\n",
      "Epoch: 17390/25000............. Loss: 0.220674693584442 Acc: 0.600000023841858\n",
      "Epoch: 17400/25000............. Loss: 0.220673710107803 Acc: 0.600000023841858\n",
      "Epoch: 17410/25000............. Loss: 0.220672860741615 Acc: 0.600000023841858\n",
      "Epoch: 17420/25000............. Loss: 0.220672026276588 Acc: 0.600000023841858\n",
      "Epoch: 17430/25000............. Loss: 0.220671281218529 Acc: 0.600000023841858\n",
      "Epoch: 17440/25000............. Loss: 0.220672890543938 Acc: 0.600000023841858\n",
      "Epoch: 17450/25000............. Loss: 0.221042662858963 Acc: 0.600000023841858\n",
      "Epoch: 17460/25000............. Loss: 0.220925778150558 Acc: 0.600000023841858\n",
      "Epoch: 17470/25000............. Loss: 0.220714986324310 Acc: 0.600000023841858\n",
      "Epoch: 17480/25000............. Loss: 0.220708176493645 Acc: 0.600000023841858\n",
      "Epoch: 17490/25000............. Loss: 0.220836356282234 Acc: 0.600000023841858\n",
      "Epoch: 17500/25000............. Loss: 0.220677420496941 Acc: 0.600000023841858\n",
      "Epoch: 17510/25000............. Loss: 0.220674857497215 Acc: 0.600000023841858\n",
      "Epoch: 17520/25000............. Loss: 0.220675721764565 Acc: 0.600000023841858\n",
      "Epoch: 17530/25000............. Loss: 0.220667168498039 Acc: 0.600000023841858\n",
      "Epoch: 17540/25000............. Loss: 0.220663934946060 Acc: 0.600000023841858\n",
      "Epoch: 17550/25000............. Loss: 0.220662698149681 Acc: 0.600000023841858\n",
      "Epoch: 17560/25000............. Loss: 0.220661953091621 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17570/25000............. Loss: 0.220661178231239 Acc: 0.600000023841858\n",
      "Epoch: 17580/25000............. Loss: 0.220660388469696 Acc: 0.600000023841858\n",
      "Epoch: 17590/25000............. Loss: 0.220659643411636 Acc: 0.600000023841858\n",
      "Epoch: 17600/25000............. Loss: 0.220658913254738 Acc: 0.600000023841858\n",
      "Epoch: 17610/25000............. Loss: 0.220658197999001 Acc: 0.600000023841858\n",
      "Epoch: 17620/25000............. Loss: 0.220657601952553 Acc: 0.600000023841858\n",
      "Epoch: 17630/25000............. Loss: 0.220660701394081 Acc: 0.600000023841858\n",
      "Epoch: 17640/25000............. Loss: 0.220839530229568 Acc: 0.600000023841858\n",
      "Epoch: 17650/25000............. Loss: 0.220702111721039 Acc: 0.600000023841858\n",
      "Epoch: 17660/25000............. Loss: 0.220710888504982 Acc: 0.600000023841858\n",
      "Epoch: 17670/25000............. Loss: 0.220661938190460 Acc: 0.600000023841858\n",
      "Epoch: 17680/25000............. Loss: 0.220654800534248 Acc: 0.600000023841858\n",
      "Epoch: 17690/25000............. Loss: 0.220808729529381 Acc: 0.600000023841858\n",
      "Epoch: 17700/25000............. Loss: 0.220724686980247 Acc: 0.600000023841858\n",
      "Epoch: 17710/25000............. Loss: 0.220712721347809 Acc: 0.600000023841858\n",
      "Epoch: 17720/25000............. Loss: 0.220666229724884 Acc: 0.600000023841858\n",
      "Epoch: 17730/25000............. Loss: 0.220653057098389 Acc: 0.600000023841858\n",
      "Epoch: 17740/25000............. Loss: 0.220651552081108 Acc: 0.600000023841858\n",
      "Epoch: 17750/25000............. Loss: 0.220648735761642 Acc: 0.600000023841858\n",
      "Epoch: 17760/25000............. Loss: 0.220647394657135 Acc: 0.600000023841858\n",
      "Epoch: 17770/25000............. Loss: 0.220646843314171 Acc: 0.600000023841858\n",
      "Epoch: 17780/25000............. Loss: 0.220645934343338 Acc: 0.600000023841858\n",
      "Epoch: 17790/25000............. Loss: 0.220645487308502 Acc: 0.600000023841858\n",
      "Epoch: 17800/25000............. Loss: 0.220665127038956 Acc: 0.600000023841858\n",
      "Epoch: 17810/25000............. Loss: 0.221000179648399 Acc: 0.600000023841858\n",
      "Epoch: 17820/25000............. Loss: 0.220656901597977 Acc: 0.600000023841858\n",
      "Epoch: 17830/25000............. Loss: 0.220675945281982 Acc: 0.600000023841858\n",
      "Epoch: 17840/25000............. Loss: 0.220646888017654 Acc: 0.600000023841858\n",
      "Epoch: 17850/25000............. Loss: 0.220643073320389 Acc: 0.600000023841858\n",
      "Epoch: 17860/25000............. Loss: 0.220642179250717 Acc: 0.600000023841858\n",
      "Epoch: 17870/25000............. Loss: 0.220639675855637 Acc: 0.600000023841858\n",
      "Epoch: 17880/25000............. Loss: 0.220639407634735 Acc: 0.600000023841858\n",
      "Epoch: 17890/25000............. Loss: 0.220677301287651 Acc: 0.600000023841858\n",
      "Epoch: 17900/25000............. Loss: 0.220813125371933 Acc: 0.600000023841858\n",
      "Epoch: 17910/25000............. Loss: 0.220637843012810 Acc: 0.600000023841858\n",
      "Epoch: 17920/25000............. Loss: 0.220669761300087 Acc: 0.600000023841858\n",
      "Epoch: 17930/25000............. Loss: 0.220637932419777 Acc: 0.600000023841858\n",
      "Epoch: 17940/25000............. Loss: 0.220636084675789 Acc: 0.600000023841858\n",
      "Epoch: 17950/25000............. Loss: 0.220635503530502 Acc: 0.600000023841858\n",
      "Epoch: 17960/25000............. Loss: 0.220633551478386 Acc: 0.600000023841858\n",
      "Epoch: 17970/25000............. Loss: 0.220632538199425 Acc: 0.600000023841858\n",
      "Epoch: 17980/25000............. Loss: 0.220633849501610 Acc: 0.600000023841858\n",
      "Epoch: 17990/25000............. Loss: 0.220768287777901 Acc: 0.600000023841858\n",
      "Epoch: 18000/25000............. Loss: 0.220649480819702 Acc: 0.600000023841858\n",
      "Epoch: 18010/25000............. Loss: 0.220700293779373 Acc: 0.600000023841858\n",
      "Epoch: 18020/25000............. Loss: 0.220632866024971 Acc: 0.600000023841858\n",
      "Epoch: 18030/25000............. Loss: 0.220632970333099 Acc: 0.600000023841858\n",
      "Epoch: 18040/25000............. Loss: 0.220630690455437 Acc: 0.600000023841858\n",
      "Epoch: 18050/25000............. Loss: 0.220627084374428 Acc: 0.600000023841858\n",
      "Epoch: 18060/25000............. Loss: 0.220627218484879 Acc: 0.600000023841858\n",
      "Epoch: 18070/25000............. Loss: 0.220700979232788 Acc: 0.600000023841858\n",
      "Epoch: 18080/25000............. Loss: 0.220641165971756 Acc: 0.600000023841858\n",
      "Epoch: 18090/25000............. Loss: 0.220671683549881 Acc: 0.600000023841858\n",
      "Epoch: 18100/25000............. Loss: 0.220643445849419 Acc: 0.600000023841858\n",
      "Epoch: 18110/25000............. Loss: 0.220623761415482 Acc: 0.600000023841858\n",
      "Epoch: 18120/25000............. Loss: 0.220625907182693 Acc: 0.600000023841858\n",
      "Epoch: 18130/25000............. Loss: 0.220621928572655 Acc: 0.600000023841858\n",
      "Epoch: 18140/25000............. Loss: 0.220620498061180 Acc: 0.600000023841858\n",
      "Epoch: 18150/25000............. Loss: 0.220619812607765 Acc: 0.600000023841858\n",
      "Epoch: 18160/25000............. Loss: 0.220619142055511 Acc: 0.600000023841858\n",
      "Epoch: 18170/25000............. Loss: 0.220619484782219 Acc: 0.600000023841858\n",
      "Epoch: 18180/25000............. Loss: 0.220794603228569 Acc: 0.600000023841858\n",
      "Epoch: 18190/25000............. Loss: 0.220780476927757 Acc: 0.600000023841858\n",
      "Epoch: 18200/25000............. Loss: 0.220625147223473 Acc: 0.600000023841858\n",
      "Epoch: 18210/25000............. Loss: 0.220646828413010 Acc: 0.600000023841858\n",
      "Epoch: 18220/25000............. Loss: 0.220624342560768 Acc: 0.600000023841858\n",
      "Epoch: 18230/25000............. Loss: 0.220614433288574 Acc: 0.600000023841858\n",
      "Epoch: 18240/25000............. Loss: 0.220618769526482 Acc: 0.600000023841858\n",
      "Epoch: 18250/25000............. Loss: 0.220796242356300 Acc: 0.600000023841858\n",
      "Epoch: 18260/25000............. Loss: 0.220662906765938 Acc: 0.600000023841858\n",
      "Epoch: 18270/25000............. Loss: 0.220670923590660 Acc: 0.600000023841858\n",
      "Epoch: 18280/25000............. Loss: 0.220616176724434 Acc: 0.600000023841858\n",
      "Epoch: 18290/25000............. Loss: 0.220611631870270 Acc: 0.600000023841858\n",
      "Epoch: 18300/25000............. Loss: 0.220612049102783 Acc: 0.600000023841858\n",
      "Epoch: 18310/25000............. Loss: 0.220609933137894 Acc: 0.600000023841858\n",
      "Epoch: 18320/25000............. Loss: 0.220608532428741 Acc: 0.600000023841858\n",
      "Epoch: 18330/25000............. Loss: 0.220607653260231 Acc: 0.600000023841858\n",
      "Epoch: 18340/25000............. Loss: 0.220606863498688 Acc: 0.600000023841858\n",
      "Epoch: 18350/25000............. Loss: 0.220606163144112 Acc: 0.600000023841858\n",
      "Epoch: 18360/25000............. Loss: 0.220605432987213 Acc: 0.600000023841858\n",
      "Epoch: 18370/25000............. Loss: 0.220604762434959 Acc: 0.600000023841858\n",
      "Epoch: 18380/25000............. Loss: 0.220604076981544 Acc: 0.600000023841858\n",
      "Epoch: 18390/25000............. Loss: 0.220603406429291 Acc: 0.600000023841858\n",
      "Epoch: 18400/25000............. Loss: 0.220603570342064 Acc: 0.600000023841858\n",
      "Epoch: 18410/25000............. Loss: 0.220646336674690 Acc: 0.600000023841858\n",
      "Epoch: 18420/25000............. Loss: 0.221270889043808 Acc: 0.600000023841858\n",
      "Epoch: 18430/25000............. Loss: 0.220730692148209 Acc: 0.600000023841858\n",
      "Epoch: 18440/25000............. Loss: 0.220657438039780 Acc: 0.600000023841858\n",
      "Epoch: 18450/25000............. Loss: 0.220612078905106 Acc: 0.600000023841858\n",
      "Epoch: 18460/25000............. Loss: 0.220608338713646 Acc: 0.600000023841858\n",
      "Epoch: 18470/25000............. Loss: 0.220598086714745 Acc: 0.600000023841858\n",
      "Epoch: 18480/25000............. Loss: 0.220598459243774 Acc: 0.600000023841858\n",
      "Epoch: 18490/25000............. Loss: 0.220596700906754 Acc: 0.600000023841858\n",
      "Epoch: 18500/25000............. Loss: 0.220596104860306 Acc: 0.600000023841858\n",
      "Epoch: 18510/25000............. Loss: 0.220595404505730 Acc: 0.600000023841858\n",
      "Epoch: 18520/25000............. Loss: 0.220594704151154 Acc: 0.600000023841858\n",
      "Epoch: 18530/25000............. Loss: 0.220593988895416 Acc: 0.600000023841858\n",
      "Epoch: 18540/25000............. Loss: 0.220593318343163 Acc: 0.600000023841858\n",
      "Epoch: 18550/25000............. Loss: 0.220592647790909 Acc: 0.600000023841858\n",
      "Epoch: 18560/25000............. Loss: 0.220592126250267 Acc: 0.600000023841858\n",
      "Epoch: 18570/25000............. Loss: 0.220598131418228 Acc: 0.600000023841858\n",
      "Epoch: 18580/25000............. Loss: 0.220910444855690 Acc: 0.600000023841858\n",
      "Epoch: 18590/25000............. Loss: 0.220732197165489 Acc: 0.600000023841858\n",
      "Epoch: 18600/25000............. Loss: 0.220605835318565 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18610/25000............. Loss: 0.220594927668571 Acc: 0.600000023841858\n",
      "Epoch: 18620/25000............. Loss: 0.220600560307503 Acc: 0.600000023841858\n",
      "Epoch: 18630/25000............. Loss: 0.220870092511177 Acc: 0.600000023841858\n",
      "Epoch: 18640/25000............. Loss: 0.220722302794456 Acc: 0.600000023841858\n",
      "Epoch: 18650/25000............. Loss: 0.220613807439804 Acc: 0.600000023841858\n",
      "Epoch: 18660/25000............. Loss: 0.220588058233261 Acc: 0.600000023841858\n",
      "Epoch: 18670/25000............. Loss: 0.220591679215431 Acc: 0.600000023841858\n",
      "Epoch: 18680/25000............. Loss: 0.220585390925407 Acc: 0.600000023841858\n",
      "Epoch: 18690/25000............. Loss: 0.220583349466324 Acc: 0.600000023841858\n",
      "Epoch: 18700/25000............. Loss: 0.220582678914070 Acc: 0.600000023841858\n",
      "Epoch: 18710/25000............. Loss: 0.220581978559494 Acc: 0.600000023841858\n",
      "Epoch: 18720/25000............. Loss: 0.220581352710724 Acc: 0.600000023841858\n",
      "Epoch: 18730/25000............. Loss: 0.220580801367760 Acc: 0.600000023841858\n",
      "Epoch: 18740/25000............. Loss: 0.220583930611610 Acc: 0.600000023841858\n",
      "Epoch: 18750/25000............. Loss: 0.220772802829742 Acc: 0.600000023841858\n",
      "Epoch: 18760/25000............. Loss: 0.220643058419228 Acc: 0.600000023841858\n",
      "Epoch: 18770/25000............. Loss: 0.220635235309601 Acc: 0.600000023841858\n",
      "Epoch: 18780/25000............. Loss: 0.220580682158470 Acc: 0.600000023841858\n",
      "Epoch: 18790/25000............. Loss: 0.220582082867622 Acc: 0.600000023841858\n",
      "Epoch: 18800/25000............. Loss: 0.220759198069572 Acc: 0.600000023841858\n",
      "Epoch: 18810/25000............. Loss: 0.220629975199699 Acc: 0.600000023841858\n",
      "Epoch: 18820/25000............. Loss: 0.220637530088425 Acc: 0.600000023841858\n",
      "Epoch: 18830/25000............. Loss: 0.220576554536819 Acc: 0.600000023841858\n",
      "Epoch: 18840/25000............. Loss: 0.220577180385590 Acc: 0.600000023841858\n",
      "Epoch: 18850/25000............. Loss: 0.220575839281082 Acc: 0.600000023841858\n",
      "Epoch: 18860/25000............. Loss: 0.220572814345360 Acc: 0.600000023841858\n",
      "Epoch: 18870/25000............. Loss: 0.220571547746658 Acc: 0.600000023841858\n",
      "Epoch: 18880/25000............. Loss: 0.220570847392082 Acc: 0.600000023841858\n",
      "Epoch: 18890/25000............. Loss: 0.220570251345634 Acc: 0.600000023841858\n",
      "Epoch: 18900/25000............. Loss: 0.220570370554924 Acc: 0.600000023841858\n",
      "Epoch: 18910/25000............. Loss: 0.220604956150055 Acc: 0.600000023841858\n",
      "Epoch: 18920/25000............. Loss: 0.220762923359871 Acc: 0.600000023841858\n",
      "Epoch: 18930/25000............. Loss: 0.220574870705605 Acc: 0.600000023841858\n",
      "Epoch: 18940/25000............. Loss: 0.220580756664276 Acc: 0.600000023841858\n",
      "Epoch: 18950/25000............. Loss: 0.220577761530876 Acc: 0.600000023841858\n",
      "Epoch: 18960/25000............. Loss: 0.220567360520363 Acc: 0.600000023841858\n",
      "Epoch: 18970/25000............. Loss: 0.220567747950554 Acc: 0.600000023841858\n",
      "Epoch: 18980/25000............. Loss: 0.220852926373482 Acc: 0.600000023841858\n",
      "Epoch: 18990/25000............. Loss: 0.220774695277214 Acc: 0.600000023841858\n",
      "Epoch: 19000/25000............. Loss: 0.220563545823097 Acc: 0.600000023841858\n",
      "Epoch: 19010/25000............. Loss: 0.220587983727455 Acc: 0.600000023841858\n",
      "Epoch: 19020/25000............. Loss: 0.220564037561417 Acc: 0.600000023841858\n",
      "Epoch: 19030/25000............. Loss: 0.220563545823097 Acc: 0.600000023841858\n",
      "Epoch: 19040/25000............. Loss: 0.220560774207115 Acc: 0.600000023841858\n",
      "Epoch: 19050/25000............. Loss: 0.220560297369957 Acc: 0.600000023841858\n",
      "Epoch: 19060/25000............. Loss: 0.220559209585190 Acc: 0.600000023841858\n",
      "Epoch: 19070/25000............. Loss: 0.220559358596802 Acc: 0.600000023841858\n",
      "Epoch: 19080/25000............. Loss: 0.220625743269920 Acc: 0.600000023841858\n",
      "Epoch: 19090/25000............. Loss: 0.220576480031013 Acc: 0.600000023841858\n",
      "Epoch: 19100/25000............. Loss: 0.220632076263428 Acc: 0.600000023841858\n",
      "Epoch: 19110/25000............. Loss: 0.220562160015106 Acc: 0.600000023841858\n",
      "Epoch: 19120/25000............. Loss: 0.220565825700760 Acc: 0.600000023841858\n",
      "Epoch: 19130/25000............. Loss: 0.220555976033211 Acc: 0.600000023841858\n",
      "Epoch: 19140/25000............. Loss: 0.220554798841476 Acc: 0.600000023841858\n",
      "Epoch: 19150/25000............. Loss: 0.220554068684578 Acc: 0.600000023841858\n",
      "Epoch: 19160/25000............. Loss: 0.220552921295166 Acc: 0.600000023841858\n",
      "Epoch: 19170/25000............. Loss: 0.220552474260330 Acc: 0.600000023841858\n",
      "Epoch: 19180/25000............. Loss: 0.220563247799873 Acc: 0.600000023841858\n",
      "Epoch: 19190/25000............. Loss: 0.220930278301239 Acc: 0.600000023841858\n",
      "Epoch: 19200/25000............. Loss: 0.220664024353027 Acc: 0.600000023841858\n",
      "Epoch: 19210/25000............. Loss: 0.220549821853638 Acc: 0.600000023841858\n",
      "Epoch: 19220/25000............. Loss: 0.220562368631363 Acc: 0.600000023841858\n",
      "Epoch: 19230/25000............. Loss: 0.220552518963814 Acc: 0.600000023841858\n",
      "Epoch: 19240/25000............. Loss: 0.220547825098038 Acc: 0.600000023841858\n",
      "Epoch: 19250/25000............. Loss: 0.220547273755074 Acc: 0.600000023841858\n",
      "Epoch: 19260/25000............. Loss: 0.220546677708626 Acc: 0.600000023841858\n",
      "Epoch: 19270/25000............. Loss: 0.220546782016754 Acc: 0.600000023841858\n",
      "Epoch: 19280/25000............. Loss: 0.220626890659332 Acc: 0.600000023841858\n",
      "Epoch: 19290/25000............. Loss: 0.220545917749405 Acc: 0.600000023841858\n",
      "Epoch: 19300/25000............. Loss: 0.220638558268547 Acc: 0.600000023841858\n",
      "Epoch: 19310/25000............. Loss: 0.220543429255486 Acc: 0.600000023841858\n",
      "Epoch: 19320/25000............. Loss: 0.220555126667023 Acc: 0.600000023841858\n",
      "Epoch: 19330/25000............. Loss: 0.220542103052139 Acc: 0.600000023841858\n",
      "Epoch: 19340/25000............. Loss: 0.220543116331100 Acc: 0.600000023841858\n",
      "Epoch: 19350/25000............. Loss: 0.220542848110199 Acc: 0.600000023841858\n",
      "Epoch: 19360/25000............. Loss: 0.220663741230965 Acc: 0.600000023841858\n",
      "Epoch: 19370/25000............. Loss: 0.220552578568459 Acc: 0.600000023841858\n",
      "Epoch: 19380/25000............. Loss: 0.220613718032837 Acc: 0.600000023841858\n",
      "Epoch: 19390/25000............. Loss: 0.220540776848793 Acc: 0.600000023841858\n",
      "Epoch: 19400/25000............. Loss: 0.220544964075089 Acc: 0.600000023841858\n",
      "Epoch: 19410/25000............. Loss: 0.220539703965187 Acc: 0.600000023841858\n",
      "Epoch: 19420/25000............. Loss: 0.220536530017853 Acc: 0.600000023841858\n",
      "Epoch: 19430/25000............. Loss: 0.220536068081856 Acc: 0.600000023841858\n",
      "Epoch: 19440/25000............. Loss: 0.220535457134247 Acc: 0.600000023841858\n",
      "Epoch: 19450/25000............. Loss: 0.220534741878510 Acc: 0.600000023841858\n",
      "Epoch: 19460/25000............. Loss: 0.220534130930901 Acc: 0.600000023841858\n",
      "Epoch: 19470/25000............. Loss: 0.220535933971405 Acc: 0.600000023841858\n",
      "Epoch: 19480/25000............. Loss: 0.220680028200150 Acc: 0.600000023841858\n",
      "Epoch: 19490/25000............. Loss: 0.220559433102608 Acc: 0.600000023841858\n",
      "Epoch: 19500/25000............. Loss: 0.220597326755524 Acc: 0.600000023841858\n",
      "Epoch: 19510/25000............. Loss: 0.220535740256310 Acc: 0.600000023841858\n",
      "Epoch: 19520/25000............. Loss: 0.220533609390259 Acc: 0.600000023841858\n",
      "Epoch: 19530/25000............. Loss: 0.220533043146133 Acc: 0.600000023841858\n",
      "Epoch: 19540/25000............. Loss: 0.220537796616554 Acc: 0.600000023841858\n",
      "Epoch: 19550/25000............. Loss: 0.220971524715424 Acc: 0.600000023841858\n",
      "Epoch: 19560/25000............. Loss: 0.220655694603920 Acc: 0.600000023841858\n",
      "Epoch: 19570/25000............. Loss: 0.220537573099136 Acc: 0.600000023841858\n",
      "Epoch: 19580/25000............. Loss: 0.220542386174202 Acc: 0.600000023841858\n",
      "Epoch: 19590/25000............. Loss: 0.220527261495590 Acc: 0.600000023841858\n",
      "Epoch: 19600/25000............. Loss: 0.220528021454811 Acc: 0.600000023841858\n",
      "Epoch: 19610/25000............. Loss: 0.220524892210960 Acc: 0.600000023841858\n",
      "Epoch: 19620/25000............. Loss: 0.220524296164513 Acc: 0.600000023841858\n",
      "Epoch: 19630/25000............. Loss: 0.220523744821548 Acc: 0.600000023841858\n",
      "Epoch: 19640/25000............. Loss: 0.220523610711098 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19650/25000............. Loss: 0.220555916428566 Acc: 0.600000023841858\n",
      "Epoch: 19660/25000............. Loss: 0.220745787024498 Acc: 0.600000023841858\n",
      "Epoch: 19670/25000............. Loss: 0.220521628856659 Acc: 0.600000023841858\n",
      "Epoch: 19680/25000............. Loss: 0.220551386475563 Acc: 0.600000023841858\n",
      "Epoch: 19690/25000............. Loss: 0.220525518059731 Acc: 0.600000023841858\n",
      "Epoch: 19700/25000............. Loss: 0.220519915223122 Acc: 0.600000023841858\n",
      "Epoch: 19710/25000............. Loss: 0.220520272850990 Acc: 0.600000023841858\n",
      "Epoch: 19720/25000............. Loss: 0.220518693327904 Acc: 0.600000023841858\n",
      "Epoch: 19730/25000............. Loss: 0.220517739653587 Acc: 0.600000023841858\n",
      "Epoch: 19740/25000............. Loss: 0.220526158809662 Acc: 0.600000023841858\n",
      "Epoch: 19750/25000............. Loss: 0.220992624759674 Acc: 0.600000023841858\n",
      "Epoch: 19760/25000............. Loss: 0.220597758889198 Acc: 0.600000023841858\n",
      "Epoch: 19770/25000............. Loss: 0.220550358295441 Acc: 0.600000023841858\n",
      "Epoch: 19780/25000............. Loss: 0.220518663525581 Acc: 0.600000023841858\n",
      "Epoch: 19790/25000............. Loss: 0.220520123839378 Acc: 0.600000023841858\n",
      "Epoch: 19800/25000............. Loss: 0.220513835549355 Acc: 0.600000023841858\n",
      "Epoch: 19810/25000............. Loss: 0.220513164997101 Acc: 0.600000023841858\n",
      "Epoch: 19820/25000............. Loss: 0.220512598752975 Acc: 0.600000023841858\n",
      "Epoch: 19830/25000............. Loss: 0.220527380704880 Acc: 0.600000023841858\n",
      "Epoch: 19840/25000............. Loss: 0.220942676067352 Acc: 0.600000023841858\n",
      "Epoch: 19850/25000............. Loss: 0.220524415373802 Acc: 0.600000023841858\n",
      "Epoch: 19860/25000............. Loss: 0.220556214451790 Acc: 0.600000023841858\n",
      "Epoch: 19870/25000............. Loss: 0.220509365200996 Acc: 0.600000023841858\n",
      "Epoch: 19880/25000............. Loss: 0.220514744520187 Acc: 0.600000023841858\n",
      "Epoch: 19890/25000............. Loss: 0.220508158206940 Acc: 0.600000023841858\n",
      "Epoch: 19900/25000............. Loss: 0.220507800579071 Acc: 0.600000023841858\n",
      "Epoch: 19910/25000............. Loss: 0.220507010817528 Acc: 0.600000023841858\n",
      "Epoch: 19920/25000............. Loss: 0.220506161451340 Acc: 0.600000023841858\n",
      "Epoch: 19930/25000............. Loss: 0.220505505800247 Acc: 0.600000023841858\n",
      "Epoch: 19940/25000............. Loss: 0.220505446195602 Acc: 0.600000023841858\n",
      "Epoch: 19950/25000............. Loss: 0.220550820231438 Acc: 0.600000023841858\n",
      "Epoch: 19960/25000............. Loss: 0.220600187778473 Acc: 0.600000023841858\n",
      "Epoch: 19970/25000............. Loss: 0.220559284090996 Acc: 0.600000023841858\n",
      "Epoch: 19980/25000............. Loss: 0.220516264438629 Acc: 0.600000023841858\n",
      "Epoch: 19990/25000............. Loss: 0.220512673258781 Acc: 0.600000023841858\n",
      "Epoch: 20000/25000............. Loss: 0.220502674579620 Acc: 0.600000023841858\n",
      "Epoch: 20010/25000............. Loss: 0.220501825213432 Acc: 0.600000023841858\n",
      "Epoch: 20020/25000............. Loss: 0.220501229166985 Acc: 0.600000023841858\n",
      "Epoch: 20030/25000............. Loss: 0.220527797937393 Acc: 0.600000023841858\n",
      "Epoch: 20040/25000............. Loss: 0.220777988433838 Acc: 0.600000023841858\n",
      "Epoch: 20050/25000............. Loss: 0.220499768853188 Acc: 0.600000023841858\n",
      "Epoch: 20060/25000............. Loss: 0.220533117651939 Acc: 0.600000023841858\n",
      "Epoch: 20070/25000............. Loss: 0.220500722527504 Acc: 0.600000023841858\n",
      "Epoch: 20080/25000............. Loss: 0.220498800277710 Acc: 0.600000023841858\n",
      "Epoch: 20090/25000............. Loss: 0.220497936010361 Acc: 0.600000023841858\n",
      "Epoch: 20100/25000............. Loss: 0.220495745539665 Acc: 0.600000023841858\n",
      "Epoch: 20110/25000............. Loss: 0.220494911074638 Acc: 0.600000023841858\n",
      "Epoch: 20120/25000............. Loss: 0.220494315028191 Acc: 0.600000023841858\n",
      "Epoch: 20130/25000............. Loss: 0.220493718981743 Acc: 0.600000023841858\n",
      "Epoch: 20140/25000............. Loss: 0.220493257045746 Acc: 0.600000023841858\n",
      "Epoch: 20150/25000............. Loss: 0.220500051975250 Acc: 0.600000023841858\n",
      "Epoch: 20160/25000............. Loss: 0.220944419503212 Acc: 0.600000023841858\n",
      "Epoch: 20170/25000............. Loss: 0.220621481537819 Acc: 0.600000023841858\n",
      "Epoch: 20180/25000............. Loss: 0.220505580306053 Acc: 0.600000023841858\n",
      "Epoch: 20190/25000............. Loss: 0.220503464341164 Acc: 0.600000023841858\n",
      "Epoch: 20200/25000............. Loss: 0.220492526888847 Acc: 0.600000023841858\n",
      "Epoch: 20210/25000............. Loss: 0.220493838191032 Acc: 0.600000023841858\n",
      "Epoch: 20220/25000............. Loss: 0.220704153180122 Acc: 0.600000023841858\n",
      "Epoch: 20230/25000............. Loss: 0.220604240894318 Acc: 0.600000023841858\n",
      "Epoch: 20240/25000............. Loss: 0.220530122518539 Acc: 0.600000023841858\n",
      "Epoch: 20250/25000............. Loss: 0.220496118068695 Acc: 0.600000023841858\n",
      "Epoch: 20260/25000............. Loss: 0.220492914319038 Acc: 0.600000023841858\n",
      "Epoch: 20270/25000............. Loss: 0.220485776662827 Acc: 0.600000023841858\n",
      "Epoch: 20280/25000............. Loss: 0.220486164093018 Acc: 0.600000023841858\n",
      "Epoch: 20290/25000............. Loss: 0.220484763383865 Acc: 0.600000023841858\n",
      "Epoch: 20300/25000............. Loss: 0.220483914017677 Acc: 0.600000023841858\n",
      "Epoch: 20310/25000............. Loss: 0.220483288168907 Acc: 0.600000023841858\n",
      "Epoch: 20320/25000............. Loss: 0.220482677221298 Acc: 0.600000023841858\n",
      "Epoch: 20330/25000............. Loss: 0.220482125878334 Acc: 0.600000023841858\n",
      "Epoch: 20340/25000............. Loss: 0.220481529831886 Acc: 0.600000023841858\n",
      "Epoch: 20350/25000............. Loss: 0.220481082797050 Acc: 0.600000023841858\n",
      "Epoch: 20360/25000............. Loss: 0.220498904585838 Acc: 0.600000023841858\n",
      "Epoch: 20370/25000............. Loss: 0.220898181200027 Acc: 0.600000023841858\n",
      "Epoch: 20380/25000............. Loss: 0.220495492219925 Acc: 0.600000023841858\n",
      "Epoch: 20390/25000............. Loss: 0.220504283905029 Acc: 0.600000023841858\n",
      "Epoch: 20400/25000............. Loss: 0.220496311783791 Acc: 0.600000023841858\n",
      "Epoch: 20410/25000............. Loss: 0.220477953553200 Acc: 0.600000023841858\n",
      "Epoch: 20420/25000............. Loss: 0.220521256327629 Acc: 0.600000023841858\n",
      "Epoch: 20430/25000............. Loss: 0.220546185970306 Acc: 0.600000023841858\n",
      "Epoch: 20440/25000............. Loss: 0.220616579055786 Acc: 0.600000023841858\n",
      "Epoch: 20450/25000............. Loss: 0.220496401190758 Acc: 0.600000023841858\n",
      "Epoch: 20460/25000............. Loss: 0.220474764704704 Acc: 0.600000023841858\n",
      "Epoch: 20470/25000............. Loss: 0.220478340983391 Acc: 0.600000023841858\n",
      "Epoch: 20480/25000............. Loss: 0.220475479960442 Acc: 0.600000023841858\n",
      "Epoch: 20490/25000............. Loss: 0.220473021268845 Acc: 0.600000023841858\n",
      "Epoch: 20500/25000............. Loss: 0.220472782850266 Acc: 0.600000023841858\n",
      "Epoch: 20510/25000............. Loss: 0.220471873879433 Acc: 0.600000023841858\n",
      "Epoch: 20520/25000............. Loss: 0.220471352338791 Acc: 0.600000023841858\n",
      "Epoch: 20530/25000............. Loss: 0.220470771193504 Acc: 0.600000023841858\n",
      "Epoch: 20540/25000............. Loss: 0.220470190048218 Acc: 0.600000023841858\n",
      "Epoch: 20550/25000............. Loss: 0.220469623804092 Acc: 0.600000023841858\n",
      "Epoch: 20560/25000............. Loss: 0.220469057559967 Acc: 0.600000023841858\n",
      "Epoch: 20570/25000............. Loss: 0.220468506217003 Acc: 0.600000023841858\n",
      "Epoch: 20580/25000............. Loss: 0.220468372106552 Acc: 0.600000023841858\n",
      "Epoch: 20590/25000............. Loss: 0.220521479845047 Acc: 0.600000023841858\n",
      "Epoch: 20600/25000............. Loss: 0.220511049032211 Acc: 0.600000023841858\n",
      "Epoch: 20610/25000............. Loss: 0.220569089055061 Acc: 0.600000023841858\n",
      "Epoch: 20620/25000............. Loss: 0.220465660095215 Acc: 0.600000023841858\n",
      "Epoch: 20630/25000............. Loss: 0.220478147268295 Acc: 0.600000023841858\n",
      "Epoch: 20640/25000............. Loss: 0.220466107130051 Acc: 0.600000023841858\n",
      "Epoch: 20650/25000............. Loss: 0.220465421676636 Acc: 0.600000023841858\n",
      "Epoch: 20660/25000............. Loss: 0.220473751425743 Acc: 0.600000023841858\n",
      "Epoch: 20670/25000............. Loss: 0.220965489745140 Acc: 0.600000023841858\n",
      "Epoch: 20680/25000............. Loss: 0.220507919788361 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20690/25000............. Loss: 0.220513358712196 Acc: 0.600000023841858\n",
      "Epoch: 20700/25000............. Loss: 0.220461219549179 Acc: 0.600000023841858\n",
      "Epoch: 20710/25000............. Loss: 0.220467850565910 Acc: 0.600000023841858\n",
      "Epoch: 20720/25000............. Loss: 0.220460191369057 Acc: 0.600000023841858\n",
      "Epoch: 20730/25000............. Loss: 0.220460548996925 Acc: 0.600000023841858\n",
      "Epoch: 20740/25000............. Loss: 0.220459043979645 Acc: 0.600000023841858\n",
      "Epoch: 20750/25000............. Loss: 0.220458477735519 Acc: 0.600000023841858\n",
      "Epoch: 20760/25000............. Loss: 0.220457926392555 Acc: 0.600000023841858\n",
      "Epoch: 20770/25000............. Loss: 0.220457330346107 Acc: 0.600000023841858\n",
      "Epoch: 20780/25000............. Loss: 0.220456779003143 Acc: 0.600000023841858\n",
      "Epoch: 20790/25000............. Loss: 0.220456317067146 Acc: 0.600000023841858\n",
      "Epoch: 20800/25000............. Loss: 0.220460399985313 Acc: 0.600000023841858\n",
      "Epoch: 20810/25000............. Loss: 0.220797479152679 Acc: 0.600000023841858\n",
      "Epoch: 20820/25000............. Loss: 0.220640331506729 Acc: 0.600000023841858\n",
      "Epoch: 20830/25000............. Loss: 0.220454290509224 Acc: 0.600000023841858\n",
      "Epoch: 20840/25000............. Loss: 0.220476120710373 Acc: 0.600000023841858\n",
      "Epoch: 20850/25000............. Loss: 0.220452934503555 Acc: 0.600000023841858\n",
      "Epoch: 20860/25000............. Loss: 0.220457300543785 Acc: 0.600000023841858\n",
      "Epoch: 20870/25000............. Loss: 0.220561817288399 Acc: 0.600000023841858\n",
      "Epoch: 20880/25000............. Loss: 0.220451459288597 Acc: 0.600000023841858\n",
      "Epoch: 20890/25000............. Loss: 0.220497429370880 Acc: 0.600000023841858\n",
      "Epoch: 20900/25000............. Loss: 0.220469608902931 Acc: 0.600000023841858\n",
      "Epoch: 20910/25000............. Loss: 0.220449671149254 Acc: 0.600000023841858\n",
      "Epoch: 20920/25000............. Loss: 0.220450535416603 Acc: 0.600000023841858\n",
      "Epoch: 20930/25000............. Loss: 0.220449656248093 Acc: 0.600000023841858\n",
      "Epoch: 20940/25000............. Loss: 0.220448419451714 Acc: 0.600000023841858\n",
      "Epoch: 20950/25000............. Loss: 0.220447570085526 Acc: 0.600000023841858\n",
      "Epoch: 20960/25000............. Loss: 0.220446899533272 Acc: 0.600000023841858\n",
      "Epoch: 20970/25000............. Loss: 0.220446303486824 Acc: 0.600000023841858\n",
      "Epoch: 20980/25000............. Loss: 0.220445737242699 Acc: 0.600000023841858\n",
      "Epoch: 20990/25000............. Loss: 0.220445662736893 Acc: 0.600000023841858\n",
      "Epoch: 21000/25000............. Loss: 0.220475733280182 Acc: 0.600000023841858\n",
      "Epoch: 21010/25000............. Loss: 0.220682367682457 Acc: 0.600000023841858\n",
      "Epoch: 21020/25000............. Loss: 0.220443904399872 Acc: 0.600000023841858\n",
      "Epoch: 21030/25000............. Loss: 0.220494359731674 Acc: 0.600000023841858\n",
      "Epoch: 21040/25000............. Loss: 0.220793589949608 Acc: 0.600000023841858\n",
      "Epoch: 21050/25000............. Loss: 0.220540314912796 Acc: 0.600000023841858\n",
      "Epoch: 21060/25000............. Loss: 0.220444351434708 Acc: 0.600000023841858\n",
      "Epoch: 21070/25000............. Loss: 0.220448762178421 Acc: 0.600000023841858\n",
      "Epoch: 21080/25000............. Loss: 0.220445558428764 Acc: 0.600000023841858\n",
      "Epoch: 21090/25000............. Loss: 0.220440849661827 Acc: 0.600000023841858\n",
      "Epoch: 21100/25000............. Loss: 0.220439374446869 Acc: 0.600000023841858\n",
      "Epoch: 21110/25000............. Loss: 0.220438688993454 Acc: 0.600000023841858\n",
      "Epoch: 21120/25000............. Loss: 0.220438152551651 Acc: 0.600000023841858\n",
      "Epoch: 21130/25000............. Loss: 0.220437586307526 Acc: 0.600000023841858\n",
      "Epoch: 21140/25000............. Loss: 0.220437005162239 Acc: 0.600000023841858\n",
      "Epoch: 21150/25000............. Loss: 0.220436438918114 Acc: 0.600000023841858\n",
      "Epoch: 21160/25000............. Loss: 0.220435902476311 Acc: 0.600000023841858\n",
      "Epoch: 21170/25000............. Loss: 0.220435604453087 Acc: 0.600000023841858\n",
      "Epoch: 21180/25000............. Loss: 0.220447704195976 Acc: 0.600000023841858\n",
      "Epoch: 21190/25000............. Loss: 0.220828235149384 Acc: 0.600000023841858\n",
      "Epoch: 21200/25000............. Loss: 0.220706626772881 Acc: 0.600000023841858\n",
      "Epoch: 21210/25000............. Loss: 0.220492646098137 Acc: 0.600000023841858\n",
      "Epoch: 21220/25000............. Loss: 0.220511764287949 Acc: 0.600000023841858\n",
      "Epoch: 21230/25000............. Loss: 0.220434695482254 Acc: 0.600000023841858\n",
      "Epoch: 21240/25000............. Loss: 0.220436543226242 Acc: 0.600000023841858\n",
      "Epoch: 21250/25000............. Loss: 0.220434814691544 Acc: 0.600000023841858\n",
      "Epoch: 21260/25000............. Loss: 0.220431357622147 Acc: 0.600000023841858\n",
      "Epoch: 21270/25000............. Loss: 0.220430150628090 Acc: 0.600000023841858\n",
      "Epoch: 21280/25000............. Loss: 0.220429494976997 Acc: 0.600000023841858\n",
      "Epoch: 21290/25000............. Loss: 0.220428943634033 Acc: 0.600000023841858\n",
      "Epoch: 21300/25000............. Loss: 0.220428407192230 Acc: 0.600000023841858\n",
      "Epoch: 21310/25000............. Loss: 0.220427840948105 Acc: 0.600000023841858\n",
      "Epoch: 21320/25000............. Loss: 0.220427274703979 Acc: 0.600000023841858\n",
      "Epoch: 21330/25000............. Loss: 0.220426768064499 Acc: 0.600000023841858\n",
      "Epoch: 21340/25000............. Loss: 0.220426186919212 Acc: 0.600000023841858\n",
      "Epoch: 21350/25000............. Loss: 0.220425769686699 Acc: 0.600000023841858\n",
      "Epoch: 21360/25000............. Loss: 0.220428645610809 Acc: 0.600000023841858\n",
      "Epoch: 21370/25000............. Loss: 0.220672562718391 Acc: 0.600000023841858\n",
      "Epoch: 21380/25000............. Loss: 0.220463052392006 Acc: 0.600000023841858\n",
      "Epoch: 21390/25000............. Loss: 0.220561340451241 Acc: 0.600000023841858\n",
      "Epoch: 21400/25000............. Loss: 0.220445796847343 Acc: 0.600000023841858\n",
      "Epoch: 21410/25000............. Loss: 0.220429033041000 Acc: 0.600000023841858\n",
      "Epoch: 21420/25000............. Loss: 0.220426514744759 Acc: 0.600000023841858\n",
      "Epoch: 21430/25000............. Loss: 0.220423653721809 Acc: 0.600000023841858\n",
      "Epoch: 21440/25000............. Loss: 0.220421358942986 Acc: 0.600000023841858\n",
      "Epoch: 21450/25000............. Loss: 0.220420733094215 Acc: 0.600000023841858\n",
      "Epoch: 21460/25000............. Loss: 0.220419988036156 Acc: 0.600000023841858\n",
      "Epoch: 21470/25000............. Loss: 0.220419377088547 Acc: 0.600000023841858\n",
      "Epoch: 21480/25000............. Loss: 0.220418840646744 Acc: 0.600000023841858\n",
      "Epoch: 21490/25000............. Loss: 0.220418289303780 Acc: 0.600000023841858\n",
      "Epoch: 21500/25000............. Loss: 0.220417767763138 Acc: 0.600000023841858\n",
      "Epoch: 21510/25000............. Loss: 0.220417231321335 Acc: 0.600000023841858\n",
      "Epoch: 21520/25000............. Loss: 0.220416739583015 Acc: 0.600000023841858\n",
      "Epoch: 21530/25000............. Loss: 0.220418155193329 Acc: 0.600000023841858\n",
      "Epoch: 21540/25000............. Loss: 0.220546901226044 Acc: 0.600000023841858\n",
      "Epoch: 21550/25000............. Loss: 0.220432981848717 Acc: 0.600000023841858\n",
      "Epoch: 21560/25000............. Loss: 0.220486372709274 Acc: 0.600000023841858\n",
      "Epoch: 21570/25000............. Loss: 0.220424517989159 Acc: 0.600000023841858\n",
      "Epoch: 21580/25000............. Loss: 0.220827579498291 Acc: 0.600000023841858\n",
      "Epoch: 21590/25000............. Loss: 0.220560699701309 Acc: 0.600000023841858\n",
      "Epoch: 21600/25000............. Loss: 0.220414012670517 Acc: 0.600000023841858\n",
      "Epoch: 21610/25000............. Loss: 0.220431715250015 Acc: 0.600000023841858\n",
      "Epoch: 21620/25000............. Loss: 0.220411539077759 Acc: 0.600000023841858\n",
      "Epoch: 21630/25000............. Loss: 0.220413178205490 Acc: 0.600000023841858\n",
      "Epoch: 21640/25000............. Loss: 0.220411106944084 Acc: 0.600000023841858\n",
      "Epoch: 21650/25000............. Loss: 0.220409914851189 Acc: 0.600000023841858\n",
      "Epoch: 21660/25000............. Loss: 0.220409408211708 Acc: 0.600000023841858\n",
      "Epoch: 21670/25000............. Loss: 0.220408886671066 Acc: 0.600000023841858\n",
      "Epoch: 21680/25000............. Loss: 0.220408365130424 Acc: 0.600000023841858\n",
      "Epoch: 21690/25000............. Loss: 0.220407798886299 Acc: 0.600000023841858\n",
      "Epoch: 21700/25000............. Loss: 0.220407307147980 Acc: 0.600000023841858\n",
      "Epoch: 21710/25000............. Loss: 0.220407932996750 Acc: 0.600000023841858\n",
      "Epoch: 21720/25000............. Loss: 0.220489457249641 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21730/25000............. Loss: 0.220410004258156 Acc: 0.600000023841858\n",
      "Epoch: 21740/25000............. Loss: 0.220469400286674 Acc: 0.600000023841858\n",
      "Epoch: 21750/25000............. Loss: 0.220416307449341 Acc: 0.600000023841858\n",
      "Epoch: 21760/25000............. Loss: 0.220417410135269 Acc: 0.600000023841858\n",
      "Epoch: 21770/25000............. Loss: 0.220803737640381 Acc: 0.600000023841858\n",
      "Epoch: 21780/25000............. Loss: 0.220539227128029 Acc: 0.600000023841858\n",
      "Epoch: 21790/25000............. Loss: 0.220402851700783 Acc: 0.600000023841858\n",
      "Epoch: 21800/25000............. Loss: 0.220420524477959 Acc: 0.600000023841858\n",
      "Epoch: 21810/25000............. Loss: 0.220402926206589 Acc: 0.600000023841858\n",
      "Epoch: 21820/25000............. Loss: 0.220402002334595 Acc: 0.600000023841858\n",
      "Epoch: 21830/25000............. Loss: 0.220401540398598 Acc: 0.600000023841858\n",
      "Epoch: 21840/25000............. Loss: 0.220400348305702 Acc: 0.600000023841858\n",
      "Epoch: 21850/25000............. Loss: 0.220399633049965 Acc: 0.600000023841858\n",
      "Epoch: 21860/25000............. Loss: 0.220399066805840 Acc: 0.600000023841858\n",
      "Epoch: 21870/25000............. Loss: 0.220398560166359 Acc: 0.600000023841858\n",
      "Epoch: 21880/25000............. Loss: 0.220398023724556 Acc: 0.600000023841858\n",
      "Epoch: 21890/25000............. Loss: 0.220397546887398 Acc: 0.600000023841858\n",
      "Epoch: 21900/25000............. Loss: 0.220399677753448 Acc: 0.600000023841858\n",
      "Epoch: 21910/25000............. Loss: 0.220622435212135 Acc: 0.600000023841858\n",
      "Epoch: 21920/25000............. Loss: 0.220528393983841 Acc: 0.600000023841858\n",
      "Epoch: 21930/25000............. Loss: 0.220429077744484 Acc: 0.600000023841858\n",
      "Epoch: 21940/25000............. Loss: 0.220410883426666 Acc: 0.600000023841858\n",
      "Epoch: 21950/25000............. Loss: 0.220493063330650 Acc: 0.600000023841858\n",
      "Epoch: 21960/25000............. Loss: 0.220395013689995 Acc: 0.600000023841858\n",
      "Epoch: 21970/25000............. Loss: 0.220468819141388 Acc: 0.600000023841858\n",
      "Epoch: 21980/25000............. Loss: 0.220399290323257 Acc: 0.600000023841858\n",
      "Epoch: 21990/25000............. Loss: 0.220399245619774 Acc: 0.600000023841858\n",
      "Epoch: 22000/25000............. Loss: 0.220394745469093 Acc: 0.600000023841858\n",
      "Epoch: 22010/25000............. Loss: 0.220391422510147 Acc: 0.600000023841858\n",
      "Epoch: 22020/25000............. Loss: 0.220391184091568 Acc: 0.600000023841858\n",
      "Epoch: 22030/25000............. Loss: 0.220390588045120 Acc: 0.600000023841858\n",
      "Epoch: 22040/25000............. Loss: 0.220389977097511 Acc: 0.600000023841858\n",
      "Epoch: 22050/25000............. Loss: 0.220389425754547 Acc: 0.600000023841858\n",
      "Epoch: 22060/25000............. Loss: 0.220388904213905 Acc: 0.600000023841858\n",
      "Epoch: 22070/25000............. Loss: 0.220388382673264 Acc: 0.600000023841858\n",
      "Epoch: 22080/25000............. Loss: 0.220387935638428 Acc: 0.600000023841858\n",
      "Epoch: 22090/25000............. Loss: 0.220388829708099 Acc: 0.600000023841858\n",
      "Epoch: 22100/25000............. Loss: 0.220474630594254 Acc: 0.600000023841858\n",
      "Epoch: 22110/25000............. Loss: 0.220391318202019 Acc: 0.600000023841858\n",
      "Epoch: 22120/25000............. Loss: 0.220430538058281 Acc: 0.600000023841858\n",
      "Epoch: 22130/25000............. Loss: 0.220407947897911 Acc: 0.600000023841858\n",
      "Epoch: 22140/25000............. Loss: 0.220522105693817 Acc: 0.600000023841858\n",
      "Epoch: 22150/25000............. Loss: 0.220425516366959 Acc: 0.600000023841858\n",
      "Epoch: 22160/25000............. Loss: 0.220461338758469 Acc: 0.600000023841858\n",
      "Epoch: 22170/25000............. Loss: 0.220386013388634 Acc: 0.600000023841858\n",
      "Epoch: 22180/25000............. Loss: 0.220392704010010 Acc: 0.600000023841858\n",
      "Epoch: 22190/25000............. Loss: 0.220382407307625 Acc: 0.600000023841858\n",
      "Epoch: 22200/25000............. Loss: 0.220383167266846 Acc: 0.600000023841858\n",
      "Epoch: 22210/25000............. Loss: 0.220381677150726 Acc: 0.600000023841858\n",
      "Epoch: 22220/25000............. Loss: 0.220380872488022 Acc: 0.600000023841858\n",
      "Epoch: 22230/25000............. Loss: 0.220380365848541 Acc: 0.600000023841858\n",
      "Epoch: 22240/25000............. Loss: 0.220379844307899 Acc: 0.600000023841858\n",
      "Epoch: 22250/25000............. Loss: 0.220379337668419 Acc: 0.600000023841858\n",
      "Epoch: 22260/25000............. Loss: 0.220378845930099 Acc: 0.600000023841858\n",
      "Epoch: 22270/25000............. Loss: 0.220378622412682 Acc: 0.600000023841858\n",
      "Epoch: 22280/25000............. Loss: 0.220410123467445 Acc: 0.600000023841858\n",
      "Epoch: 22290/25000............. Loss: 0.220581248402596 Acc: 0.600000023841858\n",
      "Epoch: 22300/25000............. Loss: 0.220433473587036 Acc: 0.600000023841858\n",
      "Epoch: 22310/25000............. Loss: 0.220386281609535 Acc: 0.600000023841858\n",
      "Epoch: 22320/25000............. Loss: 0.220393046736717 Acc: 0.600000023841858\n",
      "Epoch: 22330/25000............. Loss: 0.220423161983490 Acc: 0.600000023841858\n",
      "Epoch: 22340/25000............. Loss: 0.220490217208862 Acc: 0.600000023841858\n",
      "Epoch: 22350/25000............. Loss: 0.220379605889320 Acc: 0.600000023841858\n",
      "Epoch: 22360/25000............. Loss: 0.220406249165535 Acc: 0.600000023841858\n",
      "Epoch: 22370/25000............. Loss: 0.220376208424568 Acc: 0.600000023841858\n",
      "Epoch: 22380/25000............. Loss: 0.220373913645744 Acc: 0.600000023841858\n",
      "Epoch: 22390/25000............. Loss: 0.220373943448067 Acc: 0.600000023841858\n",
      "Epoch: 22400/25000............. Loss: 0.220372527837753 Acc: 0.600000023841858\n",
      "Epoch: 22410/25000............. Loss: 0.220371633768082 Acc: 0.600000023841858\n",
      "Epoch: 22420/25000............. Loss: 0.220371037721634 Acc: 0.600000023841858\n",
      "Epoch: 22430/25000............. Loss: 0.220370516180992 Acc: 0.600000023841858\n",
      "Epoch: 22440/25000............. Loss: 0.220369979739189 Acc: 0.600000023841858\n",
      "Epoch: 22450/25000............. Loss: 0.220369488000870 Acc: 0.600000023841858\n",
      "Epoch: 22460/25000............. Loss: 0.220368966460228 Acc: 0.600000023841858\n",
      "Epoch: 22470/25000............. Loss: 0.220368474721909 Acc: 0.600000023841858\n",
      "Epoch: 22480/25000............. Loss: 0.220368042588234 Acc: 0.600000023841858\n",
      "Epoch: 22490/25000............. Loss: 0.220369920134544 Acc: 0.600000023841858\n",
      "Epoch: 22500/25000............. Loss: 0.220569327473640 Acc: 0.600000023841858\n",
      "Epoch: 22510/25000............. Loss: 0.220844298601151 Acc: 0.600000023841858\n",
      "Epoch: 22520/25000............. Loss: 0.220540627837181 Acc: 0.600000023841858\n",
      "Epoch: 22530/25000............. Loss: 0.220376685261726 Acc: 0.600000023841858\n",
      "Epoch: 22540/25000............. Loss: 0.220384821295738 Acc: 0.600000023841858\n",
      "Epoch: 22550/25000............. Loss: 0.220369368791580 Acc: 0.600000023841858\n",
      "Epoch: 22560/25000............. Loss: 0.220365419983864 Acc: 0.600000023841858\n",
      "Epoch: 22570/25000............. Loss: 0.220364004373550 Acc: 0.600000023841858\n",
      "Epoch: 22580/25000............. Loss: 0.220363348722458 Acc: 0.600000023841858\n",
      "Epoch: 22590/25000............. Loss: 0.220362737774849 Acc: 0.600000023841858\n",
      "Epoch: 22600/25000............. Loss: 0.220362201333046 Acc: 0.600000023841858\n",
      "Epoch: 22610/25000............. Loss: 0.220361694693565 Acc: 0.600000023841858\n",
      "Epoch: 22620/25000............. Loss: 0.220361202955246 Acc: 0.600000023841858\n",
      "Epoch: 22630/25000............. Loss: 0.220360711216927 Acc: 0.600000023841858\n",
      "Epoch: 22640/25000............. Loss: 0.220360234379768 Acc: 0.600000023841858\n",
      "Epoch: 22650/25000............. Loss: 0.220359742641449 Acc: 0.600000023841858\n",
      "Epoch: 22660/25000............. Loss: 0.220359250903130 Acc: 0.600000023841858\n",
      "Epoch: 22670/25000............. Loss: 0.220359161496162 Acc: 0.600000023841858\n",
      "Epoch: 22680/25000............. Loss: 0.220390543341637 Acc: 0.600000023841858\n",
      "Epoch: 22690/25000............. Loss: 0.220604673027992 Acc: 0.600000023841858\n",
      "Epoch: 22700/25000............. Loss: 0.220740571618080 Acc: 0.600000023841858\n",
      "Epoch: 22710/25000............. Loss: 0.220416814088821 Acc: 0.600000023841858\n",
      "Epoch: 22720/25000............. Loss: 0.220386669039726 Acc: 0.600000023841858\n",
      "Epoch: 22730/25000............. Loss: 0.220367446541786 Acc: 0.600000023841858\n",
      "Epoch: 22740/25000............. Loss: 0.220357835292816 Acc: 0.600000023841858\n",
      "Epoch: 22750/25000............. Loss: 0.220357015728951 Acc: 0.600000023841858\n",
      "Epoch: 22760/25000............. Loss: 0.220354855060577 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22770/25000............. Loss: 0.220354065299034 Acc: 0.600000023841858\n",
      "Epoch: 22780/25000............. Loss: 0.220353513956070 Acc: 0.600000023841858\n",
      "Epoch: 22790/25000............. Loss: 0.220353037118912 Acc: 0.600000023841858\n",
      "Epoch: 22800/25000............. Loss: 0.220352515578270 Acc: 0.600000023841858\n",
      "Epoch: 22810/25000............. Loss: 0.220352038741112 Acc: 0.600000023841858\n",
      "Epoch: 22820/25000............. Loss: 0.220351547002792 Acc: 0.600000023841858\n",
      "Epoch: 22830/25000............. Loss: 0.220351070165634 Acc: 0.600000023841858\n",
      "Epoch: 22840/25000............. Loss: 0.220350593328476 Acc: 0.600000023841858\n",
      "Epoch: 22850/25000............. Loss: 0.220350116491318 Acc: 0.600000023841858\n",
      "Epoch: 22860/25000............. Loss: 0.220349818468094 Acc: 0.600000023841858\n",
      "Epoch: 22870/25000............. Loss: 0.220360964536667 Acc: 0.600000023841858\n",
      "Epoch: 22880/25000............. Loss: 0.220927566289902 Acc: 0.600000023841858\n",
      "Epoch: 22890/25000............. Loss: 0.220424622297287 Acc: 0.600000023841858\n",
      "Epoch: 22900/25000............. Loss: 0.220448091626167 Acc: 0.600000023841858\n",
      "Epoch: 22910/25000............. Loss: 0.220361322164536 Acc: 0.600000023841858\n",
      "Epoch: 22920/25000............. Loss: 0.220356315374374 Acc: 0.600000023841858\n",
      "Epoch: 22930/25000............. Loss: 0.220351114869118 Acc: 0.600000023841858\n",
      "Epoch: 22940/25000............. Loss: 0.220346048474312 Acc: 0.600000023841858\n",
      "Epoch: 22950/25000............. Loss: 0.220345780253410 Acc: 0.600000023841858\n",
      "Epoch: 22960/25000............. Loss: 0.220345154404640 Acc: 0.600000023841858\n",
      "Epoch: 22970/25000............. Loss: 0.220344543457031 Acc: 0.600000023841858\n",
      "Epoch: 22980/25000............. Loss: 0.220344021916389 Acc: 0.600000023841858\n",
      "Epoch: 22990/25000............. Loss: 0.220343530178070 Acc: 0.600000023841858\n",
      "Epoch: 23000/25000............. Loss: 0.220343038439751 Acc: 0.600000023841858\n",
      "Epoch: 23010/25000............. Loss: 0.220342561602592 Acc: 0.600000023841858\n",
      "Epoch: 23020/25000............. Loss: 0.220342099666595 Acc: 0.600000023841858\n",
      "Epoch: 23030/25000............. Loss: 0.220341607928276 Acc: 0.600000023841858\n",
      "Epoch: 23040/25000............. Loss: 0.220341131091118 Acc: 0.600000023841858\n",
      "Epoch: 23050/25000............. Loss: 0.220340833067894 Acc: 0.600000023841858\n",
      "Epoch: 23060/25000............. Loss: 0.220351845026016 Acc: 0.600000023841858\n",
      "Epoch: 23070/25000............. Loss: 0.220787569880486 Acc: 0.600000023841858\n",
      "Epoch: 23080/25000............. Loss: 0.220847442746162 Acc: 0.600000023841858\n",
      "Epoch: 23090/25000............. Loss: 0.220416024327278 Acc: 0.600000023841858\n",
      "Epoch: 23100/25000............. Loss: 0.220375120639801 Acc: 0.600000023841858\n",
      "Epoch: 23110/25000............. Loss: 0.220348432660103 Acc: 0.600000023841858\n",
      "Epoch: 23120/25000............. Loss: 0.220341742038727 Acc: 0.600000023841858\n",
      "Epoch: 23130/25000............. Loss: 0.220339268445969 Acc: 0.600000023841858\n",
      "Epoch: 23140/25000............. Loss: 0.220336586236954 Acc: 0.600000023841858\n",
      "Epoch: 23150/25000............. Loss: 0.220336124300957 Acc: 0.600000023841858\n",
      "Epoch: 23160/25000............. Loss: 0.220335662364960 Acc: 0.600000023841858\n",
      "Epoch: 23170/25000............. Loss: 0.220335111021996 Acc: 0.600000023841858\n",
      "Epoch: 23180/25000............. Loss: 0.220334604382515 Acc: 0.600000023841858\n",
      "Epoch: 23190/25000............. Loss: 0.220334127545357 Acc: 0.600000023841858\n",
      "Epoch: 23200/25000............. Loss: 0.220333680510521 Acc: 0.600000023841858\n",
      "Epoch: 23210/25000............. Loss: 0.220333188772202 Acc: 0.600000023841858\n",
      "Epoch: 23220/25000............. Loss: 0.220332711935043 Acc: 0.600000023841858\n",
      "Epoch: 23230/25000............. Loss: 0.220332279801369 Acc: 0.600000023841858\n",
      "Epoch: 23240/25000............. Loss: 0.220332369208336 Acc: 0.600000023841858\n",
      "Epoch: 23250/25000............. Loss: 0.220357686281204 Acc: 0.600000023841858\n",
      "Epoch: 23260/25000............. Loss: 0.220630139112473 Acc: 0.600000023841858\n",
      "Epoch: 23270/25000............. Loss: 0.220695480704308 Acc: 0.600000023841858\n",
      "Epoch: 23280/25000............. Loss: 0.220383569598198 Acc: 0.600000023841858\n",
      "Epoch: 23290/25000............. Loss: 0.220354229211807 Acc: 0.600000023841858\n",
      "Epoch: 23300/25000............. Loss: 0.220343962311745 Acc: 0.600000023841858\n",
      "Epoch: 23310/25000............. Loss: 0.220329090952873 Acc: 0.600000023841858\n",
      "Epoch: 23320/25000............. Loss: 0.220328882336617 Acc: 0.600000023841858\n",
      "Epoch: 23330/25000............. Loss: 0.220328405499458 Acc: 0.600000023841858\n",
      "Epoch: 23340/25000............. Loss: 0.220327511429787 Acc: 0.600000023841858\n",
      "Epoch: 23350/25000............. Loss: 0.220326840877533 Acc: 0.600000023841858\n",
      "Epoch: 23360/25000............. Loss: 0.220326289534569 Acc: 0.600000023841858\n",
      "Epoch: 23370/25000............. Loss: 0.220325797796249 Acc: 0.600000023841858\n",
      "Epoch: 23380/25000............. Loss: 0.220325365662575 Acc: 0.600000023841858\n",
      "Epoch: 23390/25000............. Loss: 0.220324844121933 Acc: 0.600000023841858\n",
      "Epoch: 23400/25000............. Loss: 0.220324426889420 Acc: 0.600000023841858\n",
      "Epoch: 23410/25000............. Loss: 0.220324844121933 Acc: 0.600000023841858\n",
      "Epoch: 23420/25000............. Loss: 0.220371425151825 Acc: 0.600000023841858\n",
      "Epoch: 23430/25000............. Loss: 0.220439195632935 Acc: 0.600000023841858\n",
      "Epoch: 23440/25000............. Loss: 0.220491275191307 Acc: 0.600000023841858\n",
      "Epoch: 23450/25000............. Loss: 0.220398753881454 Acc: 0.600000023841858\n",
      "Epoch: 23460/25000............. Loss: 0.220393419265747 Acc: 0.600000023841858\n",
      "Epoch: 23470/25000............. Loss: 0.220322385430336 Acc: 0.600000023841858\n",
      "Epoch: 23480/25000............. Loss: 0.220328018069267 Acc: 0.600000023841858\n",
      "Epoch: 23490/25000............. Loss: 0.220323532819748 Acc: 0.600000023841858\n",
      "Epoch: 23500/25000............. Loss: 0.220320165157318 Acc: 0.600000023841858\n",
      "Epoch: 23510/25000............. Loss: 0.220319479703903 Acc: 0.600000023841858\n",
      "Epoch: 23520/25000............. Loss: 0.220318987965584 Acc: 0.600000023841858\n",
      "Epoch: 23530/25000............. Loss: 0.220318481326103 Acc: 0.600000023841858\n",
      "Epoch: 23540/25000............. Loss: 0.220318034291267 Acc: 0.600000023841858\n",
      "Epoch: 23550/25000............. Loss: 0.220317542552948 Acc: 0.600000023841858\n",
      "Epoch: 23560/25000............. Loss: 0.220317110419273 Acc: 0.600000023841858\n",
      "Epoch: 23570/25000............. Loss: 0.220316678285599 Acc: 0.600000023841858\n",
      "Epoch: 23580/25000............. Loss: 0.220317140221596 Acc: 0.600000023841858\n",
      "Epoch: 23590/25000............. Loss: 0.220366522669792 Acc: 0.600000023841858\n",
      "Epoch: 23600/25000............. Loss: 0.220418497920036 Acc: 0.600000023841858\n",
      "Epoch: 23610/25000............. Loss: 0.220319598913193 Acc: 0.600000023841858\n",
      "Epoch: 23620/25000............. Loss: 0.220490455627441 Acc: 0.600000023841858\n",
      "Epoch: 23630/25000............. Loss: 0.220367699861526 Acc: 0.600000023841858\n",
      "Epoch: 23640/25000............. Loss: 0.220385521650314 Acc: 0.600000023841858\n",
      "Epoch: 23650/25000............. Loss: 0.220316246151924 Acc: 0.600000023841858\n",
      "Epoch: 23660/25000............. Loss: 0.220322817564011 Acc: 0.600000023841858\n",
      "Epoch: 23670/25000............. Loss: 0.220312342047691 Acc: 0.600000023841858\n",
      "Epoch: 23680/25000............. Loss: 0.220312893390656 Acc: 0.600000023841858\n",
      "Epoch: 23690/25000............. Loss: 0.220311611890793 Acc: 0.600000023841858\n",
      "Epoch: 23700/25000............. Loss: 0.220310837030411 Acc: 0.600000023841858\n",
      "Epoch: 23710/25000............. Loss: 0.220310315489769 Acc: 0.600000023841858\n",
      "Epoch: 23720/25000............. Loss: 0.220309883356094 Acc: 0.600000023841858\n",
      "Epoch: 23730/25000............. Loss: 0.220309421420097 Acc: 0.600000023841858\n",
      "Epoch: 23740/25000............. Loss: 0.220309048891068 Acc: 0.600000023841858\n",
      "Epoch: 23750/25000............. Loss: 0.220310375094414 Acc: 0.600000023841858\n",
      "Epoch: 23760/25000............. Loss: 0.220411136746407 Acc: 0.600000023841858\n",
      "Epoch: 23770/25000............. Loss: 0.220307618379593 Acc: 0.600000023841858\n",
      "Epoch: 23780/25000............. Loss: 0.220356360077858 Acc: 0.600000023841858\n",
      "Epoch: 23790/25000............. Loss: 0.220325231552124 Acc: 0.600000023841858\n",
      "Epoch: 23800/25000............. Loss: 0.220306813716888 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23810/25000............. Loss: 0.220343157649040 Acc: 0.600000023841858\n",
      "Epoch: 23820/25000............. Loss: 0.220504984259605 Acc: 0.600000023841858\n",
      "Epoch: 23830/25000............. Loss: 0.220309630036354 Acc: 0.600000023841858\n",
      "Epoch: 23840/25000............. Loss: 0.220342412590981 Acc: 0.600000023841858\n",
      "Epoch: 23850/25000............. Loss: 0.220304206013680 Acc: 0.600000023841858\n",
      "Epoch: 23860/25000............. Loss: 0.220307961106300 Acc: 0.600000023841858\n",
      "Epoch: 23870/25000............. Loss: 0.220304220914841 Acc: 0.600000023841858\n",
      "Epoch: 23880/25000............. Loss: 0.220302715897560 Acc: 0.600000023841858\n",
      "Epoch: 23890/25000............. Loss: 0.220302358269691 Acc: 0.600000023841858\n",
      "Epoch: 23900/25000............. Loss: 0.220301911234856 Acc: 0.600000023841858\n",
      "Epoch: 23910/25000............. Loss: 0.220301464200020 Acc: 0.600000023841858\n",
      "Epoch: 23920/25000............. Loss: 0.220302134752274 Acc: 0.600000023841858\n",
      "Epoch: 23930/25000............. Loss: 0.220373004674911 Acc: 0.600000023841858\n",
      "Epoch: 23940/25000............. Loss: 0.220321819186211 Acc: 0.600000023841858\n",
      "Epoch: 23950/25000............. Loss: 0.220337077975273 Acc: 0.600000023841858\n",
      "Epoch: 23960/25000............. Loss: 0.220323428511620 Acc: 0.600000023841858\n",
      "Epoch: 23970/25000............. Loss: 0.220298811793327 Acc: 0.600000023841858\n",
      "Epoch: 23980/25000............. Loss: 0.220301657915115 Acc: 0.600000023841858\n",
      "Epoch: 23990/25000............. Loss: 0.220305025577545 Acc: 0.600000023841858\n",
      "Epoch: 24000/25000............. Loss: 0.220584362745285 Acc: 0.600000023841858\n",
      "Epoch: 24010/25000............. Loss: 0.220428079366684 Acc: 0.600000023841858\n",
      "Epoch: 24020/25000............. Loss: 0.220325380563736 Acc: 0.600000023841858\n",
      "Epoch: 24030/25000............. Loss: 0.220298424363136 Acc: 0.600000023841858\n",
      "Epoch: 24040/25000............. Loss: 0.220302522182465 Acc: 0.600000023841858\n",
      "Epoch: 24050/25000............. Loss: 0.220296770334244 Acc: 0.600000023841858\n",
      "Epoch: 24060/25000............. Loss: 0.220294818282127 Acc: 0.600000023841858\n",
      "Epoch: 24070/25000............. Loss: 0.220294326543808 Acc: 0.600000023841858\n",
      "Epoch: 24080/25000............. Loss: 0.220293894410133 Acc: 0.600000023841858\n",
      "Epoch: 24090/25000............. Loss: 0.220293462276459 Acc: 0.600000023841858\n",
      "Epoch: 24100/25000............. Loss: 0.220293492078781 Acc: 0.600000023841858\n",
      "Epoch: 24110/25000............. Loss: 0.220334395766258 Acc: 0.600000023841858\n",
      "Epoch: 24120/25000............. Loss: 0.220422685146332 Acc: 0.600000023841858\n",
      "Epoch: 24130/25000............. Loss: 0.220330566167831 Acc: 0.600000023841858\n",
      "Epoch: 24140/25000............. Loss: 0.220313295722008 Acc: 0.600000023841858\n",
      "Epoch: 24150/25000............. Loss: 0.220298096537590 Acc: 0.600000023841858\n",
      "Epoch: 24160/25000............. Loss: 0.220293968915939 Acc: 0.600000023841858\n",
      "Epoch: 24170/25000............. Loss: 0.220344930887222 Acc: 0.600000023841858\n",
      "Epoch: 24180/25000............. Loss: 0.220356360077858 Acc: 0.600000023841858\n",
      "Epoch: 24190/25000............. Loss: 0.220327988266945 Acc: 0.600000023841858\n",
      "Epoch: 24200/25000............. Loss: 0.220312267541885 Acc: 0.600000023841858\n",
      "Epoch: 24210/25000............. Loss: 0.220290765166283 Acc: 0.600000023841858\n",
      "Epoch: 24220/25000............. Loss: 0.220292195677757 Acc: 0.600000023841858\n",
      "Epoch: 24230/25000............. Loss: 0.220287531614304 Acc: 0.600000023841858\n",
      "Epoch: 24240/25000............. Loss: 0.220287099480629 Acc: 0.600000023841858\n",
      "Epoch: 24250/25000............. Loss: 0.220286712050438 Acc: 0.600000023841858\n",
      "Epoch: 24260/25000............. Loss: 0.220286175608635 Acc: 0.600000023841858\n",
      "Epoch: 24270/25000............. Loss: 0.220285698771477 Acc: 0.600000023841858\n",
      "Epoch: 24280/25000............. Loss: 0.220285251736641 Acc: 0.600000023841858\n",
      "Epoch: 24290/25000............. Loss: 0.220284968614578 Acc: 0.600000023841858\n",
      "Epoch: 24300/25000............. Loss: 0.220290675759315 Acc: 0.600000023841858\n",
      "Epoch: 24310/25000............. Loss: 0.220571354031563 Acc: 0.600000023841858\n",
      "Epoch: 24320/25000............. Loss: 0.220410898327827 Acc: 0.600000023841858\n",
      "Epoch: 24330/25000............. Loss: 0.220312893390656 Acc: 0.600000023841858\n",
      "Epoch: 24340/25000............. Loss: 0.220283851027489 Acc: 0.600000023841858\n",
      "Epoch: 24350/25000............. Loss: 0.220290973782539 Acc: 0.600000023841858\n",
      "Epoch: 24360/25000............. Loss: 0.220453768968582 Acc: 0.600000023841858\n",
      "Epoch: 24370/25000............. Loss: 0.220334365963936 Acc: 0.600000023841858\n",
      "Epoch: 24380/25000............. Loss: 0.220347657799721 Acc: 0.600000023841858\n",
      "Epoch: 24390/25000............. Loss: 0.220280706882477 Acc: 0.600000023841858\n",
      "Epoch: 24400/25000............. Loss: 0.220287233591080 Acc: 0.600000023841858\n",
      "Epoch: 24410/25000............. Loss: 0.220281869173050 Acc: 0.600000023841858\n",
      "Epoch: 24420/25000............. Loss: 0.220279216766357 Acc: 0.600000023841858\n",
      "Epoch: 24430/25000............. Loss: 0.220278829336166 Acc: 0.600000023841858\n",
      "Epoch: 24440/25000............. Loss: 0.220278397202492 Acc: 0.600000023841858\n",
      "Epoch: 24450/25000............. Loss: 0.220277920365334 Acc: 0.600000023841858\n",
      "Epoch: 24460/25000............. Loss: 0.220277532935143 Acc: 0.600000023841858\n",
      "Epoch: 24470/25000............. Loss: 0.220278933644295 Acc: 0.600000023841858\n",
      "Epoch: 24480/25000............. Loss: 0.220426484942436 Acc: 0.600000023841858\n",
      "Epoch: 24490/25000............. Loss: 0.220320373773575 Acc: 0.600000023841858\n",
      "Epoch: 24500/25000............. Loss: 0.220349326729774 Acc: 0.600000023841858\n",
      "Epoch: 24510/25000............. Loss: 0.220276117324829 Acc: 0.600000023841858\n",
      "Epoch: 24520/25000............. Loss: 0.220284789800644 Acc: 0.600000023841858\n",
      "Epoch: 24530/25000............. Loss: 0.220275670289993 Acc: 0.600000023841858\n",
      "Epoch: 24540/25000............. Loss: 0.220355480909348 Acc: 0.600000023841858\n",
      "Epoch: 24550/25000............. Loss: 0.220278099179268 Acc: 0.600000023841858\n",
      "Epoch: 24560/25000............. Loss: 0.220352590084076 Acc: 0.600000023841858\n",
      "Epoch: 24570/25000............. Loss: 0.220277369022369 Acc: 0.600000023841858\n",
      "Epoch: 24580/25000............. Loss: 0.220282077789307 Acc: 0.600000023841858\n",
      "Epoch: 24590/25000............. Loss: 0.220273464918137 Acc: 0.600000023841858\n",
      "Epoch: 24600/25000............. Loss: 0.220271930098534 Acc: 0.600000023841858\n",
      "Epoch: 24610/25000............. Loss: 0.220271676778793 Acc: 0.600000023841858\n",
      "Epoch: 24620/25000............. Loss: 0.220270827412605 Acc: 0.600000023841858\n",
      "Epoch: 24630/25000............. Loss: 0.220270305871964 Acc: 0.600000023841858\n",
      "Epoch: 24640/25000............. Loss: 0.220269858837128 Acc: 0.600000023841858\n",
      "Epoch: 24650/25000............. Loss: 0.220269486308098 Acc: 0.600000023841858\n",
      "Epoch: 24660/25000............. Loss: 0.220271617174149 Acc: 0.600000023841858\n",
      "Epoch: 24670/25000............. Loss: 0.220450028777122 Acc: 0.600000023841858\n",
      "Epoch: 24680/25000............. Loss: 0.220337390899658 Acc: 0.600000023841858\n",
      "Epoch: 24690/25000............. Loss: 0.220330789685249 Acc: 0.600000023841858\n",
      "Epoch: 24700/25000............. Loss: 0.220267638564110 Acc: 0.600000023841858\n",
      "Epoch: 24710/25000............. Loss: 0.220275595784187 Acc: 0.600000023841858\n",
      "Epoch: 24720/25000............. Loss: 0.220267862081528 Acc: 0.600000023841858\n",
      "Epoch: 24730/25000............. Loss: 0.220286637544632 Acc: 0.600000023841858\n",
      "Epoch: 24740/25000............. Loss: 0.220631822943687 Acc: 0.600000023841858\n",
      "Epoch: 24750/25000............. Loss: 0.220281094312668 Acc: 0.600000023841858\n",
      "Epoch: 24760/25000............. Loss: 0.220296442508698 Acc: 0.600000023841858\n",
      "Epoch: 24770/25000............. Loss: 0.220270559191704 Acc: 0.600000023841858\n",
      "Epoch: 24780/25000............. Loss: 0.220265731215477 Acc: 0.600000023841858\n",
      "Epoch: 24790/25000............. Loss: 0.220265597105026 Acc: 0.600000023841858\n",
      "Epoch: 24800/25000............. Loss: 0.220263391733170 Acc: 0.600000023841858\n",
      "Epoch: 24810/25000............. Loss: 0.220262721180916 Acc: 0.600000023841858\n",
      "Epoch: 24820/25000............. Loss: 0.220262303948402 Acc: 0.600000023841858\n",
      "Epoch: 24830/25000............. Loss: 0.220261961221695 Acc: 0.600000023841858\n",
      "Epoch: 24840/25000............. Loss: 0.220263168215752 Acc: 0.600000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24850/25000............. Loss: 0.220366865396500 Acc: 0.600000023841858\n",
      "Epoch: 24860/25000............. Loss: 0.220261007547379 Acc: 0.600000023841858\n",
      "Epoch: 24870/25000............. Loss: 0.220319420099258 Acc: 0.600000023841858\n",
      "Epoch: 24880/25000............. Loss: 0.220273464918137 Acc: 0.600000023841858\n",
      "Epoch: 24890/25000............. Loss: 0.220260620117188 Acc: 0.600000023841858\n",
      "Epoch: 24900/25000............. Loss: 0.220262423157692 Acc: 0.600000023841858\n",
      "Epoch: 24910/25000............. Loss: 0.220259532332420 Acc: 0.600000023841858\n",
      "Epoch: 24920/25000............. Loss: 0.220258250832558 Acc: 0.600000023841858\n",
      "Epoch: 24930/25000............. Loss: 0.220263570547104 Acc: 0.600000023841858\n",
      "Epoch: 24940/25000............. Loss: 0.220920100808144 Acc: 0.600000023841858\n",
      "Epoch: 24950/25000............. Loss: 0.220266401767731 Acc: 0.600000023841858\n",
      "Epoch: 24960/25000............. Loss: 0.220303729176521 Acc: 0.600000023841858\n",
      "Epoch: 24970/25000............. Loss: 0.220283314585686 Acc: 0.600000023841858\n",
      "Epoch: 24980/25000............. Loss: 0.220263332128525 Acc: 0.600000023841858\n",
      "Epoch: 24990/25000............. Loss: 0.220255598425865 Acc: 0.600000023841858\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "model.train()\n",
    "\n",
    "epoch_start = 5000\n",
    "\n",
    "for epoch in range(epoch_start, epoch_start + n_epochs):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq.to(device)\n",
    "    output, hidden = model(X_train)\n",
    "    # print(output)\n",
    "    loss = criterion(output, Y_train.view(-1,2).float())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    loss_arr.append(loss.item())\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        acc = float((torch.sum((torch.argmax(output, axis=1) == torch.argmax(Y_train.squeeze(), axis=1)).int())/120))\n",
    "        print('Epoch: {}/{}.............'.format(epoch, epoch_start + n_epochs), end=' ')\n",
    "        print(\"Loss: {:.15f} Acc: {:.15f}\".format(loss.item(), acc))\n",
    "\n",
    "epoch_start = epoch_start + n_epochs + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acting-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode\n",
    "out, hidden = model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amateur-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0->tensor([9.9993e-01, 7.2820e-05], grad_fn=<SelectBackward>)\n",
      "1->tensor([1.0000e+00, 7.7964e-07], grad_fn=<SelectBackward>)\n",
      "2->tensor([1.0000e+00, 6.1047e-08], grad_fn=<SelectBackward>)\n",
      "3->tensor([9.9990e-01, 9.9200e-05], grad_fn=<SelectBackward>)\n",
      "4->tensor([0.8637, 0.1348], grad_fn=<SelectBackward>)\n",
      "5->tensor([0.5360, 0.4628], grad_fn=<SelectBackward>)\n",
      "6->tensor([0.4740, 0.5251], grad_fn=<SelectBackward>)\n",
      "7->tensor([0.4655, 0.5336], grad_fn=<SelectBackward>)\n",
      "8->tensor([0.4644, 0.5347], grad_fn=<SelectBackward>)\n",
      "9->tensor([0.4643, 0.5349], grad_fn=<SelectBackward>)\n",
      "10->tensor([0.6674, 0.3318], grad_fn=<SelectBackward>)\n",
      "11->tensor([0.6674, 0.3318], grad_fn=<SelectBackward>)\n",
      "12->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "13->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "14->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "15->tensor([0.9542, 0.0453], grad_fn=<SelectBackward>)\n",
      "16->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "17->tensor([0.9503, 0.0493], grad_fn=<SelectBackward>)\n",
      "18->tensor([0.9115, 0.0877], grad_fn=<SelectBackward>)\n",
      "19->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "20->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "21->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "22->tensor([0.9503, 0.0493], grad_fn=<SelectBackward>)\n",
      "23->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "24->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "25->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "26->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "27->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "28->tensor([0.9542, 0.0454], grad_fn=<SelectBackward>)\n",
      "29->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "30->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "31->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "32->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "33->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "34->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "35->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "36->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "37->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "38->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "39->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "40->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "41->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "42->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "43->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "44->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "45->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "46->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "47->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "48->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "49->tensor([0.6674, 0.3318], grad_fn=<SelectBackward>)\n",
      "50->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "51->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "52->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "53->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "54->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "55->tensor([0.0306, 0.9695], grad_fn=<SelectBackward>)\n",
      "56->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "57->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "58->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "59->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "60->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "61->tensor([0.3827, 0.6169], grad_fn=<SelectBackward>)\n",
      "62->tensor([0.4522, 0.5470], grad_fn=<SelectBackward>)\n",
      "63->tensor([0.3827, 0.6169], grad_fn=<SelectBackward>)\n",
      "64->tensor([0.4522, 0.5470], grad_fn=<SelectBackward>)\n",
      "65->tensor([0.4626, 0.5366], grad_fn=<SelectBackward>)\n",
      "66->tensor([0.4640, 0.5351], grad_fn=<SelectBackward>)\n",
      "67->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "68->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "69->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "70->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "71->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "72->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "73->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "74->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "75->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "76->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "77->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "78->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "79->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "80->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "81->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "82->tensor([0.1036, 0.8967], grad_fn=<SelectBackward>)\n",
      "83->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "84->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "85->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "86->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "87->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "88->tensor([0.0834, 0.9168], grad_fn=<SelectBackward>)\n",
      "89->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "90->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "91->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "92->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "93->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "94->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "95->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "96->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "97->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "98->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "99->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "100->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "101->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "102->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "103->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "104->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "105->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "106->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "107->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "108->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "109->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "110->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "111->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "112->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "113->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "114->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n",
      "115->tensor([0.3827, 0.6169], grad_fn=<SelectBackward>)\n",
      "116->tensor([0.4522, 0.5470], grad_fn=<SelectBackward>)\n",
      "117->tensor([0.4626, 0.5366], grad_fn=<SelectBackward>)\n",
      "118->tensor([0.4640, 0.5351], grad_fn=<SelectBackward>)\n",
      "119->tensor([0.4642, 0.5349], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(120):\n",
    "    print(str(i) + \"->\" + str(out[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-trademark",
   "metadata": {},
   "source": [
    "## Graph before loss array being erased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "smaller-colombia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for loss\n",
    "plt.plot(loss_arr)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "wrapped-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"/home/ryo/Desktop/tcc/workspace/2-train_lstm/pytorch_lstm_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-longer",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "pleasant-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "analyzed-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lstm): LSTM(20400, 200, batch_first=True)\n",
       "  (sigmoid1): Sigmoid()\n",
       "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size=20400, output_size=2, hidden_dim=200, n_layers=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "ckp_path = \"/home/ryo/Desktop/tcc/workspace/2-train_lstm/checkpoint.pt\"\n",
    "model, optimizer, start_epoch = load_ckp(ckp_path, model, optimizer)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "conventional-demographic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = Model(input_size=20400, output_size=2, hidden_dim=200, n_layers=1)\\nmodel.load_state_dict(torch.load(r\"/home/ryo/Desktop/tcc/workspace/2-train_lstm/pytorch_lstm_model\"))\\nmodel.eval()'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = Model(input_size=20400, output_size=2, hidden_dim=200, n_layers=1)\n",
    "model.load_state_dict(torch.load(r\"/home/ryo/Desktop/tcc/workspace/2-train_lstm/pytorch_lstm_model\"))\n",
    "model.eval()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "basic-armstrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 20400])\n",
      "torch.Size([800, 200])\n",
      "torch.Size([800])\n",
      "torch.Size([800])\n",
      "torch.Size([2, 200])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm.weight_ih_l0.shape)\n",
    "print(model.lstm.weight_hh_l0.shape)\n",
    "print(model.lstm.bias_ih_l0.shape)\n",
    "print(model.lstm.bias_hh_l0.shape)\n",
    "print(model.fc.weight.shape)\n",
    "print(model.fc.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-burns",
   "metadata": {},
   "source": [
    "## Output model predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "occupied-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmcell = nn.LSTMCell(20400, 200)\n",
    "sig1 = nn.Sigmoid()\n",
    "linear = nn.Linear(200, 2)\n",
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "opposed-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmcell.weight_ih = model.lstm.weight_ih_l0\n",
    "lstmcell.weight_hh = model.lstm.weight_hh_l0\n",
    "lstmcell.bias_hh = model.lstm.bias_hh_l0\n",
    "lstmcell.bias_ih = model.lstm.bias_ih_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "naughty-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight = model.fc.weight\n",
    "linear.bias = model.fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "great-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = torch.zeros(1, 200)\n",
    "cx = torch.zeros(1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "former-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-172-1e7e3824cb14>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = softmax(out)\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in range(120):\n",
    "    step_input = X_train[0][i]\n",
    "    step_input = step_input.reshape(1, -1)\n",
    "    hx, cx = lstmcell(step_input, (hx, cx))\n",
    "    out = sig1(hx)\n",
    "    out = linear(out)\n",
    "    out = softmax(out)\n",
    "    output.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ruled-check",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.9290, 0.0710]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.5556, 0.4444]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4703, 0.5297]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4587, 0.5413]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4571, 0.5429]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4959, 0.5041]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4959, 0.5041]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2942, 0.7058]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1910, 0.8090]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.8596, 0.1404]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.8326, 0.1674]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4581, 0.5419]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4570, 0.5430]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.7588, 0.2412]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4569, 0.5431]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.8360, 0.1640]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9478, 0.0522]], grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
