{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d651e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [[  0,  62,  80, 106, 117, 137, 142, 156, 159, 166, 185, 215, 226, 235, 242, 252, 259, 279, 281, 292, 296, 299, \n",
    "            303, 308, 315, 318, 321, 329, 333, 335, 354, 360, 374, 377, 382, 386, 390, 393, 402, 414, 420, 444, 447, 465, \n",
    "            469, 487, 491, 504, 506, 510, 513, 525, 528, 538, 552, 558, 570, 576, 581, 584, 588, 595, 598, 604, 607, 610, \n",
    "            618, 619, 622, 631, 641, 645, 662, 672, 685, 693, 697, 704, 718, 724, 726, 735, 739, 745, 754, 755, 770, 773, \n",
    "            789, 790, 799, 805, 809, 823, 834, 850, 854, 857, 868, 874, 875, 877, 883, 885, 904, 906, 911, 919, 923, 926, \n",
    "            932, 943, 962, 965, 984, 991, 999, 1007, 1010, 1015, 1017, 1018],\n",
    "           [  0,  31,  45,  47,  56,  63,  69,  85,  88,  92,  96, 112, 118, 124, 130, 137, 143, 151, 158, 170, 184, 202, \n",
    "            223, 243, 249, 255, 260, 269, 272, 275, 276, 282, 285, 295, 296, 297, 306, 308, 318, 322, 331, 334, 337, 338, \n",
    "            341, 343, 354, 357, 381, 387, 400, 402, 408, 420, 423, 426, 429, 441, 443, 445, 448, 454, 463, 465, 474, 481,\n",
    "            502, 506, 524, 533, 538, 548, 556, 562, 566, 570, 581, 588, 603, 604, 609, 652, 668, 681, 698, 710, 714, 725, \n",
    "            729, 736, 739, 743, 748, 749, 752, 766, 770, 774, 778, 781, 792, 795, 803, 804, 806, 814, 818, 821, 828, 834, \n",
    "            837, 843, 849, 855, 862, 868, 874, 884, 888, 895, 896, 919, 935, 942, 945, 957, 958, 963, 964, 965, 977, 980,\n",
    "            984, 989, 997, 1000], \n",
    "           [  0,  37,  53,  81,  94,  99, 119, 123, 124, 142, 144, 146, 162, 193, 211, 213, 222, 230, 232, 235, 238, 240, \n",
    "            252, 255, 258, 267, 270, 275, 278, 284, 287, 289, 300, 307, 313, 331, 352, 359, 371, 379, 386, 389, 392, 409, \n",
    "            412, 432, 435, 440, 442, 446, 462, 467, 472, 506, 527, 531, 535, 553, 557, 561, 571, 579, 589, 602, 607, 609, \n",
    "            612, 616, 621, 631, 634, 638, 640, 644, 645, 661, 669, 673, 676, 692, 695, 698, 701, 711, 717, 720, 721, 734, \n",
    "            744, 747, 750, 755, 759, 774, 785, 793, 807, 859, 891, 895, 912, 927, 948, 958, 969, 980, 997, 998, 1004], \n",
    "           [  0,  33,  58,  75,  86, 100, 118, 120, 139, 142, 147, 159, 162, 165, 167, 174, 179, 180, 183, 192, 197, 199, \n",
    "            209, 214, 217, 222, 227, 231, 232, 239, 244, 251, 262, 266, 271, 275, 277, 282, 287, 291, 294, 297, 308, 311, \n",
    "            316, 318, 322, 327, 339, 342, 348, 352, 362, 372, 383, 397, 399, 402, 405, 417, 420, 425, 428, 454, 457, 464, \n",
    "            470, 477, 481, 485, 488, 494, 495, 503, 507, 517, 535, 537, 540, 553, 556, 558, 577, 588, 598, 601, 609, 620, \n",
    "            634, 638, 647, 650, 656, 659, 660, 663, 669, 672, 681, 684, 696, 702, 705, 708, 716, 719, 726, 730, 739, 743, \n",
    "            746, 758, 765, 766, 777, 783, 785, 786, 790, 792, 800, 802, 808, 809, 810, 816, 819, 823, 827, 835, 837, 842, \n",
    "            845, 853, 865, 868, 873, 885, 889, 891, 897, 904, 906, 918, 941, 949, 956, 961, 969, 973, 976, 981, 984, 991, \n",
    "            996, 999, 1002], \n",
    "           [  0,  59,  68,  73,  76,  86,  94,  97, 101, 104, 109, 118, 120, 123, 128, 134, 136, 139, 143, 145, 148, 154, \n",
    "            160, 162, 177, 190, 195, 199, 204, 208, 212, 220, 221, 222, 232, 234, 246, 251, 253, 256, 263, 270, 273, 287, \n",
    "            302, 314, 322, 355, 357, 368, 396, 397, 401, 416, 428, 433, 437, 442, 446, 452, 456, 469, 475, 490, 500, 506, \n",
    "            525, 526, 538, 545, 551, 554, 558, 562, 571, 579, 582, 586, 587, 600, 612, 664, 823, 824, 832, 850, 858, 870, \n",
    "            879, 881, 889, 898, 901, 913, 916, 924, 932, 936, 943, 952, 955, 975, 985, 992, 997, 1001], \n",
    "           [  0,  36,  56,  94, 114, 121, 136, 140, 143, 153, 159, 162, 171, 174, 182, 190, 197, 201, 215, 219, 237, 239, \n",
    "            249, 251, 254, 265, 269, 297, 322, 335, 340, 343, 348, 352, 355, 363, 368, 371, 375, 384, 388, 395, 405, 411, \n",
    "            439, 461, 464, 468, 477, 483, 485, 487, 494, 497, 501, 503, 514, 519, 521, 526, 529, 532, 538, 547, 551, 555, \n",
    "            560, 579, 584, 597, 600, 602, 607, 609, 617, 623, 626, 634, 638, 644, 649, 652, 663, 671, 672, 675, 678, 685, \n",
    "            688, 699, 700, 712, 719, 724, 726, 735, 744, 753, 756, 775, 797, 806, 812, 818, 826, 829, 832, 848, 852, 856, \n",
    "            866, 869, 872, 875, 881, 886, 891, 893, 895, 901, 910, 930, 936, 940, 953, 958, 989, 997, 1004, 1006],\n",
    "           [ 13,  34,  56,  77,  83,  98, 103, 109, 114, 119, 121, 126, 129, 131, 136, 137, 138, 144, 147, 154, 157, 159,\n",
    "            172, 174, 178, 191, 193, 195, 208, 210, 221, 224, 226, 232, 240, 243, 251, 257, 266, 285, 299, 303, 307, 319, \n",
    "            324, 328, 330, 334, 342, 345, 348, 352, 354, 361, 364, 369, 371, 374, 379, 382, 387, 393, 397, 400, 406, 411, \n",
    "            418, 423, 428, 433, 436, 440, 445, 450, 459, 465, 468, 470, 489, 495, 497, 507, 510, 546, 557, 584, 589, 592, \n",
    "            608, 620, 624, 634, 649, 659, 661, 668, 670, 683, 690, 693, 696, 705, 723, 731, 748, 754, 770, 776, 779, 784, \n",
    "            786, 794, 797, 802, 807, 808, 819, 823, 825, 827, 834, 843, 856, 863, 865, 872, 880, 882, 883, 888, 891, 894, \n",
    "            899, 902, 904, 906, 912, 920, 923, 931, 939, 948, 956, 970, 973, 981, 989, 992, 997, 999, 1008, 1010, 1013, 1014],\n",
    "           [ 37,  57,  73,  83, 100, 119, 130, 141, 147, 155, 159, 161, 172, 174, 186, 188, 193, 195, 201, 203, 207, 209, \n",
    "            218, 220, 223, 225, 232, 236, 239, 241, 243, 244, 247, 259, 261, 263, 279, 294, 300, 302, 305, 307, 317, 319, \n",
    "            321, 324, 327, 331, 336, 337, 338, 347, 350, 351, 359, 363, 365, 374, 379, 386, 411, 422, 437, 446, 466, 469, \n",
    "            473, 492, 498, 510, 513, 524, 531, 534, 538, 550, 557, 572, 575, 594, 601, 608, 616, 619, 626, 634, 638, 641, \n",
    "            642, 650, 653, 661, 665, 668, 676, 687, 714, 725, 736, 746, 747, 760, 770, 784, 794, 807, 817, 831, 842, 846, \n",
    "            854, 864, 867, 871, 874, 878, 881, 894, 898, 901, 904, 914, 925, 928, 934, 937, 938, 947, 951, 960, 967, 970, \n",
    "            974, 979, 987, 990, 993, 996, 999, 1005, 1013, 1016, 1019, 1025],\n",
    "           [ 13,  36,  58,  77,  85, 103, 108, 114, 119, 123, 125, 130, 131, 137, 142, 150, 156, 160, 166, 173, 176, 179, \n",
    "            182, 183, 188, 192, 194, 199, 202, 204, 206, 218, 221, 222, 226, 228, 231, 243, 245, 246, 248, 252, 253, 262, \n",
    "            269, 275, 279, 281, 284, 291, 298, 300, 302, 306, 313, 315, 318, 328, 334, 336, 357, 360, 371, 376, 377, 379, \n",
    "            388, 391, 398, 402, 410, 420, 423, 431, 440, 441, 444, 450, 453, 460, 462, 471, 474, 482, 494, 507, 518, 521, \n",
    "            528, 535, 540, 542, 552, 555, 567, 570, 573, 575, 585, 588, 591, 595, 604, 607, 613, 618, 623, 628, 637, 638, \n",
    "            647, 655, 660, 669, 678, 687, 700, 717, 721, 729, 744, 747, 748, 760, 772, 775, 788, 789, 800, 818, 819, 824, \n",
    "            848, 851, 870, 878, 887, 896, 897, 899, 900, 903, 907, 925, 927, 932, 937, 940, 955, 962, 965, 968, 979, 990, \n",
    "            992, 1000, 1010, 1018],\n",
    "           [ 14,  36,  58,  77,  85, 103, 108, 114, 119, 123, 125, 130, 131, 137, 142, 150, 156, 160, 166, 173, 176, 179, \n",
    "            182, 183, 188, 192, 194, 199, 202, 204, 206, 218, 221, 222, 226, 228, 231, 243, 245, 246, 248, 252, 253, 262, \n",
    "            269, 275, 279, 281, 284, 291, 298, 300, 302, 306, 313, 315, 318, 328, 334, 336, 357, 360, 371, 376, 377, 379, \n",
    "            388, 391, 398, 402, 410, 420, 423, 431, 440, 441, 444, 450, 453, 460, 462, 471, 474, 482, 494, 507, 518, 521, \n",
    "            528, 535, 540, 542, 552, 554, 566, 570, 573, 575, 585, 588, 591, 595, 604, 607, 613, 618, 623, 628, 637, 638, \n",
    "            647, 655, 660, 669, 678, 687, 700, 717, 721, 729, 744, 747, 748, 760, 772, 775, 788, 789, 800, 818, 819, 824, \n",
    "            848, 851, 870, 878, 897, 899, 900, 903, 907, 925, 927, 932, 936, 940, 955, 962, 965, 968, 979, 990, 992, 1000, \n",
    "            1010, 1018]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {\n",
    "           \"match_1\" : [  0,  62,  80, 106, 117, 137, 142, 156, 159, 166, 185, 215, 226, 235, 242, 252, 259, 279, 281, 292, 296, 299, \n",
    "                        303, 308, 315, 318, 321, 329, 333, 335, 354, 360, 374, 377, 382, 386, 390, 393, 402, 414, 420, 444, 447, 465, \n",
    "                        469, 487, 491, 504, 506, 510, 513, 525, 528, 538, 552, 558, 570, 576, 581, 584, 588, 595, 598, 604, 607, 610, \n",
    "                        618, 619, 622, 631, 641, 645, 662, 672, 685, 693, 697, 704, 718, 724, 726, 735, 739, 745, 754, 755, 770, 773, \n",
    "                        789, 790, 799, 805, 809, 823, 834, 850, 854, 857, 868, 874, 875, 877, 883, 885, 904, 906, 911, 919, 923, 926, \n",
    "                        932, 943, 962, 965, 984, 991, 999, 1007, 1010, 1015, 1017, 1018],\n",
    "           \"match_2\" : [  0,  31,  45,  47,  56,  63,  69,  85,  88,  92,  96, 112, 118, 124, 130, 137, 143, 151, 158, 170, 184, 202, \n",
    "                        223, 243, 249, 255, 260, 269, 272, 275, 276, 282, 285, 295, 296, 297, 306, 308, 318, 322, 331, 334, 337, 338, \n",
    "                        341, 343, 354, 357, 381, 387, 400, 402, 408, 420, 423, 426, 429, 441, 443, 445, 448, 454, 463, 465, 474, 481,\n",
    "                        502, 506, 524, 533, 538, 548, 556, 562, 566, 570, 581, 588, 603, 604, 609, 652, 668, 681, 698, 710, 714, 725, \n",
    "                        729, 736, 739, 743, 748, 749, 752, 766, 770, 774, 778, 781, 792, 795, 803, 804, 806, 814, 818, 821, 828, 834, \n",
    "                        837, 843, 849, 855, 862, 868, 874, 884, 888, 895, 896, 919, 935, 942, 945, 957, 958, 963, 964, 965, 977, 980,\n",
    "                        984, 989, 997, 1000], \n",
    "           \"match_3\" : [  0,  37,  53,  81,  94,  99, 119, 123, 124, 142, 144, 146, 162, 193, 211, 213, 222, 230, 232, 235, 238, 240, \n",
    "                        252, 255, 258, 267, 270, 275, 278, 284, 287, 289, 300, 307, 313, 331, 352, 359, 371, 379, 386, 389, 392, 409, \n",
    "                        412, 432, 435, 440, 442, 446, 462, 467, 472, 506, 527, 531, 535, 553, 557, 561, 571, 579, 589, 602, 607, 609, \n",
    "                        612, 616, 621, 631, 634, 638, 640, 644, 645, 661, 669, 673, 676, 692, 695, 698, 701, 711, 717, 720, 721, 734, \n",
    "                        744, 747, 750, 755, 759, 774, 785, 793, 807, 859, 891, 895, 912, 927, 948, 958, 969, 980, 997, 998, 1004], \n",
    "           \"match_4\" : [  0,  33,  58,  75,  86, 100, 118, 120, 139, 142, 147, 159, 162, 165, 167, 174, 179, 180, 183, 192, 197, 199, \n",
    "                        209, 214, 217, 222, 227, 231, 232, 239, 244, 251, 262, 266, 271, 275, 277, 282, 287, 291, 294, 297, 308, 311, \n",
    "                        316, 318, 322, 327, 339, 342, 348, 352, 362, 372, 383, 397, 399, 402, 405, 417, 420, 425, 428, 454, 457, 464, \n",
    "                        470, 477, 481, 485, 488, 494, 495, 503, 507, 517, 535, 537, 540, 553, 556, 558, 577, 588, 598, 601, 609, 620, \n",
    "                        634, 638, 647, 650, 656, 659, 660, 663, 669, 672, 681, 684, 696, 702, 705, 708, 716, 719, 726, 730, 739, 743, \n",
    "                        746, 758, 765, 766, 777, 783, 785, 786, 790, 792, 800, 802, 808, 809, 810, 816, 819, 823, 827, 835, 837, 842, \n",
    "                        845, 853, 865, 868, 873, 885, 889, 891, 897, 904, 906, 918, 941, 949, 956, 961, 969, 973, 976, 981, 984, 991, \n",
    "                        996, 999, 1002], \n",
    "           \"match_5\" : [  0,  59,  68,  73,  76,  86,  94,  97, 101, 104, 109, 118, 120, 123, 128, 134, 136, 139, 143, 145, 148, 154, \n",
    "                        160, 162, 177, 190, 195, 199, 204, 208, 212, 220, 221, 222, 232, 234, 246, 251, 253, 256, 263, 270, 273, 287, \n",
    "                        302, 314, 322, 355, 357, 368, 396, 397, 401, 416, 428, 433, 437, 442, 446, 452, 456, 469, 475, 490, 500, 506, \n",
    "                        525, 526, 538, 545, 551, 554, 558, 562, 571, 579, 582, 586, 587, 600, 612, 664, 823, 824, 832, 850, 858, 870, \n",
    "                        879, 881, 889, 898, 901, 913, 916, 924, 932, 936, 943, 952, 955, 975, 985, 992, 997, 1001], \n",
    "           \"match_6\" : [  0,  36,  56,  94, 114, 121, 136, 140, 143, 153, 159, 162, 171, 174, 182, 190, 197, 201, 215, 219, 237, 239, \n",
    "                        249, 251, 254, 265, 269, 297, 322, 335, 340, 343, 348, 352, 355, 363, 368, 371, 375, 384, 388, 395, 405, 411, \n",
    "                        439, 461, 464, 468, 477, 483, 485, 487, 494, 497, 501, 503, 514, 519, 521, 526, 529, 532, 538, 547, 551, 555, \n",
    "                        560, 579, 584, 597, 600, 602, 607, 609, 617, 623, 626, 634, 638, 644, 649, 652, 663, 671, 672, 675, 678, 685, \n",
    "                        688, 699, 700, 712, 719, 724, 726, 735, 744, 753, 756, 775, 797, 806, 812, 818, 826, 829, 832, 848, 852, 856, \n",
    "                        866, 869, 872, 875, 881, 886, 891, 893, 895, 901, 910, 930, 936, 940, 953, 958, 989, 997, 1004, 1006],\n",
    "           \"match_7\" : [ 13,  34,  56,  77,  83,  98, 103, 109, 114, 119, 121, 126, 129, 131, 136, 137, 138, 144, 147, 154, 157, 159,\n",
    "                        172, 174, 178, 191, 193, 195, 208, 210, 221, 224, 226, 232, 240, 243, 251, 257, 266, 285, 299, 303, 307, 319, \n",
    "                        324, 328, 330, 334, 342, 345, 348, 352, 354, 361, 364, 369, 371, 374, 379, 382, 387, 393, 397, 400, 406, 411, \n",
    "                        418, 423, 428, 433, 436, 440, 445, 450, 459, 465, 468, 470, 489, 495, 497, 507, 510, 546, 557, 584, 589, 592, \n",
    "                        608, 620, 624, 634, 649, 659, 661, 668, 670, 683, 690, 693, 696, 705, 723, 731, 748, 754, 770, 776, 779, 784, \n",
    "                        786, 794, 797, 802, 807, 808, 819, 823, 825, 827, 834, 843, 856, 863, 865, 872, 880, 882, 883, 888, 891, 894, \n",
    "                        899, 902, 904, 906, 912, 920, 923, 931, 939, 948, 956, 970, 973, 981, 989, 992, 997, 999, 1008, 1010, 1013, 1014],\n",
    "           \"match_8\" : [ 37,  57,  73,  83, 100, 119, 130, 141, 147, 155, 159, 161, 172, 174, 186, 188, 193, 195, 201, 203, 207, 209, \n",
    "                        218, 220, 223, 225, 232, 236, 239, 241, 243, 244, 247, 259, 261, 263, 279, 294, 300, 302, 305, 307, 317, 319, \n",
    "                        321, 324, 327, 331, 336, 337, 338, 347, 350, 351, 359, 363, 365, 374, 379, 386, 411, 422, 437, 446, 466, 469, \n",
    "                        473, 492, 498, 510, 513, 524, 531, 534, 538, 550, 557, 572, 575, 594, 601, 608, 616, 619, 626, 634, 638, 641, \n",
    "                        642, 650, 653, 661, 665, 668, 676, 687, 714, 725, 736, 746, 747, 760, 770, 784, 794, 807, 817, 831, 842, 846, \n",
    "                        854, 864, 867, 871, 874, 878, 881, 894, 898, 901, 904, 914, 925, 928, 934, 937, 938, 947, 951, 960, 967, 970, \n",
    "                        974, 979, 987, 990, 993, 996, 999, 1005, 1013, 1016, 1019, 1025],\n",
    "           \"match_9\" : [ 13,  36,  58,  77,  85, 103, 108, 114, 119, 123, 125, 130, 131, 137, 142, 150, 156, 160, 166, 173, 176, 179, \n",
    "                        182, 183, 188, 192, 194, 199, 202, 204, 206, 218, 221, 222, 226, 228, 231, 243, 245, 246, 248, 252, 253, 262, \n",
    "                        269, 275, 279, 281, 284, 291, 298, 300, 302, 306, 313, 315, 318, 328, 334, 336, 357, 360, 371, 376, 377, 379, \n",
    "                        388, 391, 398, 402, 410, 420, 423, 431, 440, 441, 444, 450, 453, 460, 462, 471, 474, 482, 494, 507, 518, 521, \n",
    "                        528, 535, 540, 542, 552, 555, 567, 570, 573, 575, 585, 588, 591, 595, 604, 607, 613, 618, 623, 628, 637, 638, \n",
    "                        647, 655, 660, 669, 678, 687, 700, 717, 721, 729, 744, 747, 748, 760, 772, 775, 788, 789, 800, 818, 819, 824, \n",
    "                        848, 851, 870, 878, 887, 896, 897, 899, 900, 903, 907, 925, 927, 932, 937, 940, 955, 962, 965, 968, 979, 990, \n",
    "                        992, 1000, 1010, 1018],\n",
    "           \"match_10\": [ 14,  36,  58,  77,  85, 103, 108, 114, 119, 123, 125, 130, 131, 137, 142, 150, 156, 160, 166, 173, 176, 179, \n",
    "                        182, 183, 188, 192, 194, 199, 202, 204, 206, 218, 221, 222, 226, 228, 231, 243, 245, 246, 248, 252, 253, 262, \n",
    "                        269, 275, 279, 281, 284, 291, 298, 300, 302, 306, 313, 315, 318, 328, 334, 336, 357, 360, 371, 376, 377, 379, \n",
    "                        388, 391, 398, 402, 410, 420, 423, 431, 440, 441, 444, 450, 453, 460, 462, 471, 474, 482, 494, 507, 518, 521, \n",
    "                        528, 535, 540, 542, 552, 554, 566, 570, 573, 575, 585, 588, 591, 595, 604, 607, 613, 618, 623, 628, 637, 638, \n",
    "                        647, 655, 660, 669, 678, 687, 700, 717, 721, 729, 744, 747, 748, 760, 772, 775, 788, 789, 800, 818, 819, 824, \n",
    "                        848, 851, 870, 878, 897, 899, 900, 903, 907, 925, 927, 932, 936, 940, 955, 962, 965, 968, 979, 990, 992, 1000, \n",
    "                        1010, 1018]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f23226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from enduro_lstm import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7084614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU (y/n) n\n",
      "Selected CPU\n"
     ]
    }
   ],
   "source": [
    "use_gpu = input(\"Use GPU (y/n) \")\n",
    "\n",
    "if use_gpu == 'y':\n",
    "    use_gpu = True\n",
    "else:\n",
    "    use_gpu = False\n",
    "    \n",
    "device = conf_cuda(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1ed786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escreva uma observacao (sem espaco): teste_class\n"
     ]
    }
   ],
   "source": [
    "obs = input('escreva uma observacao (sem espaco): ')\n",
    "\n",
    "if obs == 'zigzag':\n",
    "    zigzag = True\n",
    "else:\n",
    "    zigzag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a27ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs: 100\n",
      "number of hidden neurons: 200\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"../1-generate/data/\"\n",
    "n_epochs = int(input(\"number of epochs: \") ) #5000\n",
    "hidden_neurons = int(input(\"number of hidden neurons: \")) #500\n",
    "stop_train = 1e-5\n",
    "\n",
    "# start_match = int(input(\"start match: \")) #45\n",
    "# end_match = int(input(\"end match: \")) #49\n",
    "\n",
    "start_match = 1\n",
    "end_match = 2\n",
    "\n",
    "# start_frame = int(input(\"start frame: \")) #1\n",
    "# end_frame = int(input(\"end frame: \")) #1000\n",
    "\n",
    "start_frame = 1\n",
    "end_frame = 1020\n",
    "\n",
    "is_softmax = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a76d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/teste_class_m1to2_f1to1020_epoch100_H200 created\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"{obs}_m{start_match}to{end_match}_f{start_frame}to{end_frame}_epoch{n_epochs}_H{hidden_neurons}\"\n",
    "newpath = f\"models/\" + model_name\n",
    "if not os.path.exists(newpath):\n",
    "    print(f\"models/\" + model_name + \" created\")\n",
    "    os.makedirs(newpath)\n",
    "else:\n",
    "    print(f\"models/\" + model_name)\n",
    "    print(\"ATTENTION! folder not created. Training informations will overwrite the existing one\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ebe3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS_LIST = get_actions_list(zigzag=zigzag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556bf9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded NPZ.\n",
      "Successfully loaded NPZ.\n"
     ]
    }
   ],
   "source": [
    "num_of_frames_arr = []\n",
    "frames_arr = []\n",
    "actions_arr = []\n",
    "\n",
    "for m in range(start_match, end_match + 1):\n",
    "    \n",
    "    num_of_frames, frames, actions, rewards, lifes = load_npz(data_path, m)\n",
    "    # frames = frames[start_frame - 1:end_frame]\n",
    "    frames = frames[start_frame - 1:end_frame, 30:130, 10:110]\n",
    "    frames = frames.reshape(end_frame - start_frame + 1, -1)\n",
    "    actions = actions[start_frame - 1:end_frame]\n",
    "    \n",
    "    action_one_hot = [prepare_action_data(i, ACTIONS_LIST) for i in actions]\n",
    "    actions = np.array(action_one_hot)\n",
    "    actions = actions.reshape(len(actions), -1)\n",
    "    \n",
    "    frames_arr.append(frames)\n",
    "    actions_arr.append(actions)\n",
    "    num_of_frames_arr.append(end_frame - start_frame + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695e2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunked = []\n",
    "target_chunked = []\n",
    "for i in range(len(frames_arr)):\n",
    "    for j in range(len(indices[i]) - 1):\n",
    "        data_chunked.append(frames_arr[i][indices[i][j]:indices[i][j+1]])\n",
    "        target_chunked.append(actions_arr[i][indices[i][j]:indices[i][j+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264fbd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alece\\miniconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\alece\\miniconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(data_chunked)/255\n",
    "Y_train = np.array(target_chunked)\n",
    "num_of_frames_arr = np.array(num_of_frames_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9dcdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtrado = []\n",
    "Y_train_filtrado = []\n",
    "for i in range(len(X_train)):\n",
    "    if len(X_train[i]) > 100:\n",
    "        pass\n",
    "    else:\n",
    "        X_train_filtrado.append(X_train[i])\n",
    "        Y_train_filtrado.append(Y_train[i])\n",
    "X_train = X_train_filtrado\n",
    "Y_train = Y_train_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186224ae",
   "metadata": {},
   "source": [
    "teste com zigzag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eecca307",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(frames_arr)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34b48f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = actions_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2d39b",
   "metadata": {},
   "source": [
    "voltando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2b09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(map(lambda x: torch.tensor(x), X_train))\n",
    "Y_train = list(map(lambda x: torch.tensor(x), Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715fe2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([62., 18., 26., 11., 20.,  5., 14.,  3.,  7., 19., 30., 11.,  9.,  7.,\n",
       "        10.,  7., 20.,  2., 11.,  4.,  3.,  4.,  5.,  7.,  3.,  3.,  8.,  4.,\n",
       "         2., 19.,  6., 14.,  3.,  5.,  4.,  4.,  3.,  9., 12.,  6., 24.,  3.,\n",
       "        18.,  4., 18.,  4., 13.,  2.,  4.,  3., 12.,  3., 10., 14.,  6., 12.,\n",
       "         6.,  5.,  3.,  4.,  7.,  3.,  6.,  3.,  3.,  8.,  1.,  3.,  9., 10.,\n",
       "         4., 17., 10., 13.,  8.,  4.,  7., 14.,  6.,  2.,  9.,  4.,  6.,  9.,\n",
       "         1., 15.,  3., 16.,  1.,  9.,  6.,  4., 14., 11., 16.,  4.,  3., 11.,\n",
       "         6.,  1.,  2.,  6.,  2., 19.,  2.,  5.,  8.,  4.,  3.,  6., 11., 19.,\n",
       "         3., 19.,  7.,  8.,  8.,  3.,  5.,  2.,  1., 31., 14.,  2.,  9.,  7.,\n",
       "         6., 16.,  3.,  4.,  4., 16.,  6.,  6.,  6.,  7.,  6.,  8.,  7., 12.,\n",
       "        14., 18., 21., 20.,  6.,  6.,  5.,  9.,  3.,  3.,  1.,  6.,  3., 10.,\n",
       "         1.,  1.,  9.,  2., 10.,  4.,  9.,  3.,  3.,  1.,  3.,  2., 11.,  3.,\n",
       "        24.,  6., 13.,  2.,  6., 12.,  3.,  3.,  3., 12.,  2.,  2.,  3.,  6.,\n",
       "         9.,  2.,  9.,  7., 21.,  4., 18.,  9.,  5., 10.,  8.,  6.,  4.,  4.,\n",
       "        11.,  7., 15.,  1.,  5., 43., 16., 13., 17., 12.,  4., 11.,  4.,  7.,\n",
       "         3.,  4.,  5.,  1.,  3., 14.,  4.,  4.,  4.,  3., 11.,  3.,  8.,  1.,\n",
       "         2.,  8.,  4.,  3.,  7.,  6.,  3.,  6.,  6.,  6.,  7.,  6.,  6., 10.,\n",
       "         4.,  7.,  1., 23., 16.,  7.,  3., 12.,  1.,  5.,  1.,  1., 12.,  3.,\n",
       "         4.,  5.,  8.,  3.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = torch.FloatTensor(list(map(len,X_train)))\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fba98461",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequence(X_train, batch_first=True).float()\n",
    "Y_train = pad_sequence(Y_train, batch_first=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36e7d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pack_padded_sequence(X_train, seq_len, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff029f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b90595edf483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mRNNModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_softmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, device, input_size, output_size, hidden_dim, n_layers, is_softmax):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # self.h0 = torch.zeros(self.n_layers, 1, self.hidden_dim).to(self.device)\n",
    "        # self.c0 = torch.zeros(self.n_layers, 1, self.hidden_dim).to(self.device)\n",
    "\n",
    "        self.init_hidden()\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)  \n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        if is_softmax:\n",
    "            self.out = nn.Softmax()\n",
    "        else:\n",
    "            self.out = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # batch_size = x.size(0)\n",
    "        batch_size = 1\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        # hidden = self.init_hidden(batch_size)\n",
    "        # self.h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        # self.c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "        # hidden = self.init_hidden()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        # out, hidden = self.lstm(x)\n",
    "        hidden = torch.randn(1, self.input_size, self.hidden_dim)\n",
    "        \n",
    "        pad_embed_pack_lstm = self.rnn(x, hidden)\n",
    "        pad_embed_pack_lstm_pad = pad_packed_sequence(pad_embed_pack_lstm[0], batch_first=True)\n",
    "        \n",
    "        outs, lens = pad_embed_pack_lstm_pad\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = outs.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(1, self.input_size, self.hidden_dim)\n",
    "        hidden_b = torch.randn(1, self.input_size, self.hidden_dim)\n",
    "\n",
    "        if self.device.type == 'cuda':\n",
    "            hidden_a = hidden_a.cuda()\n",
    "            hidden_b = hidden_b.cuda()\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4309aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(device=device, input_size=10000, output_size=len(ACTIONS_LIST), hidden_dim=hidden_neurons, n_layers=1, is_softmax=is_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7f0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "    X_train = X_train.cuda() \n",
    "    Y_train = Y_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b3a18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 1e-05\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04faee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_arr = np.array([])\n",
    "train_acc_arr = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5f89a34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100-------------------------------------------\n",
      "Train -> Loss: 0.227334633469582 Acc: 0.061617944389582\n",
      "Epoch: 20/100-------------------------------------------\n",
      "Train -> Loss: 0.224875122308731 Acc: 0.061617944389582\n",
      "Epoch: 30/100-------------------------------------------\n",
      "Train -> Loss: 0.222500130534172 Acc: 0.061617944389582\n",
      "Epoch: 40/100-------------------------------------------\n",
      "Train -> Loss: 0.220173269510269 Acc: 0.069934472441673\n",
      "Epoch: 50/100-------------------------------------------\n",
      "Train -> Loss: 0.217863976955414 Acc: 0.073399700224400\n",
      "Epoch: 60/100-------------------------------------------\n",
      "Train -> Loss: 0.215586349368095 Acc: 0.074596777558327\n",
      "Epoch: 70/100-------------------------------------------\n",
      "Train -> Loss: 0.213328033685684 Acc: 0.076171875000000\n",
      "Epoch: 80/100-------------------------------------------\n",
      "Train -> Loss: 0.211099743843079 Acc: 0.077179938554764\n",
      "Epoch: 90/100-------------------------------------------\n",
      "Train -> Loss: 0.208907648921013 Acc: 0.078188002109528\n",
      "Epoch: 100/100-------------------------------------------\n",
      "Train -> Loss: 0.206732630729675 Acc: 0.078692033886909\n",
      "--- 50.85022854804993 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time_processing = time.time()\n",
    "\n",
    "# Training Run\n",
    "loss_file = open(newpath + '/' + \"loss_file.txt\", \"w\")\n",
    "first_time = True\n",
    "\n",
    "best_loss = 1\n",
    "first_epoch = True\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    X_train.to(device)\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, Y_train.view(-1,len(ACTIONS_LIST)).float())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordinglyw\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "\n",
    "        train_loss_arr = np.append(train_loss_arr, loss.item())\n",
    "        train_acc_arr  = np.append(train_acc_arr, get_acc(output, Y_train.reshape(-1, len(ACTIONS_LIST))))\n",
    "        # train_acc_arr  = np.append(train_acc_arr, get_acc_2(output, target_padded.reshape(-1, len(ACTIONS_LIST))))\n",
    "        \n",
    "        loss_file.write(\"Epoch: {}/{}-------------------------------------------\\n\".format(epoch, n_epochs))\n",
    "        loss_file.write(\"Train -> Loss: {:.15f} Acc: {:.15f}\\n\".format(train_loss_arr[-1], train_acc_arr[-1]))\n",
    "            \n",
    "        print(\"Epoch: {}/{}-------------------------------------------\".format(epoch, n_epochs))\n",
    "        print(\"Train -> Loss: {:.15f} Acc: {:.15f}\".format(train_loss_arr[-1], train_acc_arr[-1]))\n",
    "        \n",
    "        if train_loss_arr[-1] < best_loss:\n",
    "            state = { 'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict(), 'losslogger': loss.item(), }\n",
    "            torch.save(state, newpath + '/' + model_name)\n",
    "            best_loss = loss.item()\n",
    "        else:\n",
    "            print(\"model not saved\")\n",
    "            \n",
    "loss_file.write(\"--- %s seconds ---\" % (time.time() - start_time_processing))\n",
    "loss_file.close()\n",
    "np.savez(newpath + '/' + \"train_loss_arr\", train_loss_arr)\n",
    "#np.savez(newpath + '/' + \"valid_acc_table\", valid_loss_mean_arr)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time_processing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9a023ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for loss\n",
    "plt.clf()\n",
    "plt.plot(train_loss_arr, color='blue')\n",
    "plt.title('model train loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.yscale('log')\n",
    "plt.savefig(newpath + '/' + 'train_loss.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
