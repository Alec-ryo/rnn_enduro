{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96edf5b9",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0b6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import time\n",
    "from setup_dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3890fc07",
   "metadata": {},
   "source": [
    "# Load matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03d8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_list(zigzag=False):\n",
    "\n",
    "    if zigzag:\n",
    "\n",
    "        ACTIONS = {\n",
    "            \"right\": 2,\n",
    "            \"left\": 3,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "\n",
    "        ACTIONS = {\n",
    "            \"noop\": 0,\n",
    "            \"accelerate\": 1,\n",
    "            \"right\": 2,\n",
    "            \"left\": 3,\n",
    "            \"break\": 4,\n",
    "            \"right_break\": 5,\n",
    "            \"left_break\": 6,\n",
    "            \"right_accelerate\": 7,\n",
    "            \"left_accelerate\": 8,\n",
    "        }\n",
    "\n",
    "    return list(ACTIONS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ddca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "\n",
    "    use_gpu = input((\"Do you want to use GPU (y/n)\"))\n",
    "\n",
    "    if use_gpu == 'y' and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Selected CPU\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07150a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to use GPU (y/n)y\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "data_info = {\n",
    "    'datapath' : r\"../1-generate/data/\",\n",
    "    'available_targets' : get_actions_list(zigzag=False),\n",
    "    'match_list' : [2, 3],\n",
    "    'start_frame' : 1,\n",
    "    'end_frame' : 200,\n",
    "    'device' : set_device(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43502ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "\n",
    "    def __init__(self, data_info):\n",
    "        self.datapath = data_info['datapath']\n",
    "        self.available_targets = data_info['available_targets']\n",
    "        self.match_list = data_info['match_list']\n",
    "        self.start_frame = data_info['start_frame']\n",
    "        self.end_frame = data_info['end_frame']\n",
    "        self.device = data_info['device']\n",
    "        self.data, self.targets = self.setDataFromMatch()\n",
    "        self.target_size = self.targets.shape\n",
    "        self.seq_len = self.setSeqLen(self.data)\n",
    "    \n",
    "    def setSeqLen(self, X_train):\n",
    "        return torch.FloatTensor(list(map(len,X_train)))\n",
    "\n",
    "    def setDataFromMatch(self):\n",
    "        frames_arr = []\n",
    "        actions_arr = []\n",
    "\n",
    "        for m in self.match_list:\n",
    "            \n",
    "            _, frames, actions, _, _ = self.load_npz(self.datapath, m)\n",
    "            frames = frames[self.start_frame - 1:self.end_frame, 30:130, :]\n",
    "            actions = actions[self.start_frame - 1:self.end_frame]\n",
    "            \n",
    "            action_one_hot = [self.prepare_action_data(i, self.available_targets) for i in actions]\n",
    "            actions = np.array(action_one_hot)\n",
    "            actions = actions.reshape(len(actions), -1)\n",
    "            \n",
    "            frames_arr.append(frames)\n",
    "            actions_arr.append(actions)\n",
    "\n",
    "        frames_arr = np.array(frames_arr)\n",
    "        actions_arr = np.array(actions_arr)\n",
    "\n",
    "        return frames_arr, actions_arr\n",
    "    \n",
    "    def normalize_images(self, frames):\n",
    "        return frames/255\n",
    "\n",
    "    def load_npz(self, data_path, m):\n",
    "        \n",
    "        path = data_path + \"match_\" + str(m) + \"/npz/\"\n",
    "\n",
    "        actions = np.load(path + 'actions.npz')\n",
    "        #lifes = np.load(path + 'lifes.npz')\n",
    "        frames = np.load(path + 'frames.npz')\n",
    "        rewards = np.load(path + 'rewards.npz')\n",
    "\n",
    "        arr_actions = np.float32(actions.f.arr_0)\n",
    "        # arr_lifes = lifes.f.arr_0\n",
    "        arr_lifes = np.float32(np.array([]))\n",
    "        arr_frames = np.float32(frames.f.arr_0)\n",
    "        arr_rewards = np.float32(rewards.f.arr_0)\n",
    "\n",
    "        print(\"Successfully loaded NPZ.\")\n",
    "\n",
    "        return arr_actions.shape[0], arr_frames, arr_actions, arr_rewards, arr_lifes\n",
    "\n",
    "    def prepare_action_data(self, action, ACTIONS_LIST):\n",
    "\n",
    "        new_action = np.zeros((1, len(ACTIONS_LIST)), dtype=int) \n",
    "\n",
    "        new_action[0, ACTIONS_LIST.index(action)] = 1\n",
    "\n",
    "        return new_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1c2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded NPZ.\n",
      "Successfully loaded NPZ.\n"
     ]
    }
   ],
   "source": [
    "data = dataset(data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57e71e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200., 200.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce359c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data = torch.tensor(data.data)\n",
    "data.targets = torch.tensor(data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de6e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data = data.data.view(data.data.shape[0], data.data.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22492445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data = data.data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7520c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = chunk_dataset(match_list=[1,2,3,4,5,6,7,8],\\n                     start_frame = 1,\\n                     end_frame = 1020\\n                    )\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = chunk_dataset(match_list=[1,2,3,4,5,6,7,8],\n",
    "                     start_frame = 1,\n",
    "                     end_frame = 1020\n",
    "                    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947537dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequence(raw):\n",
    "    return pad_sequence(raw, batch_first=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1573f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def packPaddedSequence(seq, seq_len):\n",
    "    return pack_padded_sequence(seq, seq_len, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab705d16",
   "metadata": {},
   "source": [
    "Prepare frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209f627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.data = padSequence(data.data)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data.data = padSequence(data.data)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3683a92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'padded_shape = data.data.shape'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"padded_shape = data.data.shape\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd30b7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.data = packPaddedSequence(data.data, data.seq_len)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data.data = packPaddedSequence(data.data, data.seq_len)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e1378",
   "metadata": {},
   "source": [
    "Prepare targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002cf8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.targets = padSequence(data.targets)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data.targets = padSequence(data.targets)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b190da0",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba435e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_type_name():\n",
    "    print(\"Choose type of RNN model:\")\n",
    "    print(\"1 - Simple RNN\")\n",
    "    print(\"2 - LSTM\")\n",
    "    print(\"3 - CNN\")\n",
    "    choise = int(input(\"type: \"))\n",
    "    if choise == 1:\n",
    "        return \"RNN\"\n",
    "    elif choise == 2:\n",
    "        return \"LSTM\"\n",
    "    elif choise == 3:\n",
    "        return \"CNN\"\n",
    "    else:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7dd86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden neurons: 200\n",
      "Number of epochs: 500\n",
      "Choose type of RNN model:\n",
      "1 - Simple RNN\n",
      "2 - LSTM\n",
      "3 - CNN\n",
      "type: 2\n"
     ]
    }
   ],
   "source": [
    "model_info = {\n",
    "    'input_size' : 12000,\n",
    "    'hidden_neurons' : int(input(\"Number of hidden neurons: \")),\n",
    "    'output_size' : 9,\n",
    "    'n_epochs' : int(input(\"Number of epochs: \")),\n",
    "    'min_loss' : 1e-5,\n",
    "    'n_layers' : 1,\n",
    "    'type' : set_type_name(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47e5625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, data_info, model_info):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.device = data_info['device']\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = model_info['input_size']\n",
    "        self.hidden_dim = model_info['hidden_neurons']\n",
    "        self.n_layers = model_info['n_layers']\n",
    "        self.output_size = model_info['output_size']\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_dim, self.n_layers, batch_first=True)  \n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "        \n",
    "        self.out = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        out, _ = self.lstm(x, hidden)\n",
    "\n",
    "        \"\"\"\n",
    "        pad_embed_pack_lstm = self.lstm(x, hidden)\n",
    "        pad_embed_pack_lstm_pad = pad_packed_sequence(pad_embed_pack_lstm[0], batch_first=True)\n",
    "        \n",
    "        outs, _ = pad_embed_pack_lstm_pad\n",
    "        \"\"\"\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(1, batch_size, self.hidden_dim).float()\n",
    "        hidden_b = torch.randn(1, batch_size, self.hidden_dim).float()\n",
    "\n",
    "        if self.device.type == 'cuda':\n",
    "            hidden_a = hidden_a.cuda()\n",
    "            hidden_b = hidden_b.cuda()\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f76326a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(model_info):\n",
    "\n",
    "    if model_info['type'] == \"RNN\":\n",
    "        model = RNNModel(device=model_structure.device, \n",
    "                         input_size=model_structure.data_size, \n",
    "                         output_size=model_structure.output_size, \n",
    "                         hidden_dim=model_structure.hidden_neurons, \n",
    "                         n_layers=1)\n",
    "    elif model_info['type'] == \"LSTM\":\n",
    "        model = LSTMModel(data_info, model_info)\n",
    "    elif model_info['type'] == \"CNN\":\n",
    "        model = CNNLSTMModel(device=model_structure.device, \n",
    "                             input_size=model_structure.data_size, \n",
    "                             output_size=model_structure.output_size, \n",
    "                             hidden_dim=model_structure.hidden_neurons, \n",
    "                             n_layers=1)\n",
    "    else:\n",
    "        print(\"ERROR defining Model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ff5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "model = set_model(model_info)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986733f",
   "metadata": {},
   "source": [
    "# Prepare to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c21774b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setName(data_info, model_info):\n",
    "    x = [str(num) for num in data_info['match_list']]\n",
    "    x = '-'.join(x)\n",
    "    obs = input(\"write a observations without space and punctuations:\")\n",
    "    name = f\"{model_info['type']}_{obs}_m{x}_f{data_info['start_frame']}to{data_info['end_frame']}_epoch{model_info['n_epochs']}_h{model_info['hidden_neurons']}\"\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4bc1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setPath(data_info, model_info):\n",
    "    name = setName(data_info, model_info)\n",
    "    newpath = f\"models/\" + name\n",
    "    if not os.path.exists(newpath):\n",
    "        print(f\"models/\" + name + \" created\")\n",
    "        os.makedirs(newpath)\n",
    "    else:\n",
    "        print(f\"models/\" + name)\n",
    "        print(\"ATTENTION! folder not created. Training informations will overwrite the existing one\")\n",
    "    return newpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "667dd8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write a observations without space and punctuations:teste\n",
      "models/LSTM_teste_m2-3_f1to200_epoch500_h200 created\n"
     ]
    }
   ],
   "source": [
    "model_path = setPath(data_info, model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9e829",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97803994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predicted, target):\n",
    "    \n",
    "    predicted = torch.argmax(predicted, axis=1)\n",
    "    target = torch.argmax(target, axis=1)\n",
    "\n",
    "    correct = torch.sum(predicted == target)\n",
    "\n",
    "    acc = correct/predicted.shape[0]\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "654ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_info['device'].type == 'cuda':\n",
    "    mymodel = model.cuda()\n",
    "    X_train = data.data.cuda() \n",
    "    Y_train = data.targets.cuda()\n",
    "else:\n",
    "    mymodel = model\n",
    "    X_train = data.data \n",
    "    Y_train = data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "900294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_arr = np.array([])\n",
    "train_acc_arr = np.array([])\n",
    "valid_acc_arr = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ef264d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_info_string(current_epoch, last_epoch, train_loss, train_acc, valid_acc):\n",
    "    epoch_info = f\"Epoch: {current_epoch}/{last_epoch}-------------------------------------------\\n\"\n",
    "    train_info = f\"Train -> Loss: {train_loss:.15f} Acc: {train_acc:.15f}\\n\"\n",
    "    valid_info = f\"Valid -> Acc: {valid_acc:.15f}\\n\"\n",
    "    return epoch_info, train_info, valid_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a15ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=len(data.data), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddbee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alece\\miniconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/500-------------------------------------------\n",
      "Train -> Loss: 0.026529757305980 Acc: 0.855000019073486\n",
      "Valid -> Acc: 0.810000002384186\n",
      "Epoch: 20/500-------------------------------------------\n",
      "Train -> Loss: 0.017210021615028 Acc: 0.930000007152557\n",
      "Valid -> Acc: 0.904999971389771\n",
      "Epoch: 30/500-------------------------------------------\n",
      "Train -> Loss: 0.011362924240530 Acc: 0.954999983310699\n",
      "Valid -> Acc: 0.774999976158142\n",
      "Epoch: 40/500-------------------------------------------\n",
      "Train -> Loss: 0.008974323049188 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.824999988079071\n",
      "Epoch: 50/500-------------------------------------------\n",
      "Train -> Loss: 0.007899773307145 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.824999988079071\n",
      "Epoch: 60/500-------------------------------------------\n",
      "Train -> Loss: 0.007780951447785 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.859999954700470\n",
      "Epoch: 70/500-------------------------------------------\n",
      "Train -> Loss: 0.007162958383560 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.839999973773956\n",
      "Epoch: 80/500-------------------------------------------\n",
      "Train -> Loss: 0.007041519042104 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.814999997615814\n",
      "Epoch: 90/500-------------------------------------------\n",
      "Train -> Loss: 0.006884909234941 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.805000007152557\n",
      "Epoch: 100/500-------------------------------------------\n",
      "Train -> Loss: 0.006781660951674 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.794999957084656\n",
      "Epoch: 110/500-------------------------------------------\n",
      "Train -> Loss: 0.006683233659714 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.805000007152557\n",
      "Epoch: 120/500-------------------------------------------\n",
      "Train -> Loss: 0.006805810146034 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.805000007152557\n",
      "model not saved\n",
      "Epoch: 130/500-------------------------------------------\n",
      "Train -> Loss: 0.006551692727953 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.810000002384186\n",
      "Epoch: 140/500-------------------------------------------\n",
      "Train -> Loss: 0.006555395666510 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.810000002384186\n",
      "model not saved\n",
      "Epoch: 150/500-------------------------------------------\n",
      "Train -> Loss: 0.006428364664316 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.794999957084656\n",
      "Epoch: 160/500-------------------------------------------\n",
      "Train -> Loss: 0.006492795888335 Acc: 0.964999973773956\n",
      "Valid -> Acc: 0.794999957084656\n",
      "model not saved\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(data.data)):\n",
    "        \n",
    "    start_time_processing = time.time()\n",
    "\n",
    "    best_loss = 1\n",
    "    first_epoch = True\n",
    "    \n",
    "    loss_file = open(model_path + '/' + f\"loss_file_{fold}.txt\", \"w\")\n",
    "    \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    X_train = data.data[train_ids]\n",
    "    Y_train = data.targets[train_ids]\n",
    "    X_valid = data.data[test_ids]\n",
    "    Y_valid = data.targets[test_ids]\n",
    "\n",
    "    #X_train = padSequence(tuple(X_train))\n",
    "    #X_valid = padSequence(tuple(X_valid))\n",
    "    \"\"\"\n",
    "    X_train_padded_shape = X_train.shape\n",
    "    X_train_seq_len = data.seq_len[train_ids]\n",
    "    X_train = packPaddedSequence(X_train, X_train_seq_len)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    X_valid_padded_shape = X_valid.shape\n",
    "    X_valid_seq_len = data.seq_len[test_ids]\n",
    "    X_valid = packPaddedSequence(X_valid, X_valid_seq_len)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if data_info['device'].type == 'cuda':\n",
    "        mymodel = model.cuda()\n",
    "        X_train = X_train.cuda() \n",
    "        Y_train = Y_train.cuda()\n",
    "        X_valid = X_valid.cuda() \n",
    "        Y_valid = Y_valid.cuda()\n",
    "    \"\"\"\n",
    "    if data_info['device'].type == 'cuda':\n",
    "        mymodel = model.cuda()\n",
    "        X_valid = X_valid.cuda() \n",
    "        Y_valid = Y_valid.cuda()\n",
    "    \n",
    "    for epoch in range(1, model_info['n_epochs'] + 1):\n",
    "        \n",
    "        predicted_outputs = torch.tensor([])\n",
    "\n",
    "        for sequence, targets in zip(X_train, Y_train):\n",
    "            \n",
    "            if data_info['device'].type == 'cuda':\n",
    "                sequence = sequence.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            mymodel.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = mymodel(sequence.view(1, sequence.shape[0], sequence.shape[1]))\n",
    "            predicted_outputs = torch.cat((predicted_outputs.to('cpu'), output.to('cpu')), 0)\n",
    "\n",
    "            loss = criterion(output, targets.view(-1, model_info['output_size']).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "\n",
    "            train_loss_arr = np.append(train_loss_arr, loss.item())\n",
    "            train_acc_arr  = np.append(train_acc_arr, get_acc(predicted_outputs.reshape(-1, len(data.available_targets)), Y_train.reshape(-1, len(data.available_targets))))\n",
    "            \n",
    "            mymodel.eval()\n",
    "            \n",
    "            output = mymodel(X_valid)\n",
    "            valid_acc_arr  = np.append(valid_acc_arr, get_acc(output, Y_valid.reshape(-1, len(data.available_targets))))\n",
    "                        \n",
    "            epoch_info, train_info, valid_info = create_info_string(epoch, \n",
    "                                                                    model_info['n_epochs'], \n",
    "                                                                    train_loss_arr[-1], \n",
    "                                                                    train_acc_arr[-1],\n",
    "                                                                    valid_acc_arr[-1])\n",
    "            loss_file.write(epoch_info)\n",
    "            loss_file.write(train_info)\n",
    "            loss_file.write(valid_info)\n",
    "            print(epoch_info, end = '')\n",
    "            print(train_info, end = '')\n",
    "            print(valid_info, end = '')\n",
    "\n",
    "            if train_loss_arr[-1] < best_loss:\n",
    "                state = {'epoch': epoch, \n",
    "                         'state_dict': mymodel.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(), \n",
    "                         'losslogger': loss.item(), \n",
    "                        }\n",
    "                torch.save(state, model_path + '/' + f'model_fold_{fold}')\n",
    "                best_loss = loss.item()\n",
    "            else:\n",
    "                print(\"model not saved\")\n",
    "                \n",
    "    time_info = \"--- %s seconds ---\" % (time.time() - start_time_processing)\n",
    "    loss_file.write(time_info)\n",
    "    print(time_info)\n",
    "\n",
    "    loss_file.close()\n",
    "    np.savez(model_path + '/' + f\"train_loss_arr_{fold}\", train_loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time_processing = time.time()\n",
    "\n",
    "loss_file = open(model_path + '/' + \"loss_file.txt\", \"w\")\n",
    "\n",
    "best_loss = 1\n",
    "first_epoch = True\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(data.data)):\n",
    "    \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    X_train = data.data[train_ids]\n",
    "    Y_train = data.targets[train_ids]\n",
    "    X_valid = data.data[test_ids]\n",
    "    Y_valid = data.targets[test_ids]\n",
    "    \n",
    "    X_train = padSequence(X_train)\n",
    "    X_valid = padSequence(X_valid)\n",
    "    \n",
    "    X_train_padded_shape = X_train.shape\n",
    "    X_train_seq_len = data.seq_len[train_ids]\n",
    "    X_train = packPaddedSequence(X_train, X_train_seq_len)\n",
    "    \n",
    "    X_valid_padded_shape = X_valid.shape\n",
    "    X_valid_seq_len = data.seq_len[test_ids]\n",
    "    X_valid = packPaddedSequence(X_valid, X_valid_seq_len)\n",
    "    \n",
    "    if data_info['device'].type == 'cuda':\n",
    "        mymodel = model.cuda()\n",
    "        X_train = X_train.cuda() \n",
    "        Y_train = Y_train.cuda()\n",
    "        X_valid = X_valid.cuda() \n",
    "        Y_valid = Y_valid.cuda()\n",
    "    \n",
    "    for epoch in range(1, model_info['n_epochs'] + 1):\n",
    "\n",
    "        mymodel.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mymodel(X_train)\n",
    "\n",
    "        loss = criterion(output, Y_train.view(-1,model_info['output_size']).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "\n",
    "            train_loss_arr = np.append(train_loss_arr, loss.item())\n",
    "            train_acc_arr  = np.append(train_acc_arr, get_acc(output, Y_train.reshape(-1, len(data.available_targets))))\n",
    "            \n",
    "            mymodel.eval()\n",
    "            \n",
    "            output = mymodel(X_valid)\n",
    "            valid_acc_arr  = np.append(valid_acc_arr, get_acc(output, Y_valid.reshape(-1, len(data.available_targets))))\n",
    "                        \n",
    "            epoch_info, train_info, valid_info = create_info_string(epoch, \n",
    "                                                                    model_info['n_epochs'], \n",
    "                                                                    train_loss_arr[-1], \n",
    "                                                                    train_acc_arr[-1],\n",
    "                                                                    valid_acc_arr[-1])\n",
    "            loss_file.write(epoch_info)\n",
    "            loss_file.write(train_info)\n",
    "            loss_file.write(valid_info)\n",
    "            print(epoch_info)\n",
    "            print(train_info)\n",
    "            print(valid_info)\n",
    "\n",
    "            if train_loss_arr[-1] < best_loss:\n",
    "                state = {'epoch': epoch, \n",
    "                         'state_dict': mymodel.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(), \n",
    "                         'losslogger': loss.item(), \n",
    "                        }\n",
    "                torch.save(state, model_path + '/' + f'model_fold_{fold}')\n",
    "                best_loss = loss.item()\n",
    "            else:\n",
    "                print(\"model not saved\")\n",
    "                \n",
    "time_info = \"--- %s seconds ---\" % (time.time() - start_time_processing)\n",
    "loss_file.write(time_info)\n",
    "print(time_info)\n",
    "\n",
    "loss_file.close()\n",
    "np.savez(model_path + '/' + \"train_loss_arr\", train_loss_arr)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ae108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time_processing = time.time()\n",
    "\n",
    "loss_file = open(model_path + '/' + \"loss_file.txt\", \"w\")\n",
    "\n",
    "best_loss = 1\n",
    "first_epoch = True\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(data.data)):\n",
    "    \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    X_train = data.data[train_ids]\n",
    "    Y_train = data.targets[train_ids]\n",
    "    X_valid = data.data[test_ids]\n",
    "    Y_valid = data.targets[test_ids]\n",
    "    \n",
    "    X_train = padSequence(X_train)\n",
    "    X_valid = padSequence(X_valid)\n",
    "    \n",
    "    X_valid_padded_shape = X_valid.shape\n",
    "    X_valid_seq_len = data.seq_len[test_ids]\n",
    "    X_valid = packPaddedSequence(X_valid, X_valid_seq_len)\n",
    "    \n",
    "    if data_info['device'].type == 'cuda':\n",
    "        mymodel = model.cuda()\n",
    "        X_train = X_train.cuda() \n",
    "        Y_train = Y_train.cuda()\n",
    "        X_valid = X_valid.cuda() \n",
    "        Y_valid = Y_valid.cuda()\n",
    "    \n",
    "    for epoch in range(1, model_info['n_epochs'] + 1):\n",
    "        \n",
    "        print(\"epoch:\", epoch)\n",
    "        \n",
    "        mymodel.train()\n",
    "            \n",
    "        for sequence, targets, seq_len2 in zip(X_train, Y_train, data.seq_len[train_ids]):\n",
    "            \n",
    "            print(sequence.shape)\n",
    "            print(targets.shape)\n",
    "            \n",
    "            sequence = sequence.view(1, sequence.shape[0], sequence.shape[1])\n",
    "            X_train_padded_shape = sequence.shape\n",
    "            X_train_seq_len = torch.tensor([seq_len2])\n",
    "            sequence = packPaddedSequence(sequence, X_train_seq_len)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = mymodel(sequence)\n",
    "\n",
    "            loss = criterion(output, targets.view(-1,model_info['output_size']).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "\n",
    "            train_loss_arr = np.append(train_loss_arr, loss.item())\n",
    "            train_acc_arr  = np.append(train_acc_arr, get_acc(output, sequence.reshape(-1, len(data.available_targets))))\n",
    "            \n",
    "            mymodel.eval()\n",
    "            \n",
    "            output = mymodel(X_valid)\n",
    "            valid_acc_arr  = np.append(valid_acc_arr, get_acc(output, Y_valid.reshape(-1, len(data.available_targets))))\n",
    "                        \n",
    "            epoch_info, train_info, valid_info = create_info_string(epoch, \n",
    "                                                                    model_info['n_epochs'], \n",
    "                                                                    train_loss_arr[-1], \n",
    "                                                                    train_acc_arr[-1],\n",
    "                                                                    valid_acc_arr[-1])\n",
    "            loss_file.write(epoch_info)\n",
    "            loss_file.write(train_info)\n",
    "            loss_file.write(valid_info)\n",
    "            print(epoch_info)\n",
    "            print(train_info)\n",
    "            print(valid_info)\n",
    "\n",
    "            if train_loss_arr[-1] < best_loss:\n",
    "                state = {'epoch': epoch, \n",
    "                         'state_dict': mymodel.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(), \n",
    "                         'losslogger': loss.item(), \n",
    "                        }\n",
    "                torch.save(state, model_path + '/' + f'model_fold_{fold}')\n",
    "                best_loss = loss.item()\n",
    "            else:\n",
    "                print(\"model not saved\")\n",
    "                \n",
    "time_info = \"--- %s seconds ---\" % (time.time() - start_time_processing)\n",
    "loss_file.write(time_info)\n",
    "print(time_info)\n",
    "\n",
    "loss_file.close()\n",
    "np.savez(model_path + '/' + \"train_loss_arr\", train_loss_arr)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
